[
["index.html", "Statistical Inference via Data Science A moderndive into R and the tidyverse Chapter 1 Introduction 1.1 Introduction for students 1.2 Introduction for instructors 1.3 DataCamp 1.4 Connect and contribute 1.5 About this book 1.6 About the authors", " Statistical Inference via Data Science A moderndive into R and the tidyverse Chester Ismay, Albert Y. Kim, Arend M. Kuyper, Elizabeth Tipton, and Kaitlyn Fitzgerald September 05, 2019 Chapter 1 Introduction Special Announcement We’re excited to announce that we’ve signed a book deal with CRC Press! We will be publishing our first fully complete online version of ModernDive in Summer 2019, with a corresponding print edition to follow in Fall 2019. Don’t worry though, our content will always remain freely available on ModernDive.com. Please note that you are currently looking at the “development version” of ModernDive, which is a work in progress currently being edited and thus subject to frequent change. For the latest “released version” of ModernDive, which changes much less frequently, please visit ModernDive.com. Help! I’m new to R and RStudio and I need to learn about them! However, I’m completely new to coding! What do I do? If you’re asking yourself this question, then you’ve come to the right place! Start with our Introduction for Students. Are you an instructor hoping to use this book in your courses? Then click here for more information on how to teach with this book. Are you looking to connect with and contribute to ModernDive? Then click here for information on how. Are you curious about the publishing of this book? Then click here for more information on the open-source technology, in particular R Markdown and the bookdown package. This is version 0.5.0.9000 of ModernDive published on September 05, 2019. For previous versions of ModernDive, see Section 1.5. While a PDF version of this book can be found here, this is very much a work in progress with many things that still need to be fixed. We appreciate your patience. 1.1 Introduction for students This book assumes no prerequisites: no algebra, no calculus, and no prior programming/coding experience. This is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians, data journalists, and other researchers would. In Figure 1.1 we present a flowchart of what you’ll cover in this book. You’ll first get started with data in Chapter 2, where you’ll learn about the difference between R and RStudio, start coding in R, understand what R packages are, and explore your first dataset: all domestic departure flights from a New York City airport in 2013. Then Data science: You’ll assemble your data science toolbox using tidyverse packages. In particular: Ch.3: Visualizing data via the ggplot2 package. Ch.5: Understanding the concept of “tidy” data as a standardized data input format for all packages in the tidyverse Ch.4: Wrangling data via the dplyr package. Data modeling: Using these data science tools and helper functions from the moderndive package, you’ll start performing data modeling. In particular: Ch.6: Constructing basic regression models. Ch.7: Constructing multiple regression models. Statistical inference: Once again using your newly acquired data science tools, we’ll unpack statistical inference using the infer package. In particular: Ch.9: Understanding the role that sampling variability plays in statistical inference using both tactile and virtual simulations of sampling from a “bowl” with an unknown proportion of red balls. Ch.??: Building confidence intervals. Ch.??: Conducting hypothesis tests. Data modeling revisited: Armed with your new understanding of statistical inference, you’ll revisit and review the models you constructed in Ch.6 &amp; Ch.7. In particular: Ch.??: Interpreting both the statistical and practice significance of the results of the models. We’ll end with a discussion on what it means to “think with data” in Chapter ?? and present an example case study data analysis of house prices in Seattle. FIGURE 1.1: ModernDive Flowchart 1.1.1 What you will learn from this book We hope that by the end of this book, you’ll have learned How to use R to explore data. How to answer statistical questions using tools like confidence intervals and hypothesis tests. How to effectively create “data stories” using these tools. What do we mean by data stories? We mean any analysis involving data that engages the reader in answering questions with careful visuals and thoughtful discussion, such as How strong is the relationship between per capita income and crime in Chicago neighborhoods? and How many f**ks does Quentin Tarantino give (as measured by the amount of swearing in his films)?. Further discussions on data stories can be found in this Think With Google article. For other examples of data stories constructed by students like yourselves, look at the final projects for two courses that have previously used ModernDive: Middlebury College MATH 116 Introduction to Statistical and Data Sciences using student collected data. Pacific University SOC 301 Social Statistics using data from the fivethirtyeight R package. This book will help you develop your “data science toolbox”, including tools such as data visualization, data formatting, data wrangling, and data modeling using regression. With these tools, you’ll be able to perform the entirety of the “data/science pipeline” while building data communication skills (see Subsection 1.1.2 for more details). In particular, this book will lean heavily on data visualization. In today’s world, we are bombarded with graphics that attempt to convey ideas. We will explore what makes a good graphic and what the standard ways are to convey relationships with data. You’ll also see the use of visualization to introduce concepts like mean, median, standard deviation, distributions, etc. In general, we’ll use visualization as a way of building almost all of the ideas in this book. To impart the statistical lessons in this book, we have intentionally minimized the number of mathematical formulas used and instead have focused on developing a conceptual understanding via data visualization, statistical computing, and simulations. We hope this is a more intuitive experience than the way statistics has traditionally been taught in the past and how it is commonly perceived. Finally, you’ll learn the importance of literate programming. By this we mean you’ll learn how to write code that is useful not just for a computer to execute but also for readers to understand exactly what your analysis is doing and how you did it. This is part of a greater effort to encourage reproducible research (see Subsection 1.1.3 for more details). Hal Abelson coined the phrase that we will follow throughout this book: “Programs must be written for people to read, and only incidentally for machines to execute.” We understand that there may be challenging moments as you learn to program. Both of us continue to struggle and find ourselves often using web searches to find answers and reach out to colleagues for help. In the long run though, we all can solve problems faster and more elegantly via programming. We wrote this book as our way to help you get started and you should know that there is a huge community of R users that are always happy to help everyone along as well. This community exists in particular on the internet on various forums and websites such as stackoverflow.com. 1.1.2 Data/science pipeline You may think of statistics as just being a bunch of numbers. We commonly hear the phrase “statistician” when listening to broadcasts of sporting events. Statistics (in particular, data analysis), in addition to describing numbers like with baseball batting averages, plays a vital role in all of the sciences. You’ll commonly hear the phrase “statistically significant” thrown around in the media. You’ll see articles that say “Science now shows that chocolate is good for you.” Underpinning these claims is data analysis. By the end of this book, you’ll be able to better understand whether these claims should be trusted or whether we should be wary. Inside data analysis are many sub-fields that we will discuss throughout this book (though not necessarily in this order): data collection data wrangling data visualization data modeling inference correlation and regression interpretation of results data communication/storytelling These sub-fields are summarized in what Grolemund and Wickham term the “Data/Science Pipeline” in Figure 1.2. FIGURE 1.2: Data/Science Pipeline We will begin by digging into the gray Understand portion of the cycle with data visualization, then with a discussion on what is meant by tidy data and data wrangling, and then conclude by talking about interpreting and discussing the results of our models via Communication. These steps are vital to any statistical analysis. But why should you care about statistics? “Why did they make me take this class?” There’s a reason so many fields require a statistics course. Scientific knowledge grows through an understanding of statistical significance and data analysis. You needn’t be intimidated by statistics. It’s not the beast that it used to be and, paired with computation, you’ll see how reproducible research in the sciences particularly increases scientific knowledge. 1.1.3 Reproducible research “The most important tool is the mindset, when starting, that the end product will be reproducible.” – Keith Baggerly Another goal of this book is to help readers understand the importance of reproducible analyses. The hope is to get readers into the habit of making their analyses reproducible from the very beginning. This means we’ll be trying to help you build new habits. This will take practice and be difficult at times. You’ll see just why it is so important for you to keep track of your code and well-document it to help yourself later and any potential collaborators as well. Copying and pasting results from one program into a word processor is not the way that efficient and effective scientific research is conducted. It’s much more important for time to be spent on data collection and data analysis and not on copying and pasting plots back and forth across a variety of programs. In a traditional analyses if an error was made with the original data, we’d need to step through the entire process again: recreate the plots and copy and paste all of the new plots and our statistical analysis into your document. This is error prone and a frustrating use of time. We’ll see how to use R Markdown to get away from this tedious activity so that we can spend more time doing science. “We are talking about computational reproducibility.” - Yihui Xie Reproducibility means a lot of things in terms of different scientific fields. Are experiments conducted in a way that another researcher could follow the steps and get similar results? In this book, we will focus on what is known as computational reproducibility. This refers to being able to pass all of one’s data analysis, data-sets, and conclusions to someone else and have them get exactly the same results on their machine. This allows for time to be spent interpreting results and considering assumptions instead of the more error prone way of starting from scratch or following a list of steps that may be different from machine to machine. 1.1.4 Final note for students At this point, if you are interested in instructor perspectives on this book, ways to contribute and collaborate, or the technical details of this book’s construction and publishing, then continue with the rest of the chapter below. Otherwise, let’s get started with R and RStudio in Chapter 2! 1.2 Introduction for instructors This book is inspired by the following books: “Mathematical Statistics with Resampling and R” (Chihara and Hesterberg 2011), “OpenIntro: Intro Stat with Randomization and Simulation” (Diez, Barr, and Çetinkaya-Rundel 2014), and “R for Data Science” (Grolemund and Wickham 2016). The first book, while designed for upper-level undergraduates and graduate students, provides an excellent resource on how to use resampling to impart statistical concepts like sampling distributions using computation instead of large-sample approximations and other mathematical formulas. The last two books are free options to learning introductory statistics and data science, providing an alternative to the many traditionally expensive introductory statistics textbooks. When looking over the large number of introductory statistics textbooks that currently exist, we found that there wasn’t one that incorporated many newly developed R packages directly into the text, in particular the many packages included in the tidyverse collection of packages, such as ggplot2, dplyr, tidyr, and broom. Additionally, there wasn’t an open-source and easily reproducible textbook available that exposed new learners all of three of the learning goals listed at the outset of Subsection 1.1.1. 1.2.1 Who is this book for? This book is intended for instructors of traditional introductory statistics classes using RStudio, either the desktop or server version, who would like to inject more data science topics into their syllabus. We assume that students taking the class will have no prior algebra, calculus, nor programming/coding experience. Here are some principles and beliefs we kept in mind while writing this text. If you agree with them, this might be the book for you. Blur the lines between lecture and lab With increased availability and accessibility of laptops and open-source non-proprietary statistical software, the strict dichotomy between lab and lecture can be loosened. It’s much harder for students to understand the importance of using software if they only use it once a week or less. They forget the syntax in much the same way someone learning a foreign language forgets the rules. Frequent reinforcement is key. Focus on the entire data/science research pipeline We believe that the entirety of Grolemund and Wickham’s data/science pipeline should be taught. We believe in “minimizing prerequisites to research”: students should be answering questions with data as soon as possible. It’s all about the data We leverage R packages for rich, real, and realistic data-sets that at the same time are easy-to-load into R, such as the nycflights13 and fivethirtyeight packages. We believe that data visualization is a gateway drug for statistics and that the Grammar of Graphics as implemented in the ggplot2 package is the best way to impart such lessons. However, we often hear: “You can’t teach ggplot2 for data visualization in intro stats!” We, like David Robinson, are much more optimistic. dplyr has made data wrangling much more accessible to novices, and hence much more interesting data-sets can be explored. Use simulation/resampling to introduce statistical inference, not probability/mathematical formulas Instead of using formulas, large-sample approximations, and probability tables, we teach statistical concepts using resampling-based inference. This allows for a de-emphasis of traditional probability topics, freeing up room in the syllabus for other topics. Don’t fence off students from the computation pool, throw them in! Computing skills are essential to working with data in the 21st century. Given this fact, we feel that to shield students from computing is to ultimately do them a disservice. We are not teaching a course on coding/programming per se, but rather just enough of the computational and algorithmic thinking necessary for data analysis. Complete reproducibility and customizability We are frustrated when textbooks give examples, but not the source code and the data itself. We give you the source code for all examples as well as the whole book! Ultimately the best textbook is one you’ve written yourself. You know best your audience, their background, and their priorities. You know best your own style and the types of examples and problems you like best. Customization is the ultimate end. For more about how to make this book your own, see About this Book. 1.3 DataCamp DataCamp logo DataCamp is a browser-based interactive platform for learning data science, offering courses on a wide array of courses on data science, analytics, statistics, machine learning, and artificial intelligence, where each course is a combination of lectures and exercises that offer immediate feedback. The following chapters of ModernDive roughly map to the following closely-integrated DataCamp courses that use the same R tools and often even the same datasets. By no means is this an exhaustive list of possible DataCamp courses that are relevant to the topics in this book, we recommend these ones in particular to supplement your ModernDive experience. Click on the image for each course to access its webpage on datacamp.com. Instructors at accredited universities can sign their class up for a free academic licence at DataCamp For The Classroom, giving their students access to all premium courses for 6 months for free. Chapter Topic DataCamp Courses 2 Basic R programming concepts 3 &amp; 5 Introductory data visualization and wrangling 4 &amp; 5 Data “tidying” and intermediate data wrangling 6 &amp; 7 Data modeling, basic regression, and multiple regression 9 &amp; 10 Statistical inference: confidence intervals and hypothesis testing 11 Inference for regression 1.4 Connect and contribute If you would like to connect with ModernDive, check out the following links: If you would like to receive periodic updates about ModernDive (roughly every 3 months), please sign up for our mailing list. Contact Albert at albert.ys.kim@gmail.com and Chester at chester.ismay@gmail.com. We’re on Twitter at ModernDive. If you would like to contribute to ModernDive, there are many ways! Let’s all work together to make this book as great as possible for as many students and instructors as possible! Please let us know if you find any errors, typos, or areas from improvement on our GitHub issues page. If you are familiar with GitHub and would like to contribute more, please see Section 1.5 below. For example, we thank Dr Andrew Heiss for contributing Subsection 2.2.3 on “Errors, warnings, and messages”. The authors would like to thank Nina Sonneborn, Kristin Bott, and the participants of our USCOTS 2017 workshop for their feedback and suggestions. A special thanks goes to Dr. Yana Weinstein, cognitive psychological scientist and co-founder of The Learning Scientists, for her extensive contributions. 1.5 About this book This book was written using RStudio’s bookdown package by Yihui Xie (Xie 2019). This package simplifies the publishing of books by having all content written in R Markdown. The bookdown/R Markdown source code for all versions of ModernDive is available on GitHub: Latest published version The most up-to-date release: Version 0.5.0 released on February 24, 2019 (source code). Available at ModernDive.com Development version The working copy of the next version which is currently being edited: Preview of development version is available at https://moderndive.netlify.com/ Source code: Available on ModernDive’s GitHub repository page Previous versions Older versions that may be out of date: Version 0.4.0 released on July 21, 2018 (source code) Version 0.3.0 released on February 3, 2018 (source code) Version 0.2.0 released on August 02, 2017 (source code) Version 0.1.3 released on February 09, 2017 (source code) Version 0.1.2 released on January 22, 2017 (source code) Could this be a new paradigm for textbooks? Instead of the traditional model of textbook companies publishing updated editions of the textbook every few years, we apply a software design influenced model of publishing more easily updated versions. We can then leverage open-source communities of instructors and developers for ideas, tools, resources, and feedback. As such, we welcome your pull requests. Finally, feel free to modify the book as you wish for your own needs, but please list the authors at the top of index.Rmd as “Chester Ismay, Albert Y. Kim, and YOU!” 1.6 About the authors Who we are! Chester Ismay Albert Y. Kim Chester Ismay: Senior Curriculum Lead - DataCamp, Portland, OR, USA. Email: chester.ismay@gmail.com Webpage: http://chester.rbind.io/ Twitter: old_man_chester GitHub: https://github.com/ismayc Albert Y. Kim: Assistant Professor of Statistical &amp; Data Sciences - Smith College, Northampton, MA, USA. Email: albert.ys.kim@gmail.com Webpage: http://rudeboybert.rbind.io/ Twitter: rudeboybert GitHub: https://github.com/rudeboybert References "],
["2-getting-started.html", "Chapter 2 Getting Started with Data in R 2.1 What are R and RStudio? 2.2 How do I code in R? 2.3 What are R packages? 2.4 Explore your first dataset 2.5 Conclusion", " Chapter 2 Getting Started with Data in R Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? We’ll introduce these concepts in upcoming Sections 2.1-2.3. If you are already somewhat familiar with these concepts, feel free to skip to Section 2.4 where we’ll introduce our first data set: all domestic flights departing a New York City airport in 2013. This is a dataset we will explore in depth in this book. 2.1 What are R and RStudio? For much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest: R is like a car’s engine. RStudio is like a car’s dashboard. R: Engine RStudio: Dashboard More precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well. If you are still not sure about the difference between R and RStudio IDE, we suggest you watch this DataCamp video. 2.1.1 Installing R and RStudio Note about RStudio Server: If your instructor has provided you with a link and access to RStudio Server, then you can skip this section. We do recommend though after a few months of working on the RStudio Server that you return to these instructions. You will first need to download and install both R and RStudio (Desktop version) on your computer. You must do this first: Download and install R. Click on the download link corresponding to your computer’s operating system. You must do this second: Download and install RStudio. Scroll down to “Installers for Supported Platforms” Click on the download link corresponding to your computer’s operating system. If you had trouble with these two steps, we suggest you watch this DataCamp video. 2.1.2 Using R via RStudio Recall our car analogy from above. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs AKA applications you can open. We will always work in RStudio and not R. In other words: R: Do not open this RStudio: Open this After you open RStudio, you should see the following: Note the three panes, which are three panels dividing the screen: The Console pane, the Files pane, and the Environment pane. Over the course of this chapter, you’ll come to learn what purpose each of these panes serve. If however you would like an in depth explanation right now however, we suggest you watch following DataCamp video. 2.2 How do I code in R? Now that you’re set up with R and RStudio, you are probably asking yourself “OK. Now how do I use R?” The first thing to note as that unlike other statistical software programs like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this book is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 2.2.1 Basic programming concepts and terminology To introduce you to many of these basic programming concepts and terminology, we direct you to the following DataCamp online interactive tutorials. For each of the tutorials, we give a list of the basic programming concepts covered. Note that in this book, we will use a different font to distinguish regular font from computer_code. It is important to note that while these tutorials serve as excellent introductions, a single pass through them is insufficient for long-term learning and retention. The ultimate tools for long-term learning and retention are “learning by doing” and repetition, something we will have you do over the course of the entire book and we encourage this process as much as possible as you learn any new skill. From the Introduction to R course complete the following chapters. As you work through the chapters, carefully note the important terms and what they are used for. We recommend you do so in a notebook that you can easily refer back to. Chapter 1 Intro to basics: Console pane: where you enter in commands Objects: where values are saved, how to assign values to objects. Data types: integers, doubles/numerics, logicals, characters. Chapter 2 Vectors: Vectors: a series of values. Chapter 4 Factors: Categorical data (as opposed to numerical data) are represented in R as factors. Chapter 5 Data frames: Data frames are analogous to rectangular spreadsheets: they are representations of datasets in R where the rows correspond observations and the columns correspond to variables that describe the observations. We will revisit this later in Section 2.4. From the Intermediate R course complete the following chapters: Chapter 1 Conditionals and Control Flow: Testing for equality in R using == (and not = which is typically used for assignment). Ex: 2 + 1 == 3 compares 2 + 1 to 3 and is correct R syntax, while 2 + 1 = 3 is not and is incorrect R syntax. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). Logical operators: &amp; representing “and”, | representing “or”. Ex: (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE while (2 + 1 == 3) | (2 + 1 == 4) returns TRUE. Chapter 3 Functions: Concept of functions: they take in inputs (called arguments) and return outputs. You either manually specify a function’s arguments or use the function’s defaults. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, we feel this is the bare minimum you need to know before you get started; the rest we feel you can learn as you go. Remember that your knowledge of all of these concepts will build as you get better and better at “speaking R” and getting used to its syntax. 2.2.2 Errors, warnings, and messages One slightly confusing part of R is how it reports errors, warnings, and messages. The default theme in RStudio colors errors, warnings, and messages in red, which makes them seem like you did something wrong. However, seeing red text in the console is not always bad. R will show red text in the console in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and try to explain what went wrong. Generally when there’s an error, the code will not run. For example, as shown in Subsection 2.3.3 below if you see Error in ggplot(...) : could not find function &quot;ggplot&quot;, it means that the ggplot() function is not accessible because the package was not loaded with library(ggplot2), and thus you cannot use it. Warnings: When the red text is a warning, it will be prefaced with “Warning:” and try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example, you see in Chapter 3 if you plot a scatterplot and one of the rows in your data frame is missing a value, you will see this warning: Warning: Removed 1 rows containing missing values (geom_point). R will still make the scatterplot with all the remaining values, but it’s warning you that one of the points isn’t there. Messages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load some packages like the dplyr package in Subsection 2.3.2 below, or when you read data saved in spreadsheet files with read_csv() as you’ll see in Chapter 5. These are helpful diagnostic messages and they don’t stop your code from working. Remember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. If the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong! If the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine. 2.2.3 Tips on learning to code Learning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. Such frustrations are very common and it is very normal to feel discouraged as you learn. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone are “smart,” but really people spent a lot of time and energy designing them to appear “smart.” Rather you have to tell a computer everything it needs to do. Furthermore the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way. Take the “copy, paste, and tweak” approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy, paste, and tweak approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. Don’t be afraid to play around! The best way to learn to code is by doing: Rather than learning to code for its own sake, we feel that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in. Practice is key: Just as the only method to improving your foreign language skills is through practice, practice, and practice; so also the only method to improving your coding is through practice, practice, and practice. Don’t worry however; we’ll give you plenty of opportunities to do so! 2.3 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a world-wide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are: The ggplot2 package for data visualization in Chapter 3. The dplyr package for data wrangling in Chapter 4. The moderndive package that accompanies this book. The infer package for “tidy” and transparent statistical inference in Chapters ??, ??, and ??. A good analogy for R packages is they are like apps you can download onto a mobile phone: R: A new phone R Packages: Apps you can download So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play. Let’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a recent photo you have taken on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set. You might do this again in the future any time there is an update to the app. Open the app: After you’ve installed Instagram, you need to open the app. Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version. “Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio. Let’s now show you how to perform these two steps for the ggplot2 package for data visualization. 2.3.1 Package installation Note about RStudio Server: If your instructor has provided you with a link and access to RStudio Server, you probably will not need to install packages, as they have likely been pre-installed for you by your instructor. That being said, it is still a good idea to know this process for later on when you are not using RStudio Server, but rather RStudio Desktop on your own computer. There are two ways to install an R package. For example, to install the ggplot2 package: Easy way: In the Files pane of RStudio: Click on the “Packages” tab Click on “Install” Type the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2 Click “Install” Slightly harder way: An alternative but slightly less convenient way to install a package is by typing install.packages(&quot;ggplot2&quot;) in the Console pane of RStudio and hitting enter. Note you must include the quotation marks. Much like an app on your phone, you only have to install a package once. However, if you want to update an already installed package to a newer verions, you need to re-install it by repeating the above steps. Learning check (LC2.1) Repeat the above installing steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for writing reports in R. 2.3.2 Package loading Recall that after you’ve installed a package, you need to “load” it, in other words open it. We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the Console pane. What do we mean by “run the following code”? Either type or copy &amp; paste the following code into the Console pane and then hit the enter key. library(ggplot2) If after running the above code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If however, you get a red “error message” that reads… Error in library(ggplot2) : there is no package called ‘ggplot2’ … it means that you didn’t successfully install it. In that case, go back to the previous subsection “Package installation” and install it. Learning check (LC2.2) “Load” the dplyr, nycflights13, and knitr packages as well by repeating the above steps. 2.3.3 Package use One extremely common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to: Error: could not find function R is telling you that you are trying to use a function in a package that has not yet been “loaded.” Almost all new users forget do this when starting out, and it is a little annoying to get used. However, you’ll remember with pratice. 2.4 Explore your first dataset Let’s put everything we’ve learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we’ll focus on datasets that are saved in “spreadsheet”-type format; this is probably the most common way data are collected and saved in many fields. Remember from Subsection 2.2.1 that these “spreadsheet”-type datasets are called data frames in R; we will focus on working with data saved as data frames throughout this book. Let’s first load all the packages needed for this chapter, assuming you’ve already installed them. Read Section 2.3 for information on how to install and load R packages if you haven’t already. library(nycflights13) library(dplyr) library(knitr) At the beginning of all subsequent chapters in this text, we’ll always have a list of packages that you should have installed and loaded to work with that chapter’s R code. 2.4.1 nycflights13 package Many of us have flown on airplanes or know someone who has. Air travel has become an ever-present aspect in many people’s lives. If you live in or are visiting a relatively large city and you walk around that city’s airport, you see gates showing flight information from many different airlines. And you will frequently see that some flights are delayed because of a variety of conditions. Are there ways that we can avoid having to deal with these flight delays? We’d all like to arrive at our destinations on time whenever possible. (Unless you secretly love hanging out at airports. If you are one of these people, pretend for the moment that you are very much anticipating being at your final destination.) Throughout this book, we’re going to analyze data related to flights contained in the nycflights13 package (Wickham 2018). Specifically, this package contains five data sets saved in five separate data frames with information about all domestic flights departing from New York City in 2013. These include Newark Liberty International (EWR), John F. Kennedy International (JFK), and LaGuardia (LGA) airports: flights: Information on all 336,776 flights airlines: A table matching airline names and their two letter IATA airline codes (also known as carrier codes) for 16 airline companies planes: Information about each of 3,322 physical aircraft used. weather: Hourly meteorological data for each of the three NYC airports. This data frame has 26,115 rows, roughtly corresponding to the 365 \\(\\times\\) 24 \\(\\times\\) 3 = 26,280 possible hourly measurements one can observe at three locations over the course of a year. airports: Airport names, codes, and locations for 1,458 destination airports. 2.4.2 flights data frame We will begin by exploring the flights data frame that is included in the nycflights13 package and getting an idea of its structure. Run the following code in your console (either by typing it or cutting &amp; pasting it): it loads in the flights dataset into your Console. Note depending on the size of your monitor, the output may vary slightly. flights # A tibble: 336,776 x 19 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 2013 1 1 517 515 2 830 819 2 2013 1 1 533 529 4 850 830 3 2013 1 1 542 540 2 923 850 4 2013 1 1 544 545 -1 1004 1022 5 2013 1 1 554 600 -6 812 837 6 2013 1 1 554 558 -4 740 728 7 2013 1 1 555 600 -5 913 854 8 2013 1 1 557 600 -3 709 723 9 2013 1 1 557 600 -3 838 846 10 2013 1 1 558 600 -2 753 745 # … with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Let’s unpack this output: A tibble: 336,776 x 19: A tibble is a kind of data frame used in R. This particular data frame has 336,776 rows 19 columns corresponding to 19 variables describing each observation year month day dep_time sched_dep_time dep_delay arr_time are different columns, in other words variables, of this data frame. We then have the first 10 rows of observations corresponding to 10 flights. ... with 336,766 more rows, and 11 more variables: indicating to us that 336,766 more rows of data and 11 more variables could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well. Let’s look at different tools to explore data frames. 2.4.3 Exploring data frames Among the many ways of getting a feel for the data contained in a data frame such as flights, we present three functions that take as their “argument”, in other words their input, the data frame in question. We also include a fourth method for exploring one particular column of a data frame: Using the View() function built for use in RStudio. We will use this the most. Using the glimpse() function, which is included in the dplyr package. Using the kable() function, which is included in the knitr package. Using the $ operator to view a single variable in a data frame. 1. View(): Run View(flights) in your Console in RStudio, either by typing it or cutting &amp; pasting it into the Console pane, and explore this data frame in the resulting pop-up viewer. You should get into the habit of always Viewing any data frames that come your way. Note the capital “V” in View. R is case-sensitive so you’ll receive an error is you run view(flights) instead of View(flights). Learning check (LC2.3) What does any ONE row in this flights dataset refer to? A. Data on an airline B. Data on a flight C. Data on an airport D. Data on multiple flights By running View(flights), we see the different variables listed in the columns and we see that there are different types of variables. Some of the variables like distance, day, and arr_delay are what we will call quantitative variables. These variables are numerical in nature. Other variables here are categorical. Note that if you look in the leftmost column of the View(flights) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row corresponds to. In other words, this will allow you to identify what object is being referred to in a given row. This is often called the observational unit. The observational unit in this example is an individual flight departing New York City in 2013. You can identify the observational unit by determining what “thing” is being measured or described by each of the variables. 2. glimpse(): The second way to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package. This function provides us with an alternative method for exploring a data frame than the View() function: glimpse(flights) Observations: 336,776 Variables: 19 $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, … $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558,… $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600,… $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -… $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849… $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851… $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -… $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;, … $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, … $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N39… $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LGA&quot;… $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IAD&quot;… $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, … $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733,… $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, … $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, … $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 … We see that glimpse() will give you the first few entries of each variable in a row after the variable. In addition, the data type (see Subsection 2.2.1) of the variable is given immediately after each variable’s name inside &lt; &gt;. Here, int and dbl refer to “integer” and “double”, which are computer coding terminology for quantitative/numerical variables. In contrast, chr refers to “character”, which is computer terminology for text data. Text data, such as the carrier or origin of a flight, are categorical variables. The time_hour variable is an example of one more type of data type: dttm. As you may suspect, this variable corresponds to a specific date and time of day. However, we won’t work with dates in this class and leave it to a more advanced book on data science. Learning check (LC2.4) What are some examples in this dataset of categorical variables? What makes them different than quantitative variables? 3. kable(): The final way to explore the entirety of a data frame is using the kable() function from the knitr package. Let’s explore the different carrier codes for all the airlines in our dataset two ways. Run both of these lines of code in your Console: airlines kable(airlines) At first glance, it may not appear that there is much difference in the outputs. However when using tools for document production such as R Markdown, the latter code produces output that is much more legible and reader-friendly. 4. $ operator Lastly, the $ operator allows us to explore a single variable within a data frame. For example, run the following in your console airlines airlines$name We used the $ operator to extract only the name variable and return it as a vector of length 16. We will only be occasionally exploring data frames using this operator, instead favoring the View() and glimpse() functions. 2.4.4 Help files Another nice feature of R is the help system. You can get help in R by entering a ? before the name of a function or data frame in question and you will be presented with a page showing the documentation. For example, let’s look at the help file for the flights data frame: ?flights A help file should pop-up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away. 2.5 Conclusion We’ve given you what we feel are the most essential concepts to know before you can start exploring data in R. Is this chapter exhaustive? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn’t be useful! 2.5.1 Additional resources If you are completely new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out ModernDive co-author Chester Ismay’s Getting used to R, RStudio, and R Markdown short book (Ismay 2016), which includes screencast recordings that you can follow along and pause as you learn. Furthermore, there is an introduction to R Markdown, a tool used for reproducible research in R. 2.5.2 What’s to come? As we stated earlier however, the best way to learn R is to learn by doing. We now start the “data science” portion of the book in Chapter 3 with what we feel is the most important tool in a data scientist’s toolbox: data visualization. We will continue to explore the data included in the nycflights13 package through data visualization. We’ll see that data visualization is a powerful tool to add to our toolbox for data exploring that provides additional insight to what the View() and glimpse() functions can provide. FIGURE 2.1: ModernDive flowchart References "],
["3-viz.html", "Chapter 3 Data Visualization 3.1 The Grammar of Graphics 3.2 Five Named Graphs - The 5NG 3.3 5NG#1: Scatterplots 3.4 5NG#2: Linegraphs 3.5 5NG#3: Histograms 3.6 Facets 3.7 5NG#4: Boxplots 3.8 5NG#5: Barplots 3.9 Conclusion", " Chapter 3 Data Visualization We begin the development of your data science toolbox with data visualization. By visualizing our data, we gain valuable insights that we couldn’t initially see from just looking at the raw data in spreadsheet form. We will use the ggplot2 package as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as The Grammar of Graphics (Wilkinson 2005). At the most basic level, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way for us to get a sense for how quantitative variables compare in terms of their center (where the values tend to be located) and their spread (how they vary around the center). Graphics should be designed to emphasize the findings and insight you want your audience to understand. This does however require a balancing act. On the one hand, you want to highlight as many meaningful relationships and interesting findings as possible; on the other you don’t want to include so many as to overwhelm your audience. As we will see, plots/graphics also help us to identify patterns and outliers in our data. We will see that a common extension of these ideas is to compare the distribution of one quantitative variable (i.e., what the spread of a variable looks like or how the variable is distributed in terms of its values) as we go across the levels of a different categorical variable. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Read Section 2.3 for information on how to install and load R packages. library(nycflights13) library(ggplot2) library(dplyr) 3.1 The Grammar of Graphics We begin with a discussion of a theoretical framework for data visualization known as “The Grammar of Graphics,” which serves as the foundation for the ggplot2 package. Think of how we construct sentences in English to form sentences by combining different elements, like nouns, verbs, particles, subjects, objects, etc. However, we can’t just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, “The Grammar of Graphics” define a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2005) and has been implemented in a variety of data visualization software including R. 3.1.1 Components of the Grammar In short, the grammar tells us that: A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. Specifically, we can break a graphic into the following three essential components: data: the data set composed of variables that we map. geom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars. aes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Each assigned aesthetic attribute can be mapped to a variable in our data set. You might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example. 3.1.2 Gapminder data In February 2006, a statistician named Hans Rosling gave a TED talk titled “The best stats you’ve ever seen” where he presented global economic, health, and development data from the website gapminder.org. For example, for the 142 countries included from 2007, let’s consider only the first 6 countries when listed alphabetically in Table 3.1. TABLE 3.1: Gapminder 2007 Data: First 6 of 142 countries Country Continent Life Expectancy Population GDP per Capita Afghanistan Asia 43.8 31889923 975 Albania Europe 76.4 3600523 5937 Algeria Africa 72.3 33333216 6223 Angola Africa 42.7 12420476 4797 Argentina Americas 75.3 40301927 12779 Australia Oceania 81.2 20434176 34435 Each row in this table corresponds to a country in 2007. For each row, we have 5 columns: Country: Name of country. Continent: Which of the five continents the country is part of. (Note that “Americas” includes countries in both North and South America and that Antarctica is excluded.) Life Expectancy: Life expectancy in years. Population: Number of people living in the country. GDP per Capita: Gross domestic product (in US dollars). Now consider Figure 3.1, which plots this data for all 142 countries in the data. FIGURE 3.1: Life Expectancy over GDP per Capita in 2007 Let’s view this plot through the grammar of graphics: The data variable GDP per Capita gets mapped to the x-position aesthetic of the points. The data variable Life Expectancy gets mapped to the y-position aesthetic of the points. The data variable Population gets mapped to the size aesthetic of the points. The data variable Continent gets mapped to the color aesthetic of the points. We’ll see shortly that data corresponds to the particular data frame where our data is saved and a “data variable” corresponds to a particular column in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. Other plots involve lines while others involve bars. Let’s summarize the three essential components of the Grammar in Table 3.2. TABLE 3.2: Summary of Grammar of Graphics for this plot data variable aes geom GDP per Capita x point Life Expectancy y point Population size point Continent color point 3.1.3 Other components There are other components of the Grammar of Graphics we can control as well. As you start to delve deeper into the Grammar of Graphics, you’ll start to encounter these topics more frequently. In this book however, we’ll keep things simple and only work with the two additional components listed below: faceting breaks up a plot into small multiples corresponding to the levels of another variable (Section 3.6) position adjustments for barplots (Section 3.8) Other more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (Grolemund and Wickham 2016). Generally speaking, the Grammar of Graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them. 3.1.4 ggplot2 package In this book, we will be using the ggplot2 package for data visualization, which is an implementation of the Grammar of Graphics for R (Wickham et al. 2019). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the Grammar of Graphics are specified in the ggplot() function included in the ggplot2 package, which expects at a minimum as arguments (i.e. inputs): The data frame where the variables exist: the data argument. The mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved. After we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include layers specifying the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 3.6). Let’s now put the theory of the Grammar of Graphics into practice. 3.2 Five Named Graphs - The 5NG In order to keep things simple, we will only five different types of graphics in this book, each with a commonly given name. We term these “five named graphs” the 5NG: scatterplots linegraphs boxplots histograms barplots We will discuss some variations of these plots, but with this basic repertoire of graphics in your toolbox you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables and while others are only appropriate for quantitative variables. You’ll want to quiz yourself often as we go along on which plot makes sense a given a particular problem or data set. 3.3 5NG#1: Scatterplots The simplest of the 5NG are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the Grammar of Graphics. Specifically, we will visualize the relationship between the following two numerical variables in the flights data frame included in the nycflights13 package: dep_delay: departure delay on the horizontal “x” axis and arr_delay: arrival delay on the vertical “y” axis for Alaska Airlines flights leaving NYC in 2013. This requires paring down the data from all 336,776 flights that left NYC in 2013, to only the 714 Alaska Airlines flights that left NYC in 2013. What this means computationally is: we’ll take the flights data frame, extract only the 714 rows corresponding to Alaska Airlines flights, and save this in a new data frame called alaska_flights. Run the code below to do this: alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) For now we suggest you ignore how this code works; we’ll explain this in detail in Chapter 4 when we cover data wrangling. However, convince yourself that this code does what it is supposed to by running View(alaska_flights): it creates a new data frame alaska_flights consisting of only the 714 Alaska Airlines flights. We’ll see later in Chapter 4 on data wrangling that this code uses the dplyr package for data wrangling to achieve our goal: it takes the flights data frame and filters it to only return the rows where carrier is equal to &quot;AS&quot;, Alaska Airlines’ carrier code. Other examples of carrier codes include “AA” for American Airlines and “UA” for United Airlines. Recall from Section 2.2 that testing for equality is specified with == and not =. Fasten your seat belts and sit tight for now however, we’ll introduce these ideas more fully in Chapter 4. Learning check (LC3.1) Take a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights). In what respect do these data frames differ? 3.3.1 Scatterplots via geom_point Let’s now go over the code that will create the desired scatterplot, keeping in mind our discussion on the Grammar of Graphics in Section 3.1. We’ll be using the ggplot() function included in the ggplot2 package. ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point() Let’s break this down piece-by-piece: Within the ggplot() function, we specify two of the components of the Grammar of Graphics as arguments (i.e. inputs): The data frame to be alaska_flights by setting data = alaska_flights. The aesthetic mapping by setting aes(x = dep_delay, y = arr_delay). Specifically: the variable dep_delay maps to the x position aesthetic the variable arr_delay maps to the y position aesthetic We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object. In this case the geometric object are points, set by specifying geom_point(). After running the above code, you’ll notice two outputs: a warning message and the graphic shown in Figure 3.2. Let’s first unpack the warning message: Warning: Removed 5 rows containing missing values (geom_point). FIGURE 3.2: Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013 After running the above code, R returns a warning message alerting us to the fact that 5 rows were ignored due to them being missing. For 5 rows either the value for dep_delay or arr_delay or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Turning our attention to the resulting scatterplot in Figure 3.2, we see that a positive relationship exists between dep_delay and arr_delay: as departure delays increase, arrival delays tend to also increase. We also note the large mass of points clustered near (0, 0). Before we continue, let’s consider a few more notes on the layers in the above code that generated the scatterplot: Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning. When adding layers to a plot, you are encouraged to start a new line after the + so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code. To stress the importance of adding layers in particular the layer specifying the geometric object, consider Figure 3.3 where no layers are added. A not very useful plot! ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) FIGURE 3.3: Plot with No Layers Learning check (LC3.2) What are some practical reasons why dep_delay and arr_delay have a positive relationship? (LC3.3) What variables (not necessarily in the flights data frame) would you expect to have a negative correlation (i.e. a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here. (LC3.4) Why do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaskan flights? (LC3.5) What are some other features of the plot that stand out to you? (LC3.6) Create a new scatterplot using different variables in the alaska_flights data frame by modifying the example above. 3.3.2 Over-plotting The large mass of points near (0, 0) in Figure 3.2 can cause some confusion as it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to values being plotted on top of each other over and over again. It is often difficult to know just how many values are plotted in this way when looking at a basic scatterplot as we have here. There are two methods to address the issue of overplotting: By adjusting the transparency of the points. By adding a little random “jitter”, or random “nudges”, to each of the points. Method 1: Changing the transparency The first way of addressing overplotting is by changing the transparency of the points by using the alpha argument in geom_point(). By default, this value is set to 1. We can change this to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. Note how the following code is identical to the code in Section 3.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point(): ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point(alpha = 0.2) FIGURE 3.4: Delay scatterplot with alpha=0.2 The key feature to note in Figure 3.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line above to read geom_point(aes(alpha = 0.2)). Method 2: Jittering the points The second way of addressing overplotting is by jittering all the points, in other words give each point a small nudge in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame jitter_example with 4 rows of identical value 0 for both x and y: # A tibble: 4 x 2 x y &lt;dbl&gt; &lt;dbl&gt; 1 0 0 2 0 0 3 0 0 4 0 0 We display the resulting scatterplot in Figure 3.5; observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. FIGURE 3.5: Regular scatterplot of jitter example data In Figure 3.6 we instead display a jittered scatterplot where each point is given a random “nudge.” It is now plainly evident that this plot involves four points. Keep in mind that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in jitter_example remain unchanged. FIGURE 3.6: Jittered scatterplot of jitter example data To create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). To specify how much jitter to add, we adjust the width and height arguments. This corresponds to how hard you’d like to shake the plot in units corresponding to those for both the horizontal and vertical variables (in this case minutes). ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_jitter(width = 30, height = 30) FIGURE 3.7: Jittered delay scatterplot Observe how the above code is identical to the code that created the scatterplot with overplotting in Subsection 3.3.1, but with geom_point() replaced with geom_jitter(). The resulting plot in Figure 3.7 helps us a little bit in getting a sense for the overplotting, but with a relatively large data set like this one (714 flights), it can be argued that changing the transparency of the points by setting alpha proved more effective. In terms of how much jitter one should add using the width and height arguments, it is important to add just enough jitter to break any overlap in points, but not so much that we completely alter the overall pattern in points. Learning check (LC3.7) Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? (LC3.8) After viewing the Figure 3.4 above, give an approximate range of arrival delays and departure delays that occur the most frequently. How has that region changed compared to when you observed the same plot without the alpha = 0.2 set in Figure 3.2? 3.3.3 Summary Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful! With medium to large data sets, you may need to play around with the different modifications one can make to a scatterplot. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships come about as you make subtle changes to your plots. 3.4 5NG#2: Linegraphs The next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature; in other words there is an inherent ordering to the variable. The most common example of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots. Linegraphs should be avoided when there is not a clear sequential ordering to the variable on the x-axis. Let’s illustrate linegraphs using another data set in the nycflights13 package: the weather data frame. Let’s get a sense for the weather data frame: Explore the weather data by running View(weather). Run ?weather to bring up the help file. We can see that there is a variable called temp of hourly temperature recordings in Fahrenheit at weather stations near all three airports in New York City: Newark (origin code EWR), JFK, and La Guardia (LGA). Instead of considering hourly temperatures for all days in 2013 for all three airports however, for simplicity let’s only consider hourly temperatures at only Newark airport for the first 15 days in January. Recall in Section 3.3 we used the filter() function to only choose the subset of rows of flights corresponding to Alaska Airlines flights. We similarly use filter() here, but by using the &amp; operator we only choose the subset of rows of weather where The origin is &quot;EWR&quot; and the month is January and the day is between 1 and 15 early_january_weather &lt;- weather %&gt;% filter(origin == &quot;EWR&quot; &amp; month == 1 &amp; day &lt;= 15) Learning check (LC3.9) Take a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather). In what respect do these data frames differ? (LC3.10) View() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement whereas the hour variable does not? 3.4.1 Linegraphs via geom_line Let’s plot a linegraph of hourly temperatures in early_january_weather by using geom_line() instead of geom_point() like we did for scatterplots: ggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) + geom_line() FIGURE 3.8: Hourly Temperature in Newark for January 1-15, 2013 Much as with the ggplot() code that created the scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 3.2, let’s break down the above code piece-by-piece in terms of the Grammar of Graphics: Within the ggplot() function call, we specify two of the components of the Grammar of Graphics as arguments: The data frame to be early_january_weather by setting data = early_january_weather The aesthetic mapping by setting aes(x = time_hour, y = temp). Specifically: the variable time_hour maps to the x position aesthetic. the variable temp maps to the y position aesthetic We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case the geometric object is a line, set by specifying geom_line(). Learning check (LC3.11) Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis? (LC3.12) Why are linegraphs frequently used when time is the explanatory variable on the x-axis? (LC3.13) Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013. 3.4.2 Summary Linegraphs, just like scatterplots, display the relationship between two numerical variables. However it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e. the explanatory variable) has an inherent ordering, like some notion of time. 3.5 5NG#3: Histograms Let’s consider the temp variable in the weather data frame once again, but unlike with the linegraphs in Section 3.4, let’s say we don’t care about the relationship of temperature to time, but rather we only care about how the values of temp distribute. In other words: What are the smallest and largest values? What is the “center” value? How do the values spread out? What are frequent and infrequent values? One way to visualize this distribution of this single variable temp is to plot them on a horizontal line as we do in Figure 3.9: FIGURE 3.9: Plot of Hourly Temperature Recordings from NYC in 2013 This gives us a general idea of how the values of temp distribute: observe that temperatures vary from around 11°F up to 100°F. Furthermore, there appear to be more recorded temperatures between 40°F and 60°F than outside this range. However, because of the high degree of overlap in the points, it’s hard to get a sense of exactly how many values are between, say, 50°F and 55°F. What is commonly produced instead of the above plot is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows: We first cut up the x-axis into a series of bins, where each bin represents a range of values. For each bin, we count the number of observations that fall in the range corresponding to that bin. Then for each bin, we draw a bar whose height marks the corresponding count. Let’s drill-down on an example of a histogram, shown in Figure 3.10. FIGURE 3.10: Example histogram. Observe that there are three bins of equal width between 30°F and 60°F, thus we have three bins of width 10°F each: one bin for the 30-40°F range, another bin for the 40-50°F range, and another bin for the 50-60°F range. Since: The bin for the 30-40°F range has a height of around 5000, this histogram is telling us that around 5000 of the hourly temperature recordings are between 30°F and 40°F. The bin for the 40-50°F range has a height of around 4300, this histogram is telling us that around 4300 of the hourly temperature recordings are between 40°F and 50°F. The bin for the 50-60°F range has a height of around 3500, this histogram is telling us that around 3500 of the hourly temperature recordings are between 50°F and 60°F. The remaining bins all have a similar interpretation. 3.5.1 Histograms via geom_histogram Let’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram() ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Warning: Removed 1 rows containing non-finite values (stat_bin). FIGURE 3.11: Histogram of hourly temperatures at three NYC airports. Let’s unpack the messages R sent us first. The first message is telling us that the histogram was constructed using bins = 30, in other words 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We’ll see in the next section how to change this default number of bins. The second message is telling us something similar to the warning message we received when we ran the code to create a scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 3.2: that because one row has a missing NA value for temp, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case. Now’s let’s unpack the resulting histogram in Figure 3.11. Observe that values less than 25°F as well as values above 80°F are rather rare. However, because of the large number of bins, its hard to get a sense for which range of temperatures is covered by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = &quot;white&quot; argument to geom_histogram(): ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(color = &quot;white&quot;) FIGURE 3.12: Histogram of hourly temperatures at three NYC airports with white borders. We can now better associate ranges of temperatures to each of the bins. We can also vary the color of the bars by setting the fill argument. Run colors() to see all 657 possible choice of colors! ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;) FIGURE 3.13: Histogram of hourly temperatures at three NYC airports with white borders. 3.5.2 Adjusting the bins Observe in both Figure 3.12 and Figure 3.13 that in the 50-75°F range there appear to be roughly 8 bins. Thus each bin has width 25 divided by 8, or roughly 3.12°F which is not a very easily interpretable range to work with. Let’s now adjust the number of bins in our histogram in one of two methods: By adjusting the number of bins via the bins argument to geom_histogram(). By adjusting the width of the bins via the binwidth argument to geom_histogram(). Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows: ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(bins = 40, color = &quot;white&quot;) FIGURE 3.14: Histogram with 40 bins. Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 10°F. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) FIGURE 3.15: Histogram with binwidth 10. Learning check (LC3.14) What does changing the number of bins from 30 to 40 tell us about the distribution of temperatures? (LC3.15) Would you classify the distribution of temperatures as symmetric or skewed? (LC3.16) What would you guess is the “center” value in this distribution? Why did you make that choice? (LC3.17) Is this data spread out greatly from the center or is it close? Why? 3.5.3 Summary Histograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question. 3.6 Facets Before continuing the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization of variables by another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ. For example, suppose we were interested in looking at how the histogram of hourly temperature recordings at the three NYC airports we saw in Section 3.5 differed by month. We would “split” this histogram by the 12 possible months in a given year, in other words plot histograms of temp for each month. We do this by adding facet_wrap(~ month) layer. ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + facet_wrap(~ month) FIGURE 3.16: Faceted histogram. Note the use of the tilde ~ before month in facet_wrap(). The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'month' not found if you don’t include it before month here. We can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted plot to have 4 rows instead of 3. Add the nrow = 4 argument to facet_wrap(~ month) ggplot(data = weather, mapping = aes(x = temp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + facet_wrap(~ month, nrow = 4) FIGURE 3.17: Faceted histogram with 4 instead of 3 rows. Observe in both Figure 3.16 and Figure 3.17 that as we might expect in the Northern Hemisphere, temperatures tend to be higher in the summer months, while they tend to be lower in the winter. Learning check (LC3.18) What other things do you notice about the faceted plot above? How does a faceted plot help us see relationships between two variables? (LC3.19) What do the numbers 1-12 correspond to in the plot above? What about 25, 50, 75, 100? (LC3.20) For which types of data sets would these types of faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics. (LC3.21) Does the temp variable in the weather data set have a lot of variability? Why do you say that? 3.7 5NG#4: Boxplots While faceted histograms are one visualization that allows us to compare distributions of a numerical variable split by another variable, another visualization that achieves this same goal are side-by-side boxplots. A boxplot is constructed from the information provided in the five-number summary of a numerical variable (see Appendix A). To keep things simple for now, let’s only consider hourly temperature recordings for the month of November in Figure 3.18. FIGURE 3.18: November temperatures. These 2141 observations have the following five-number summary: Minimum: 21.02°F First quartile AKA 25th percentile: 35.96°F Median AKA second quartile AKA 50th percentile: 44.96°F Third quartile AKA 75th percentile: 51.98°F Maximum: 71.06°F Let’s mark these 5 values with dashed horizontal lines in Figure 3.19. FIGURE 3.19: November temperatures. Let’s add the boxplot underneath these points and dashed horizontal lines in Figure 3.20. FIGURE 3.20: November temperatures. What the boxplot does summarize the 2141 points by emphasizing that: 25% of points (about 534 observations) fall below the bottom edge of the box, which is the first quartile of 35.96°F. In other words 25% of observations were colder than 35.96°F. 25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 44.96°F. In other words 25% of observations were between 35.96 and 44.96°F and 50% of observations were colder than 44.96°F. 25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 51.98°F. In other words 25% of observations were between 44.96 and 51.98°F and 75% of observations were colder than 51.98°F. 25% of points fall over the top edge of the box. In other words 25% of observations were warmer than 51.98°F. The middle 50% of points lie within the interquartile range between the first and third quartile of 51.98 - 35.96 = 16.02°F. Lastly, for clarity’s sake let’s remove the points but keep the dashed horizontal lines in Figure 3.21. FIGURE 3.21: November temperatures. We can now better see the whiskers of the boxplot. They stick out from either end of the box all the way to the minimum and maximum observed temperatures of 21.02°F and 71.06°F respectively. However, the whiskers don’t always extend to the smallest and largest observed values. They in fact can extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box, in this case 1.5 \\(\\times\\) 16.02°F = 24.03°F from either end of the box. Any observed values outside this whiskers get marked with points called outliers, which we’ll see in the next section. 3.7.1 Boxplots via geom_boxplot Let’s now create a side-by-side boxplot of hourly temperatures split by the 12 months as we did above with the faceted histograms. We do this by mapping the month variable to the x-position aesthetic, the temp variable to the y-position aesthetic, and by adding a geom_boxplot() layer: ggplot(data = weather, mapping = aes(x = month, y = temp)) + geom_boxplot() FIGURE 3.22: Invalid boxplot specification Warning messages: 1: Continuous x aesthetic -- did you forget aes(group=...)? 2: Removed 1 rows containing non-finite values (stat_boxplot). Observe in Figure 3.22 that this plot does not provide information about temperature separated by month. The warning messages clue us in as to why. The second warning message is identical to the warning message when plotting a histogram of hourly temperatures: that one of the values was recorded as NA missing. However, the first warning message is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots however require a categorical variable on the x-axis. We can convert the numerical variable month into a categorical variable by using the factor() function. So after applying factor(month), month goes from having numerical values 1, 2, …, 12 to having labels “1”, “2”, …, “12.” ggplot(data = weather, mapping = aes(x = factor(month), y = temp)) + geom_boxplot() FIGURE 3.23: Month by temp boxplot The resulting Figure 3.23 shows 12 separate “box and whiskers” plots with the features we saw earlier focusing only on November: The “box” portions of this visualization represent the 1st quartile, the median AKA the 2nd quartile, and the 3rd quartile. The height of each box, i.e. the value of the 3rd quartile minus the value of the 1st quartile, is the interquartile range. It is a measure of spread of the middle 50% of values, with longer boxes indicating more variability. The “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed temperatures. The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability. The dots representing values falling outside the whiskers are called outliers. These can be thought of as anomalous values. It is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long. Looking at this plot we can see, as expected, that summer months (6 through 8) have higher median temperatures as evidenced by the higher solid lines in the middle of the boxes. We can easily compare temperatures across months by drawing imaginary horizontal lines across the plot. Furthermore, the height of the 12 boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of temperatures recorded in a given month. Learning check (LC3.22) What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point. (LC3.23) Which months have the highest variability in temperature? What reasons can you give for this? (LC3.24) We looked at the distribution of the numerical variable temp split by the numerical variable month that we converted to a categorical variable using the factor() function. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative? (LC3.25) Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram? 3.7.2 Summary Side-by-side boxplots provide us with a way to compare and contrast the distribution of a quantitative variable across multiple levels of another categorical variable. One can see where the median falls across the different groups by looking at the center line in the boxes. To see how spread out the variable is across the different groups, look at both the width of the box and also how far the whiskers stretch out away from the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with points. 3.8 5NG#5: Barplots Both histograms and boxplots are tools to visualize the distribution of numerical variables. Another common task is visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories, also known as levels, of a categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot (also known as a barchart). One complication, however, is how your data is represented: is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges. fruits &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;apple&quot;, &quot;orange&quot;) ) fruits_counted &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;orange&quot;), number = c(3, 2) ) We see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually… # A tibble: 5 x 1 fruit &lt;chr&gt; 1 apple 2 apple 3 orange 4 apple 5 orange … fruits_counted has a variable count which represents pre-counted values of each fruit. # A tibble: 2 x 2 fruit number &lt;chr&gt; &lt;dbl&gt; 1 apple 3 2 orange 2 Depending on how your categorical data is represented, you’ll need to use add a different geom layer to your ggplot() to create a barplot, as we now explore. 3.8.1 Barplots via geom_bar or geom_col Let’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer. ggplot(data = fruits, mapping = aes(x = fruit)) + geom_bar() FIGURE 3.24: Barplot when counts are not pre-counted However, using the fruits_counted data frame where the fruit have been “pre-counted”, we map the fruit variable to the x-position aesthetic as with geom_bar(), but we also map the count variable to the y-position aesthetic, and add a geom_col() layer. ggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) + geom_col() FIGURE 3.25: Barplot when counts are pre-counted Compare the barplots in Figures 3.24 and 3.25. They are identical because they reflect count of the same 5 fruit. However depending on how our data is saved, either pre-counted or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize is: Is not pre-counted in your data frame: use geom_bar(). Is pre-counted in your data frame, use geom_col() with the y-position aesthetic mapped to the variable that has the counts. Let’s now go back to the flights data frame in the nycflights13 package and visualize the distribution of the categorical variable carrier. In other words, let’s visualize the number of domestic flights out of the three New York City airports each airline company flew in 2013. Recall from Section 2.4.3 when you first explored the flights data frame you saw that each row corresponds to a flight. In other words the flights data frame is more like the fruits data frame than the fruits_counted data frame above, and thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable carrier gets mapped to the x-position. ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() FIGURE 3.26: Number of flights departing NYC in 2013 by airline using geom_bar Observe in Figure 3.26 that United Air Lines (UA), JetBlue Airways (B6), and ExpressJet Airlines (EV) had the most flights depart New York City in 2013. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example: AA is American Airlines; B6 is JetBlue Airways; DL is Delta Airlines; EV is ExpressJet Airlines; MQ is Envoy Air; while UA is United Airlines. Alternatively, say you had a data frame flights_counted where the number of flights for each carrier was pre-counted like in Table 3.3. TABLE 3.3: Number of flights pre-counted for each carrier. carrier number 9E 18460 AA 32729 AS 714 B6 54635 DL 48110 EV 54173 F9 685 FL 3260 HA 342 MQ 26397 OO 32 UA 58665 US 20536 VX 5162 WN 12275 YV 601 In order to create a barplot visualizing the distribution of the categorical variable carrier in this case, we would use geom_col() instead with x mapped to carrier and y mapped to number as seen below. The resulting barplot would be identical to Figure 3.26. ggplot(data = flights_table, mapping = aes(x = carrier, y = number)) + geom_col() Learning check (LC3.26) Why are histograms inappropriate for visualizing categorical variables? (LC3.27) What is the difference between histograms and barplots? (LC3.28) How many Envoy Air flights departed NYC in 2013? (LC3.29) What was the seventh highest airline in terms of departed flights from NYC in 2013? How could we better present the table to get this answer quickly? 3.8.2 Must avoid pie charts! Unfortunately, one of the most common plots seen today for categorical data is the pie chart. While they may seem harmless enough, they actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book “Creating More Effective Graphs” (Robbins 2013), we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine relative size of one piece of the pie compared to another. Let’s examine the same data used in our previous barplot of the number of flights departing NYC by airline in Figure 3.26, but this time we will use a pie chart in Figure 3.27. FIGURE 3.27: The dreaded pie chart Try to answer the following questions: How much larger the portion of the pie is for ExpressJet Airlines (EV) compared to US Airways (US), What the third largest carrier is in terms of departing flights, and How many carriers have fewer flights than United Airlines (UA)? While it is quite difficult to answer these questions when looking at the pie chart in Figure 3.27, we can much more easily answer these questions using the barchart in Figure 3.26. This is true since barplots present the information in a way such that comparisons between categories can be made with single horizontal lines, whereas pie charts present the information in a way such that comparisons between categories must be made by comparing angles. There may be one exception of a pie chart not to avoid courtesy Nathan Yau at FlowingData.com, but we will leave this for the reader to decide: FIGURE 3.28: The only good pie chart Learning check (LC3.30) Why should pie charts be avoided and replaced by barplots? (LC3.31) Why do you think people continue to use pie charts? 3.8.3 Two categorical variables Barplots are the go-to way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of outgoing domestic flights from NYC by carrier and origin, or in other words the number of flights for each carrier and origin combination. For example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure 3.26: ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() We can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping; the fill aesthetic of any bar corresponds to the color used to fill the bars. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar() FIGURE 3.29: Stacked barplot comparing the number of flights by carrier and origin. Figure 3.29 is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of flights from each origin airport between the carriers. Before we continue, let’s address some common points of confusion amongst new R users. First, note that fill is another aesthetic mapping much like x-position; thus it must be included within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make: ggplot(data = flights, mapping = aes(x = carrier), fill = origin) + geom_bar() Second, the fill aesthetic corresponds to the color used to fill the bars, while the color aesthetic corresponds to the color of the outline of the bars. Observe in Figure 3.30 that mapping origin to color and not fill yields grey bars with different colored outlines. ggplot(data = flights, mapping = aes(x = carrier, color = origin)) + geom_bar() FIGURE 3.30: Stacked barplot with color aesthetic used instead of fill. Learning check (LC3.32) What kinds of questions are not easily answered by looking at the above figure? (LC3.33) What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights? Another alternative to stacked barplots are side-by-side barplots, also known as a dodged barplot. The code to created a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = &quot;dodge&quot; argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot. ggplot(data = flights, mapping = aes(x = carrier, fill = origin)) + geom_bar(position = &quot;dodge&quot;) FIGURE 3.31: Side-by-side AKA dodged barplot comparing the number of flights by carrier and origin. Learning check (LC3.34) Why might the side-by-side (AKA dodged) barplot be preferable to a stacked barplot in this case? (LC3.35) What are the disadvantages of using a side-by-side (AKA dodged) barplot, in general? Lastly, another type of barplot is a faceted barplot. Recall in Section 3.6 we visualized the distribution of hourly temperatures at the 3 NYC airports split by month using facets. We apply the same principle to our barplot visualizing the frequency of carrier split by origin: instead of mapping origin ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() + facet_wrap(~ origin, ncol = 1) FIGURE 3.32: Faceted barplot comparing the number of flights by carrier and origin. Learning check (LC3.36) Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case? (LC3.37) What information about the different carriers at different airports is more easily seen in the faceted barplot? 3.8.4 Summary Barplots are the preferred way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories called levels occur. They are easy to understand and make it easy to make comparisons across levels. When trying to visualize two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the joint distribution you are trying to emphasize, you will need to make a choice between these three types of barplots. 3.9 Conclusion 3.9.1 Summary table Let’s recap all five of the Five Named Graphs (5NG) in Table 3.4 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package. TABLE 3.4: Summary of 5NG Named graph Shows Geometric object Notes 1 Scatterplot Relationship between 2 numerical variables geom_point() 2 Linegraph Relationship between 2 numerical variables geom_line() Used when there is a sequential order to x-variable e.g. time 3 Histogram Distribution of 1 numerical variable geom_histogram() Facetted histograms show the distribution of 1 numerical variable split by the values of another variable 4 Boxplot Distribution of 1 numerical variable split by the values of another variable geom_boxplot() 5 Barplot Distribution of 1 categorical variable geom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted Stacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables 3.9.2 Argument specification Run the following two segments of code. First this: ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() then this: ggplot(flights, aes(x = carrier)) + geom_bar() You’ll notice that that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() by default assumes that the data argument comes first and the mapping argument comes second. So as long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. Going forward for the rest of this book, all ggplot() will be like the second segment above: with the data = and mapping = explicit naming of the argument omitted and the default ordering of arguments respected. 3.9.3 Additional resources An R script file of all R code used in this chapter is available here. If you want to further unlock the power of the ggplot2 package for data visualization, we suggest you that you check out RStudio’s “Data Visualization with ggplot2” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular the many more than the 5 geom geometric objects we covered in this Chapter, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Visualization with ggplot2”: FIGURE 3.33: Data Visualization with ggplot2 cheatsheat 3.9.4 What’s to come Recall in Figure 3.2 in Section 3.3 we visualized the relationship between departure delay and arrival delay for Alaska Airlines flights. This necessitated paring down the flights data frame to a new data frame alaska_flights consisting of only carrier == AS flights first: alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point() Furthermore recall in Figure 3.8 in Section 3.4 we visualized hourly temperature recordings at Newark airport only for the first 15 days of January 2013. This necessitated paring down the weather data frame to a new data frame early_january_weather consisting of hourly temperature recordings only for origin == &quot;EWR&quot;, month == 1, and day less than or equal to 15 first: early_january_weather &lt;- weather %&gt;% filter(origin == &quot;EWR&quot; &amp; month == 1 &amp; day &lt;= 15) ggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) + geom_line() These two code segments were a preview of Chapter 4 on data wrangling where we’ll delve further into the dplyr package. Data wrangling is the process of transforming and modifying existing data with the intent of making it more appropriate for analysis purposes. For example, the two code segments used the filter() function to create new data frames (alaska_flights and early_january_weather) by choosing only a subset of rows of existing data frames (flights and weather). In this next chapter, we’ll formally introduce the filter() and other data wrangling functions as well as the pipe operator %&gt;% which allows you to combine multiple data wrangling actions into a single sequential chain of actions. On to Chapter 4 on data wrangling! References "],
["4-wrangling.html", "Chapter 4 Data Wrangling 4.1 The pipe operator: %&gt;% 4.2 filter rows 4.3 summarize variables 4.4 group_by rows 4.5 mutate existing variables 4.6 arrange and sort rows 4.7 join data frames 4.8 Other verbs 4.9 Conclusion", " Chapter 4 Data Wrangling So far in our journey, we’ve seen how to look at data saved in data frames using the glimpse() and View() functions in Chapter 2 on and how to create data visualizations using the ggplot2 package in Chapter 3. In particular we studied what we term the “five named graphs” (5NG): scatterplots via geom_point() linegraphs via geom_line() boxplots via geom_boxplot() histograms via geom_histogram() barplots via geom_bar() or geom_col() We created these visualizations using the “Grammar of Graphics”, which maps variables in a data frame to the aesthetic attributes of one the above 5 geometric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure 3.1. Recall however in Section 3.9.4 we discussed that for two of our visualizations we needed transformed/modified versions of existing data frames. Recall for example the scatterplot of departure and arrival delay only for Alaska Airlines flights. In order to create this visualization, we needed to first pare down the flights data frame to a new data frame alaska_flights consisting of only carrier == &quot;AS&quot; flights using the filter() function. alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) ggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + geom_point() In this chapter, we’ll introduce a series of functions from the dplyr package that will allow you to take a data frame and filter() its existing rows to only pick out a subset of them. For example, the alaska_flights data frame above. summarize() one of its columns/variables with a summary statistic. Examples include the median and interquartile range of temperatures as we saw in Section 3.7 on boxplots. group_by() its rows. In other words assign different rows to be part of the same group and report summary statistics for each group separately. For example, say perhaps you don’t want a single overall average departure delay dep_delay for all three origin airports combined, but rather three separate average departure delays, one for each of the three origin airports. mutate() its existing columns/variables to create new ones. For example, convert hourly temperature recordings from °F to °C. arrange() its rows. For example, sort the rows of weather in ascending or descending order of temp. join() it with another data frame by matching along a “key” variable. In other words, merge these two data frames together. Notice how we used computer code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling that we’ll introduce in this chapter has intuitively verb-named functions that are easy to remember. We’ll start by introducing the pipe operator %&gt;%, which allows you to combine multiple data wrangling verb-named functions into a single sequential chain of actions. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 2.3 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(nycflights13) 4.1 The pipe operator: %&gt;% Before we start data wrangling, let’s first introduce a very nifty tool that gets loaded along with the dplyr package: the pipe operator %&gt;%. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) The above code isn’t so hard to read since we are applying only three functions: f(), then g(), then h(). However, you can imagine that this can get progressively harder and harder to read as the number of functions applied in your sequence increases. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes one output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then.” For example, you can obtain the same output as the above sequence of operations as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this above sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches above would achieve the same goal, the latter is much more human-readable because you can read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example: flights. The sequence of functions, here f(), g(), and h(), will be a sequence of any number of the 6 data wrangling verb-named functions we listed in the introduction to this chapter. For example: filter(carrier == &quot;AS&quot;). The result will the transformed/modified data frame that you want. For example: a data frame consisting of only the subset of rows in flights corresponding to Alaska Airlines flights. Much like when adding layers to a ggplot() using the + sign at the end of lines, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence with pipe operators %&gt;% at the end of lines. So continuing our example involving Alaska Airlines flights, we form a chain using the pipe operator %&gt;% and save the resulting data frame in alaska_flights: alaska_flights &lt;- flights %&gt;% filter(carrier == &quot;AS&quot;) Keep in mind, there are many more advanced data wrangling functions than just the 6 listed in the introduction to this chapter; you’ll see some examples of these near in Section 4.8. However, just with these 6 verb-named functions you’ll be able to perform a broad array of data wrangling tasks for the rest of this book. 4.2 filter rows FIGURE 4.1: Diagram of The filter() function here works much like the “Filter” option in Microsoft Excel; it allows you to specify criteria about the values of a variables in your dataset and then filters out only those rows that match that criteria. We begin by focusing only on flights from New York City to Portland, Oregon. The dest code (or airport code) for Portland, Oregon is &quot;PDX&quot;. Run the following and look at the resulting spreadsheet to ensure that only flights heading to Portland are chosen here: portland_flights &lt;- flights %&gt;% filter(dest == &quot;PDX&quot;) View(portland_flights) Note the following: The ordering of the commands: Take the flights data frame flights then filter the data frame so that only those where the dest equals &quot;PDX&quot; are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(dest = &quot;PDX&quot;) will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other mathematical operations beyond just == to form criteria: &gt; corresponds to “greater than” &lt; corresponds to “less than” &gt;= corresponds to “greater than or equal to” &lt;= corresponds to “less than or equal to” != corresponds to “not equal to”. The ! is used in many programming languages to indicate “not”. Furthermore, you can combine multiple criteria together using operators that make comparisons: | corresponds to “or” &amp; corresponds to “and” To see many of these in action, let’s filter flights for all rows that: Departed from JFK airport and Were heading to Burlington, Vermont (&quot;BTV&quot;) or Seattle, Washington (&quot;SEA&quot;) and Departed in the months of October, November, or December. Run the following: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot; &amp; (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) &amp; month &gt;= 10) View(btv_sea_flights_fall) Note that even though colloquially speaking one might say “all flights leaving Burlington, Vermont and Seattle, Washington,” in terms of computer operations, we really mean “all flights leaving Burlington, Vermont or leaving Seattle, Washington.” For a given row in the data, dest can be “BTV”, “SEA”, or something else, but not “BTV” and “SEA” at the same time. Furthermore, note the careful use of parentheses around the dest == &quot;BTV&quot; | dest == &quot;SEA&quot;. We can often skip the use of &amp; and just separate our conditions with a comma. In other words the code above will return the identical output btv_sea_flights_fall as this code below: btv_sea_flights_fall &lt;- flights %&gt;% filter(origin == &quot;JFK&quot;, (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;), month &gt;= 10) View(btv_sea_flights_fall) Let’s present another example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to flights that didn’t go to Burlington, VT or Seattle, WA. not_BTV_SEA &lt;- flights %&gt;% filter(!(dest == &quot;BTV&quot; | dest == &quot;SEA&quot;)) View(not_BTV_SEA) Again, note the careful use of parentheses around the (dest == &quot;BTV&quot; | dest == &quot;SEA&quot;). If we didn’t use parentheses as follows: flights %&gt;% filter(!dest == &quot;BTV&quot; | dest == &quot;SEA&quot;) We would be returning all flights not headed to &quot;BTV&quot; or those headed to &quot;SEA&quot;, which is an entirely different resulting data frame. Now say we have a large list of airports we want to filter for, say BTV, SEA, PDX, SFO, and BDL. We could continue to use the | or operator as so: many_airports &lt;- flights %&gt;% filter(dest == &quot;BTV&quot; | dest == &quot;SEA&quot; | dest == &quot;PDX&quot; | dest == &quot;SFO&quot; | dest == &quot;BDL&quot;) View(many_airports) but as we progressively include more airports, this will get unwieldy. A slightly shorter approach uses the %in% operator: many_airports &lt;- flights %&gt;% filter(dest %in% c(&quot;BTV&quot;, &quot;SEA&quot;, &quot;PDX&quot;, &quot;SFO&quot;, &quot;BDL&quot;)) View(many_airports) What this code is doing is filtering flights for all flights where dest is in the list of airports c(&quot;BTV&quot;, &quot;SEA&quot;, &quot;PDX&quot;, &quot;SFO&quot;, &quot;BDL&quot;). Recall from Chapter 2 that the c() function “combines” or “concatenates” values in a vector of values. Both outputs of many_airports are the same, but as you can see the latter takes much less time to code. As a final note we point out that filter() should often be among the first verbs you apply to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations your care about. Learning check (LC4.1) What’s another way of using the “not” operator ! to filter only the rows that are not going to Burlington VT nor Seattle WA in the flights data frame? Test this out using the code above. 4.3 summarize variables The next common task when working with data is to return summary statistics: a single numerical value that summarizes a large number of values, for example the mean/average or the median. Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value AKA the minimum, the largest value AKA the maximum, and the standard deviation; they are all summaries of a large number of values. FIGURE 4.2: Summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet FIGURE 4.3: Another summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet Let’s calculate the mean and the standard deviation of the temperature variable temp in the weather data frame included in the nycflights13 package (See Appendix A). We’ll do this in one step using the summarize() function from the dplyr package and save the results in a new data frame summary_temp with columns/variables mean and the std_dev. Note you can also use the UK spelling of “summarise” using the summarise() function. As shown in Figures 4.2 and 4.3, the weather data frame’s many rows will be collapsed into a single row of just the summary values, in this case the mean and standard deviation: summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp), std_dev = sd(temp)) summary_temp # A tibble: 1 x 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 NA NA Why are the values returned NA? As we saw in Section 3.3.1 when creating the scatterplot of departure and arrival delays for alaska_flights, NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it? Perhaps the data was not collected at all because it was too difficult? Perhaps there was an erroneous value that someone entered that has been correct to read as missing? You’ll often encounter issues with missing values when working with real data. Going back to our summary_temp output above, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, then NA is returned. To work around this fact, you can set the na.rm argument to TRUE, where rm is short for “remove”; this will ignore any NA missing values and only return the summary value for all non-missing values. The code below computes the mean and standard deviation of all non-missing values of temp. Notice how the na.rm=TRUE are used as arguments to the mean() and sd() functions individually, and not to the summarize() function. summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_temp # A tibble: 1 x 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 55.3 17.8 However, one needs to be cautious whenever ignoring missing values as we’ve done above. In the upcoming Learning Checks we’ll consider the possible ramifications of blindly sweeping rows with missing values “under the rug.” This is in fact why the na.rm argument to any summary statistic function in R has is set to FALSE by default; in other words, do not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should by mindful of this missingness and any potential causes of this missingness throughtout your analysis. What are other summary statistic functions can we use inside the summarize() verb? As seen in Figure 4.3, you can use any function in R that takes many values and returns just one. Here are just a few: mean(): the mean AKA the average sd(): the standard deviation, which is a measure of spread min() and max(): the minimum and maximum values respectively IQR(): Interquartile range sum(): the sum n(): a count of the number of rows/observations in each group. This particular summary function will make more sense when group_by() is covered in Section 4.4. Learning check (LC4.2) Say a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach? (LC4.3) Modify the above summarize function to create summary_temp to also use the n() summary function: summarize(count = n()). What does the returned value correspond to? (LC4.4) Why doesn’t the following code work? Run the code line by line instead of all at once, and then look at the data. In other words, run summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE)) first. summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE)) %&gt;% summarize(std_dev = sd(temp, na.rm = TRUE)) 4.4 group_by rows FIGURE 4.4: Group by and summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet Say instead of the a single mean temperature for the whole year, you would like 12 mean temperatures, one for each of the 12 months separately? In other words, we would like to compute the mean temperature split by month AKA sliced by month AKA aggregated by month. We can do this by “grouping” temperature observations by the values of another variable, in this case by the 12 values of the variable month. Run the following code: summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE)) summary_monthly_temp month mean std_dev 1 35.6 10.22 2 34.3 6.98 3 39.9 6.25 4 51.7 8.79 5 61.8 9.68 6 72.2 7.55 7 80.1 7.12 8 74.5 5.19 9 67.4 8.47 10 60.1 8.85 11 45.0 10.44 12 38.4 9.98 This code is identical to the previous code that created summary_temp, but with an extra group_by(month) added before the summarize(). Grouping the weather dataset by month and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the 12 months of the year. It is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the group structure. It is only after we apply the summarize() function that the data frame changes. For example, let’s consider the diamonds data frame included in the ggplot2 package. Run this code, specifically in the console: diamonds # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # … with 53,930 more rows Observe that the first line of the output reads # A tibble: 53,940 x 10. This is an example of meta-data, in this case the number of observations/rows and variables/columns in diamonds. The actual data itself are the subsequent table of values. Now let’s pipe the diamonds data frame into group_by(cut). Run this code, specifically in the console: diamonds %&gt;% group_by(cut) # A tibble: 53,940 x 10 # Groups: cut [5] carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # … with 53,930 more rows Observe that now there is additional meta-data: # Groups: cut [5] indicating that the grouping structure meta-data has been set based on the 5 possible values AKA levels of the categorical variable cut: &quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;. On the other hand observe that the data has not changed: it is still a table of 53,940 \\(\\times\\) 10 values. Only by combining a group_by() with another data wrangling operation, in this case summarize() will the actual data be transformed. diamonds %&gt;% group_by(cut) %&gt;% summarize(avg_price = mean(price)) # A tibble: 5 x 2 cut avg_price &lt;ord&gt; &lt;dbl&gt; 1 Fair 4359. 2 Good 3929. 3 Very Good 3982. 4 Premium 4584. 5 Ideal 3458. If we would like to remove this group structure meta-data, we can pipe the resulting data frame into the ungroup() function. Observe how the # Groups: cut [5] meta-data is no longer present. Run this code, specifically in the console: diamonds %&gt;% group_by(cut) %&gt;% ungroup() # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # … with 53,930 more rows Let’s now revisit the n() counting summary function we introduced in the previous section. For example, suppose we’d like to count how many flights departed each of the three airports in New York City: by_origin &lt;- flights %&gt;% group_by(origin) %&gt;% summarize(count = n()) by_origin # A tibble: 3 x 2 origin count &lt;chr&gt; &lt;int&gt; 1 EWR 120835 2 JFK 111279 3 LGA 104662 We see that Newark (&quot;EWR&quot;) had the most flights departing in 2013 followed by &quot;JFK&quot; and lastly by LaGuardia (&quot;LGA&quot;). Note there is a subtle but important difference between sum() and n(); While sum() returns the sum of a numerical variable, n() returns counts of the the number of rows/observations. 4.4.1 Grouping by more than one variable You are not limited to grouping by one variable! Say you wanted to know the number of flights leaving each of the three New York City airports for each month, we can also group by a second variable month: group_by(origin, month). We see there are 36 rows to by_origin_monthly because there are 12 months for 3 airports (EWR, JFK, and LGA). by_origin_monthly &lt;- flights %&gt;% group_by(origin, month) %&gt;% summarize(count = n()) by_origin_monthly # A tibble: 36 x 3 # Groups: origin [3] origin month count &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 EWR 1 9893 2 EWR 2 9107 3 EWR 3 10420 4 EWR 4 10531 5 EWR 5 10592 6 EWR 6 10175 7 EWR 7 10475 8 EWR 8 10359 9 EWR 9 9550 10 EWR 10 10104 # … with 26 more rows Why do we group_by(origin, month) and not group_by(origin) and then group_by(month)? Let’s investigate: by_origin_monthly_incorrect &lt;- flights %&gt;% group_by(origin) %&gt;% group_by(month) %&gt;% summarize(count = n()) by_origin_monthly_incorrect # A tibble: 12 x 2 month count &lt;int&gt; &lt;int&gt; 1 1 27004 2 2 24951 3 3 28834 4 4 28330 5 5 28796 6 6 28243 7 7 29425 8 8 29327 9 9 27574 10 10 28889 11 11 27268 12 12 28135 What happened here is that the second group_by(month) overrode the group structure meta-data of the first group_by(origin), so that in the end we are only grouping by month. The lesson here is if you want to group_by() two or more variables, you should include all these variables in a single group_by() function call. Learning check (LC4.5) Recall from Chapter 3 when we looked at plots of temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in New York City throughout the year? (LC4.6) What code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC? (LC4.7) Recreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin). What differs in the resulting dataset? (LC4.8) How could we identify how many flights left each of the three airports for each carrier? (LC4.9) How does the filter operation differ from a group_by followed by a summarize? 4.5 mutate existing variables FIGURE 4.5: Mutate diagram from Data Wrangling with dplyr and tidyr cheatsheet Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius °C and not degrees Farenheit °F. The formula to convert temperatures from °F to °C is: \\[ \\text{temp in C} = \\frac{\\text{temp in F} - 32}{1.8} \\] We can apply this formula to the temp variable using the mutate() function, which takes existing variables and mutates them to create new ones. weather &lt;- weather %&gt;% mutate(temp_in_C = (temp-32)/1.8) View(weather) Note that we have overwritten the original weather data frame with a new version that now includes the additional variable temp_in_C. In other words, the mutate() command outputs a new data frame which then gets saved over the original weather data frame. Furthermore, note how in mutate() we used temp_in_C = (temp-32)/1.8 to create a new variable temp_in_C. Why did we overwrite the data frame weather instead of assigning the result to a new data frame like weather_new, but on the other hand why did we not overwrite temp, but instead created a new variable called temp_in_C? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames. On the other hand, had we used mutate(temp = (temp-32)/1.8) instead of mutate(temp_in_C = (temp-32)/1.8), we would have overwritten the original variable temp and lost its values. Let’s compute average monthly temperatures in both °F and °C using the similar group_by() and summarize() code as in the previous section. summary_monthly_temp &lt;- weather %&gt;% group_by(month) %&gt;% summarize(mean_temp_in_F = mean(temp, na.rm = TRUE), mean_temp_in_C = mean(temp_in_C, na.rm = TRUE)) summary_monthly_temp # A tibble: 12 x 3 month mean_temp_in_F mean_temp_in_C &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 35.6 2.02 2 2 34.3 1.26 3 3 39.9 4.38 4 4 51.7 11.0 5 5 61.8 16.6 6 6 72.2 22.3 7 7 80.1 26.7 8 8 74.5 23.6 9 9 67.4 19.7 10 10 60.1 15.6 11 11 45.0 7.22 12 12 38.4 3.58 Let’s consider another example. Passengers are often frustrated when their flights depart late, but change their mood a bit if pilots can make up some time during the flight to get them to their destination close to the original arrival time. This is commonly referred to as “gain” and we will create this variable using the mutate() function. flights &lt;- flights %&gt;% mutate(gain = dep_delay - arr_delay) Let’s take a look at dep_delay, arr_delay, and the resulting gain variables for the first 5 rows in our new flights data frame: # A tibble: 5 x 3 dep_delay arr_delay gain &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2 11 -9 2 4 20 -16 3 2 33 -31 4 -1 -18 17 5 -6 -25 19 The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its “gained time in the air” is actually a loss of 9 minutes, hence its gain is -9. Contrast this to the flight in the fourth row which departed a minute early (dep_delay of -1) but arrived 18 minutes early (arr_delay of -18), so its “gained time in the air” is 17 minutes, hence its gain is +17. Let’s look at summary measures of this gain variable and even plot it in the form of a histogram: gain_summary &lt;- flights %&gt;% summarize( min = min(gain, na.rm = TRUE), q1 = quantile(gain, 0.25, na.rm = TRUE), median = quantile(gain, 0.5, na.rm = TRUE), q3 = quantile(gain, 0.75, na.rm = TRUE), max = max(gain, na.rm = TRUE), mean = mean(gain, na.rm = TRUE), sd = sd(gain, na.rm = TRUE), missing = sum(is.na(gain)) ) gain_summary min q1 median q3 max mean sd missing -196 -3 7 17 109 5.66 18 9430 We’ve recreated the summary function we saw in Chapter 3 here using the summarize function in dplyr. ggplot(data = flights, mapping = aes(x = gain)) + geom_histogram(color = &quot;white&quot;, bins = 20) FIGURE 4.6: Histogram of gain variable We can also create multiple columns at once and even refer to columns that were just created in a new column. Hadley and Garrett produce one such example in Chapter 5 of “R for Data Science” (Grolemund and Wickham 2016): flights &lt;- flights %&gt;% mutate( gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours ) Learning check (LC4.10) What do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value? (LC4.11) Could we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights. (LC4.12) What can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values. 4.6 arrange and sort rows One of the most common tasks people working with data would like to perform is sort the data frame’s rows in alphanumeric order of the values in a variable/column. For example, when calculating a median by hand requires you to first sort the data from the smallest to highest in value and then identify the “middle” value. The dplyr package has a function called arrange() that we will use to sort/reorder a data frame’s rows according to the values of the specified variable. This is often used after we have used the group_by() and summarize() functions as we will see. Let’s suppose we were interested in determining the most frequent destination airports for all domestic flights departing from New York City in 2013: freq_dest &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) freq_dest # A tibble: 105 x 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ABQ 254 2 ACK 265 3 ALB 439 4 ANC 8 5 ATL 17215 6 AUS 2439 7 AVL 275 8 BDL 443 9 BGR 375 10 BHM 297 # … with 95 more rows Observe that by default the rows of the resulting freq_dest data frame are sorted in alphabetical order of dest destination. Say instead we would like to see the same data, but sorted from the most to the least number of flights num_flights instead: freq_dest %&gt;% arrange(num_flights) # A tibble: 105 x 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 LEX 1 2 LGA 1 3 ANC 8 4 SBN 10 5 HDN 15 6 MTJ 15 7 EYW 17 8 PSP 19 9 JAC 25 10 BZN 36 # … with 95 more rows This is actually giving us the opposite of what we are looking for: the rows are sorted with the least frequent destination airports displayed first. To switch the ordering to be descending instead of ascending we use the desc() function, which is short for “descending”: freq_dest %&gt;% arrange(desc(num_flights)) # A tibble: 105 x 2 dest num_flights &lt;chr&gt; &lt;int&gt; 1 ORD 17283 2 ATL 17215 3 LAX 16174 4 BOS 15508 5 MCO 14082 6 CLT 14064 7 SFO 13331 8 FLL 12055 9 MIA 11728 10 DCA 9705 # … with 95 more rows In other words, arrange() sorts in ascending order by default unless you override this default behavior by using desc(). 4.7 join data frames Another common data transformation task is “joining” or “merging” two different datasets. For example in the flights data frame the variable carrier lists the carrier code for the different flights. While the corresponding airline names for &quot;UA&quot; and &quot;AA&quot; might be somewhat easy to guess (United and American Airlines), what airlines have codes? &quot;VX&quot;, &quot;HA&quot;, and &quot;B6&quot;? This information is provided in a separate data frame airlines. View(airlines) We see that in airports, carrier is the carrier code while name is the full name of the airline company. Using this table, we can see that &quot;VX&quot;, &quot;HA&quot;, and &quot;B6&quot; correspond to Virgin America, Hawaiian Airlines, and JetBlue respectively. However, wouldn’t it be nice to have all this information in a single data frame instead of two separate data frames? We can do this by “joining” i.e. “merging” the flights and airlines data frames. Note that the values in the variable carrier in the flights data frame match the values in the variable carrier in the airlines data frame. In this case, we can use the variable carrier as a key variable to match the rows of the two data frames. Key variables are almost always identification variables that uniquely identify the observational units as we saw in Subsection ??. This ensures that rows in both data frames are appropriately matched during the join. Hadley and Garrett (Grolemund and Wickham 2016) created the following diagram to help us understand how the different datasets are linked by various key variables: FIGURE 4.7: Data relationships in nycflights13 from R for Data Science 4.7.1 Matching “key” variable names In both the flights and airlines data frames, the key variable we want to join/merge/match the rows of the two data frames by have the same name: carriers. We make use of the inner_join() function to join the two data frames, where the rows will be matched by the variable carrier. flights_joined &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(flights) View(flights_joined) Observe that the flights and flights_joined data frames are identical except that flights_joined has an additional variable name whose values correspond to the airline company names drawn from the airlines data frame. A visual representation of the inner_join() is given below (Grolemund and Wickham 2016). There are other types of joins available (such as left_join(), right_join(), outer_join(), and anti_join()), but the inner_join() will solve nearly all of the problems you’ll encounter in this book. FIGURE 4.8: Diagram of inner join from R for Data Science 4.7.2 Different “key” variable names Say instead you are interested in the destinations of all domestic flights departing NYC in 2013 and ask yourself: “What cities are these airports in?” “Is &quot;ORD&quot; Orlando?” &quot;Where is &quot;FLL&quot;? The airports data frame contains airport codes: View(airports) However, looking at both the airports and flights frames and the visual representation of the relations between these data frames in Figure 4.8 above, we see that in: the airports data frame the airport code is in the variable faa the flights data frame the airport codes are in the variables origin and dest So to join these two data frames so that we can identify the destination cities for example, our inner_join() operation will use the by = c(&quot;dest&quot; = &quot;faa&quot;) argument, which allows us to join two data frames where the key variable has a different name: flights_with_airport_names &lt;- flights %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) View(flights_with_airport_names) Let’s construct the sequence of commands that computes the number of flights from NYC to each destination, but also includes information about each destination airport: named_dests &lt;- flights %&gt;% group_by(dest) %&gt;% summarize(num_flights = n()) %&gt;% arrange(desc(num_flights)) %&gt;% inner_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) %&gt;% rename(airport_name = name) named_dests # A tibble: 101 x 9 dest num_flights airport_name lat lon alt tz dst tzone &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 ORD 17283 Chicago Ohare Intl 42.0 -87.9 668 -6 A America… 2 ATL 17215 Hartsfield Jackson… 33.6 -84.4 1026 -5 A America… 3 LAX 16174 Los Angeles Intl 33.9 -118. 126 -8 A America… 4 BOS 15508 General Edward Law… 42.4 -71.0 19 -5 A America… 5 MCO 14082 Orlando Intl 28.4 -81.3 96 -5 A America… 6 CLT 14064 Charlotte Douglas … 35.2 -80.9 748 -5 A America… 7 SFO 13331 San Francisco Intl 37.6 -122. 13 -8 A America… 8 FLL 12055 Fort Lauderdale Ho… 26.1 -80.2 9 -5 A America… 9 MIA 11728 Miami Intl 25.8 -80.3 8 -5 A America… 10 DCA 9705 Ronald Reagan Wash… 38.9 -77.0 15 -5 A America… # … with 91 more rows In case you didn’t know, &quot;ORD&quot; is the airport code of Chicago O’Hare airport and &quot;FLL&quot; is the main airport in Fort Lauderdale, Florida, which we can now see in the airport_name variable in the resulting named_dests data frame. 4.7.3 Multiple “key” variables Say instead we are in a situation where we need to join by multiple variables. For example, in Figure 4.7 above we see that in order to join the flights and weather data frames, we need more than one key variable: year, month, day, hour, and origin. This is because the combination of these 5 variables act to uniquely identify each observational unit in the weather data frame: hourly weather recordings at each of the 3 NYC airports. We achieve this by specifying a vector of key variables to join by using the c() function for “combine” or “concatenate” that we saw earlier: flights_weather_joined &lt;- flights %&gt;% inner_join(weather, by = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;origin&quot;)) View(flights_weather_joined) Learning check (LC4.13) Looking at Figure 4.7, when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour? (LC4.14) What surprises you about the top 10 destinations from NYC in 2013? 4.7.4 Normal forms The data frames included in the nycflights13 package are in a form that minimizes redundancy of data. For example, the flights data frame only saves the carrier code of the airline company; it does not include the actual name of the airline. For example the first row of flights has carrier equal to UA, but does it does not include the airline name “United Air Lines Inc.” The names of the airline companies are included in the name variable of the airlines data frame. In order to have the airline company name included in flights, we could join these two data frames as follows: joined_flights &lt;- flights %&gt;% inner_join(airlines, by = &quot;carrier&quot;) View(joined_flights) We are capable of performing this join because each of the data frames have keys in common to relate one to another: the carrier variable in both the flights and airlines data frames. The key variable(s) that we join are often identification variables we mentioned previously. This is an important property of what’s known as normal forms of data. The process of decomposing data frames into less redundant tables without losing information is called normalization. More information is available on Wikipedia. Learning check (LC4.15) What are some advantages of data in normal forms? What are some disadvantages? 4.8 Other verbs Here are some other useful data wrangling verbs that might come in handy: select() only a subset of variables/columns rename() variables/columns to have new names Return only the top_n() values of a variable 4.8.1 select variables FIGURE 4.9: Select diagram from Data Wrangling with dplyr and tidyr cheatsheet We’ve seen that the flights data frame in the nycflights13 package contains 19 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package: glimpse(flights) However, say you only need two of these variables, say carrier and flight. You can select() these two variables: flights %&gt;% select(carrier, flight) This function makes exploring data frames with a very large number of variables easier for humans to process by restricting consideration to only those we care about, like our example with carrier and flight above. This might make viewing the dataset using the View() spreadsheet viewer more digestible. However, as far as the computer is concerned, it doesn’t care how many additional variables are in the data frame in question, so long as carrier and flight are included. Let’s say instead you want to drop i.e deselect certain variables. For example, take the variable year in the flights data frame. This variable isn’t quite a “variable” in the sense that all the values are 2013 i.e. it doesn’t change. Say you want to remove the year variable from the data frame; we can deselect year by using the - sign: flights_no_year &lt;- flights %&gt;% select(-year) glimpse(flights_no_year) Another way of selecting columns/variables is by specifying a range of columns: flight_arr_times &lt;- flights %&gt;% select(month:day, arr_time:sched_arr_time) flight_arr_times The select() function can also be used to reorder columns in combination with the everything() helper function. Let’s suppose we’d like the hour, minute, and time_hour variables, which appear at the end of the flights dataset, to appear immediately after the year, month, and day variables while keeping the rest of the variables. In the code below everything() picks up all remaining variables. flights_reorder &lt;- flights %&gt;% select(year, month, day, hour, minute, time_hour, everything()) glimpse(flights_reorder) Lastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/column that match those conditions. For example: flights_begin_a &lt;- flights %&gt;% select(starts_with(&quot;a&quot;)) flights_begin_a flights_delays &lt;- flights %&gt;% select(ends_with(&quot;delay&quot;)) flights_delays flights_time &lt;- flights %&gt;% select(contains(&quot;time&quot;)) flights_time 4.8.2 rename variables Another useful function is rename(), which as you may have guessed renames one column to another name. Suppose we want dep_time and arr_time to be departure_time and arrival_time instead in the flights_time data frame: flights_time_new &lt;- flights %&gt;% select(contains(&quot;time&quot;)) %&gt;% rename(departure_time = dep_time, arrival_time = arr_time) glimpse(flights_time) Note that in this case we used a single = sign within the rename(), for example departure_time = dep_time. This is because we are not testing for equality like we would using ==, but instead we want to assign a new variable departure_time to have the same values as dep_time and then delete the variable dep_time. It’s easy to forget if the new name comes before or after the equals sign. I usually remember this as “New Before, Old After” or NBOA. 4.8.3 top_n values of a variable We can also return the top n values of a variable using the top_n() function. For example, we can return a data frame of the top 10 destination airports using the example from Section 4.7.2. Observe that we set the number of values to return to n = 10 and wt = num_flights to indicate that we want the rows of corresponding to the top 10 values of num_flights. See the help file for top_n() by running ?top_n for more information. named_dests %&gt;% top_n(n = 10, wt = num_flights) Let’s further arrange() these results in descending order of num_flights: named_dests %&gt;% top_n(n = 10, wt = num_flights) %&gt;% arrange(desc(num_flights)) Learning check (LC4.16) What are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways. (LC4.17) How could one use starts_with, ends_with, and contains to select columns from the flights data frame? Provide three different examples in total: one for starts_with, one for ends_with, and one for contains. (LC4.18) Why might we want to use the select function on a data frame? (LC4.19) Create a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013. 4.9 Conclusion 4.9.1 Summary table Let’s recap our data wrangling verbs in Table 4.1. Using these verbs and the pipe %&gt;% operator from Section 4.1, you’ll be able to write easily legible code to perform almost all the data wrangling necessary for the rest of this book. TABLE 4.1: Summary of data wrangling verbs Verb Data wrangling operation filter() Pick out a subset of rows summarize() Summarize many values to one using a summary statistic function like mean(), median(), etc. group_by() Add grouping structure to rows in data frame. Note this does not change values in data frame. mutate() Create new variables by mutating existing ones arrange() Arrange rows of a data variable in ascending (default) or descending order inner_join() Join/merge two data frames, matching rows by a key variable Learning check (LC4.20) Let’s now put your newly acquired data wrangling skills to the test! An airline industry measure of a passenger airline’s capacity is the available seat miles, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights. So for example say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be 2 \\(\\times\\) 10 \\(\\times\\) 500 \\(+\\) 3 \\(\\times\\) 20 \\(\\times\\) 1000 = 70,000 seat miles. Using the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints: Crucial: Unless you are very confident in what you are doing, it is worthwhile to not starting coding right away, but rather first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code). Take a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles. Figure 4.7 above showing how the various datasets can be joined will also be useful. Consider the data wrangling verbs in Table 4.1 as your toolbox! 4.9.2 Additional resources An R script file of all R code used in this chapter is available here. If you want to further unlock the power of the dplyr package for data wrangling, we suggest you that you check out RStudio’s “Data Transformation with dplyr” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular more-intermediate level and advanced data wrangling functions, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; “Data Transformation with dplyr”: FIGURE 4.10: Data Transformation with dplyr cheatsheat On top of data wrangling verbs and examples we presented in this section, if you’d like to see more examples of using the dplyr package for data wrangling check out Chapter 5 of Garrett Grolemund and Hadley Wickham’s and Garrett’s book (Grolemund and Wickham 2016). 4.9.3 What’s to come? So far in this book, we’ve explored, visualized, and wrangled data saved in data frames that are in spreadsheet-type format: rectangular with a certain number of rows corresponding to observations and a certain number of columns corresponding to variables describing the observations. We’ll see in Chapter 5 that there are actually two ways to represent data in spreadsheet-type rectangular format: 1) “wide” format and 2) “tall/narrow” format also known in R circles as “tidy” format. While the distinction between “tidy” and non-“tidy” formatted data is very subtle, it has very large implications for whether or not we can use the ggplot2 package for data visualization and the dplyr package for data wrangling. Furthermore, we’ve only explored, visualized, and wrangled data saved within R packages. What if you have spreadsheet data saved in a Microsoft Excel, Google Sheets, or “Comma-Separated Values” (CSV) file that you would like to analyze? In Chapter 5, we’ll show you how to import this data into R using the readr package. References "],
["5-tidy.html", "Chapter 5 Data Importing &amp; “Tidy” Data 5.1 Importing data 5.2 Tidy data 5.3 Case study: Democracy in Guatemala 5.4 Conclusion", " Chapter 5 Data Importing &amp; “Tidy” Data In Subsection 2.2.1 we introduced the concept of a data frame: a rectangular spreadsheet-like representation of data in R where the rows correspond to observations and the columns correspond to variables describing each observation. In Section 2.4, we started exploring our first data frame: the flights data frame included in the nycflights13 package. In Chapter 3 we created visualizations based on the data included in flights and other data frames such as weather. In Chapter 4, we learned how to wrangle data, in other words take existing data frames and transform/ modify them to suit our analysis goals. In this final chapter of the “Data Science via the tidyverse” portion of the book, we extend some of these ideas by discussing a type of data formatting called “tidy” data. You will see that having data stored in “tidy” format is about more than what the colloquial definition of the term “tidy” might suggest: having your data “neatly organized.” Instead, we define the term “tidy” in a more rigorous fashion, outlining a set of rules by which data can be stored, and the implications of these rules for analyses. Although knowledge of this type of data formatting was not necessary for our treatment of data visualization in Chapter 3 and data wrangling in Chapter 4 since all the data was already in “tidy” format, we’ll now see this format is actually essential to using the tools we covered in these two chapters. Furthermore, it will also be useful for all subsequent chapters in this book when we cover regression and statistical inference. First however, we’ll show you how to import spreadsheet data for use in R. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 2.3 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(readr) library(tidyr) library(nycflights13) library(fivethirtyeight) 5.1 Importing data Up to this point, we’ve almost entirely used data stored inside of an R package. Say instead you have your own data saved on your computer or somewhere online? How can you analyze this data in R? Spreadsheet data is often saved in one of the following formats: A Comma Separated Values .csv file. You can think of a .csv file as a bare-bones spreadsheet where: Each line in the file corresponds to one row of data/one observation. Values for each line are separated with commas. In other words, the values of different variables are separated by commas. The first line is often, but not always, a header row indicating the names of the columns/variables. An Excel .xlsx file. This format is based on Microsoft’s proprietary Excel software. As opposed to a bare-bones .csv files, .xlsx Excel files contain a lot of meta-data, or put more simply, data about the data. (Recall we saw a previous example of meta-data in Section 4.4 when adding “group structure” meta-data to a data frame by using the group_by() verb.) Some examples of spreadsheet meta-data include the use of bold and italic fonts, colored cells, different column widths, and formula macros. A Google Sheets file, which is a “cloud” or online-based way to work with a spreadsheet. Google Sheets allows you to download your data in both comma separated values .csv and Excel .xlsx formats however: go to the Google Sheets menu bar -&gt; File -&gt; Download as -&gt; Select “Microsoft Excel” or “Comma-separated values.” We’ll cover two methods for importing .csv and .xlsx spreadsheet data in R: one using the R console and the other using RStudio’s graphical user interface, abbreviated a GUI. 5.1.1 Using the console First, let’s import a Comma Separated Values .csv file of data directly off the internet. The .csv file dem_score.csv accessible at https://moderndive.com/data/dem_score.csv contains ratings of the level of democracy in different countries spanning 1952 to 1992. Let’s use the read_csv() function from the readr package to read it off the web, import it into R, and save it in a data frame called dem_score library(readr) dem_score &lt;- read_csv(&quot;https://moderndive.com/data/dem_score.csv&quot;) dem_score # A tibble: 96 x 10 country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Albania -9 -9 -9 -9 -9 -9 -9 -9 5 2 Argentina -9 -1 -1 -9 -9 -9 -8 8 7 3 Armenia -9 -7 -7 -7 -7 -7 -7 -7 7 4 Australia 10 10 10 10 10 10 10 10 10 5 Austria 10 10 10 10 10 10 10 10 10 6 Azerbaijan -9 -7 -7 -7 -7 -7 -7 -7 1 7 Belarus -9 -7 -7 -7 -7 -7 -7 -7 7 8 Belgium 10 10 10 10 10 10 10 10 10 9 Bhutan -10 -10 -10 -10 -10 -10 -10 -10 -10 10 Bolivia -4 -3 -3 -4 -7 -7 8 9 9 # … with 86 more rows In this dem_score data frame, the minimum value of -10 corresponds to a highly autocratic nation whereas a value of 10 corresponds to a highly democratic nation. We’ll revisit the dem_score data frame in a case study in the upcoming Section 5.3. Note that the read_csv() function included in the readr package is different than the read.csv() function that comes installed with R by default. While the difference in the names might seem near meaningless (an _ instead of a .), the read_csv() function is in our opinion easier to use since it can more easily read data off the web and generally imports data at a much faster speed. 5.1.2 Using RStudio’s interface Let’s read in the exact same data saved in Excel format, but this time via RStudio’s graphical interface instead of via the R console. First download the Excel file dem_score.xlsx by clicking here, then Go to the Files panel of RStudio. Navigate to the directory i.e. folder on your computer where the downloaded dem_score.xlsx Excel file is saved. Click on dem_score.xlsx. Click “Import Dataset…” At this point you should see an image like this: After clicking on the “Import” button on the bottom right RStudio, RStudio will save this spreadsheet’s data in a data frame called dem_score and display its contents in the spreadsheet viewer. Furthermore, note in the bottom right of the above image there exists a “Code Preview”: you can copy and paste this code to reload your data again later automatically instead of repeating the above manual point-and-click process. 5.2 Tidy data Let’s now switch gears and learn about the concept of “tidy” data format by starting with a motivating example. Let’s consider the drinks data frame included in the fivethirtyeight data. Run the following: drinks # A tibble: 193 x 5 country beer_servings spirit_servings wine_servings total_litres_of_pur… &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan 0 0 0 0 2 Albania 89 132 54 4.9 3 Algeria 25 0 14 0.7 4 Andorra 245 138 312 12.4 5 Angola 217 57 45 5.9 6 Antigua &amp; B… 102 128 45 4.9 7 Argentina 193 25 221 8.3 8 Armenia 21 179 11 3.8 9 Australia 261 72 212 10.4 10 Austria 279 75 191 9.7 # … with 183 more rows After reading the help file by running ?drinks, we see that drinks is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed for 193 countries. This data was originally reported on the data journalism website FiveThirtyEight.com in Mona Chalabi’s article “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?” Let’s apply some of the data wrangling verbs we learned in Chapter 4 on the drinks data frame. Let’s filter() the drinks data frame to only consider 4 countries (the United States, China, Italy, and Saudi Arabia) then select() all columns except total_litres_of_pure_alcohol by using - sign, then rename() the variables beer_servings, spirit_servings, and wine_servings to beer, spirit, and wine respectively and save the resulting data frame in drinks_smaller. drinks_smaller &lt;- drinks %&gt;% filter(country %in% c(&quot;USA&quot;, &quot;China&quot;, &quot;Italy&quot;, &quot;Saudi Arabia&quot;)) %&gt;% select(-total_litres_of_pure_alcohol) %&gt;% rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings) drinks_smaller # A tibble: 4 x 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 Using the drinks_smaller data frame, how would we create the side-by-side AKA dodged barplot in Figure 5.1? Recall we saw barplots displaying two categorical variables in Section 3.8.3. FIGURE 5.1: Alcohol consumption in 4 countries. Let’s break down the Grammar of Graphics: The categorical variable country with four levels (China, Italy, Saudi Arabia, USA) would have to be mapped to the x-position of the bars. The numerical variable servings would have to be mapped to the y-position of the bars, in other words the height of the bars. The categorical variable type with three levels (beer, spirit, wine) who have to be mapped to the fill color of the bars. Observe however that drinks_smaller has three separate variables for beer, spirit, and wine, whereas in order to recreate the side-by-side AKA dodged barplot in Figure 5.1 we would need a single variable type with three possible values: beer, spirit, and wine, which we would then map to the fill aesthetic. In other words, for us to be able to create the barplot in Figure 5.1, our data frame would have to look like this: drinks_smaller_tidy # A tibble: 12 x 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 Italy beer 85 3 Saudi Arabia beer 0 4 USA beer 249 5 China spirit 192 6 Italy spirit 42 7 Saudi Arabia spirit 5 8 USA spirit 158 9 China wine 8 10 Italy wine 237 11 Saudi Arabia wine 0 12 USA wine 84 Let’s compare the drinks_smaller_tidy with the drinks_smaller data frame from earlier: drinks_smaller # A tibble: 4 x 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 Observe that while drinks_smaller and drinks_smaller_tidy are both rectangular in shape and contain the same 12 numerical values (3 alcohol types \\(\\times\\) 4 countries), they are formatted differently. drinks_smaller is formatted in what’s known as “wide” format, whereas drinks_smaller_tidy is formatted in what’s known as “long/narrow”. In the context of using R, long/narrow format is also known as “tidy” format. Furthermore, in order to use the ggplot2 and dplyr packages for data visualization and data wrangling, your input data frames must be in “tidy” format. So all non-“tidy” data must be converted to “tidy” format first. Before we show you how to convert non-“tidy” data frames like drinks_smaller to “tidy” data frames like drinks_smaller_tidy, let’s go over the explicit definition of “tidy” data. 5.2.1 Definition of “tidy” data You have surely heard the word “tidy” in your life: “Tidy up your room!” “Please write your homework in a tidy way so that it is easier to grade and to provide feedback.” Marie Kondo’s best-selling book The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing and Netflix TV series Tidying Up with Marie Kondo. “I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant What does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized”, “tidy” in the context of data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of tidy data here (Wickham 2014): A dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes. Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. FIGURE 5.2: Tidy data graphic from R for Data Science. For example, say you have the following table of stock prices in Table 5.1: TABLE 5.1: Stock Prices (Non-Tidy Format) Date Boeing Stock Price Amazon Stock Price Google Stock Price 2009-01-01 $173.55 $174.90 $174.34 2009-01-02 $172.61 $171.42 $170.04 Although the data are neatly organized in a rectangular spreadsheet-type format, they are not in tidy format because while there are three variables corresponding to three unique pieces of information (Date, Stock Name, and Stock Price), there are not three columns. In “tidy” data format each variable should be its own column, as shown in Table 5.2. Notice that both tables present the same information, but in different formats. TABLE 5.2: Stock Prices (Tidy Format) Date Stock Name Stock Price 2009-01-01 Boeing $173.55 2009-01-02 Boeing $172.61 2009-01-01 Amazon $174.90 2009-01-02 Amazon $171.42 2009-01-01 Google $174.34 2009-01-02 Google $170.04 Now we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider the data in Table 5.3. TABLE 5.3: Date, Boeing Price, Weather Data Date Boeing Price Weather 2009-01-01 $173.55 Sunny 2009-01-02 $172.61 Overcast In this case, even though the variable “Boeing Price” occurs just like in our non-“tidy” data in Table 5.1, the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Boeing stock price, and the weather that particular day. Learning check (LC5.1) What are common characteristics of “tidy” data frames? (LC5.2) What makes “tidy” data frames useful for organizing data? 5.2.2 Converting to “tidy” data In this book so far, you’ve only seen data frames that were already in “tidy” format. Furthermore for the rest of this book, you’ll mostly only see data frames that are already in “tidy” format as well. This is not always the case however with data in the wild. If your original data frame is in wide i.e. non-“tidy” format and you would like to use the ggplot2 package for data visualization or the dplyr package for data wrangling, you will first have to convert it “tidy” format using the gather() function in the tidyr package (Wickham and Henry 2019). Going back to our drinks_smaller data frame from earlier: drinks_smaller # A tibble: 4 x 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 We convert it to “tidy” format by using the gather() function from the tidyr package as follows: drinks_smaller_tidy &lt;- drinks_smaller %&gt;% gather(key = type, value = servings, -country) drinks_smaller_tidy # A tibble: 12 x 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 Italy beer 85 3 Saudi Arabia beer 0 4 USA beer 249 5 China spirit 192 6 Italy spirit 42 7 Saudi Arabia spirit 5 8 USA spirit 158 9 China wine 8 10 Italy wine 237 11 Saudi Arabia wine 0 12 USA wine 84 We set the arguments to gather() as follows: key is the name of the column/variable in the new “tidy” frame that contains the column names of the original data frame that you want to tidy. Observe how we set key = type and in the resulting drinks_smaller_tidy the column type contains the three types of alcohol beer, spirit, and wine. value is the name of the column/variable in the “tidy” frame that contains the rows and columns of values in the original data frame you want to tidy. Observe how we set value = servings and in the resulting drinks_smaller_tidy the column value contains the 4 \\(\\times\\) 3 = 12 numerical values. The third argument are the columns you either want to or don’t want to tidy. Observe how we set this to -country indicating that we don’t want to tidy the country variable in drinks_smaller and rather only beer, spirit, and wine. The third argument is a little nuanced, so let’s consider another example. Note the code below is very similar, but now the third argument species which columns we’d want to tidy c(beer, spirit, wine), instead of the columns we don’t want to tidy -country. Note the use of c() to create a vector of the columns in drinks_smaller that we’d like to tidy. If you run the code below, you’ll see that the resulting drinks_smaller_tidy is the same. drinks_smaller_tidy &lt;- drinks_smaller %&gt;% gather(key = type, value = servings, c(beer, spirit, wine)) drinks_smaller_tidy With our drinks_smaller_tidy “tidy” format data frame, we can now produce a side-by-side AKA dodged barplot using geom_col() and not geom_bar(), since we would like to map the servings variable to the y-aesthetic of the bars. ggplot(drinks_smaller_tidy, aes(x=country, y=servings, fill=type)) + geom_col(position = &quot;dodge&quot;) Converting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the gather() function is with practice, practice, and more practice. For example, see the examples in the bottom of the help file for gather() by running ?gather. We’ll show another example of using gather() to convert a “wide” formatted data frame to “tidy” format in Section 5.3. For other examples of converting a dataset into “tidy” format, check out the different functions available for data tidying and a case study using data from the World Health Organization in R for Data Science (Grolemund and Wickham 2016). Learning check (LC5.3) Take a look the airline_safety data frame included in the fivethirtyeight data. Run the following: airline_safety After reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airlines companies’ safety records. This data was originally reported on the data journalism website FiveThirtyEight.com in Nate Silver’s article “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s ignore the incl_reg_subsidiaries and avail_seat_km_per_week variables for simplicity: airline_safety_smaller &lt;- airline_safety %&gt;% select(-c(incl_reg_subsidiaries, avail_seat_km_per_week)) airline_safety_smaller # A tibble: 56 x 7 airline incidents_85_99 fatal_accidents… fatalities_85_99 incidents_00_14 &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 Aer Li… 2 0 0 0 2 Aerofl… 76 14 128 6 3 Aeroli… 6 0 0 1 4 Aerome… 3 1 64 5 5 Air Ca… 2 0 0 2 6 Air Fr… 14 4 79 6 7 Air In… 2 1 329 4 8 Air Ne… 3 0 0 5 9 Alaska… 5 0 0 5 10 Alital… 7 2 50 4 # … with 46 more rows, and 2 more variables: fatal_accidents_00_14 &lt;int&gt;, # fatalities_00_14 &lt;int&gt; This data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable incident_type_years indicating the incident type/year and a variable count of the counts? 5.2.3 nycflights13 package Recall the nycflights13 package with data about all domestic flights departing from New York City in 2013 that we introduced in Section 2.4 and used extensively in Chapter 3 on data visualization and Chapter 4 on data wrangling. Let’s revisit the flights data frame by running View(flights). We saw that flights has a rectangular shape with each of its 336,776 rows corresponding to a flight and each of its 22 columns corresponding to different characteristics/measurements of each flight. This matches exactly with our definition of “tidy” data from above. Each variable forms a column. Each observation forms a row. But what about the third property of “tidy” data? Each type of observational unit forms a table. Recall that we also saw in Section 2.4.3 that the observational unit for the flights data frame is an individual flight. In other words, the rows of the flights data frame refer to characteristics/measurements of individual flights. Also included in the nycflights13 package are other data frames with their rows representing different observational units (Wickham 2018): airlines: translation between two letter IATA carrier codes and names (16 in total). i.e. the observational unit is an airline company. planes: construction information about each of 3,322 planes used. i.e. the observational unit is an aircraft. weather: hourly meteorological data (about 8705 observations) for each of the three NYC airports. i.e. the observational unit is an hourly measurement. airports: airport names and locations. i.e. the observational unit is an airport. The organization of the information into these five data frames follow the third “tidy” data property: observations corresponding to the same observational unit should be saved in the same table i.e. data frame. You could think of this property as the old English expression: “birds of a feather flock together.” 5.3 Case study: Democracy in Guatemala In this section, we’ll show you another example of how to convert a data frame that isn’t in “tidy” format i.e. “wide” format, to a data frame that is in “tidy” format i.e. “long/narrow” format. We’ll do this using the gather() function from the tidyr package again. Furthermore, we’ll make use of some of the ggplot2 data visualization and dplyr data wrangling tools you learned in Chapters 3 and 4. Let’s use the dem_score data frame we imported in Section 5.1, but focus on only data corresponding to Guatemala. guat_dem &lt;- dem_score %&gt;% filter(country == &quot;Guatemala&quot;) guat_dem # A tibble: 1 x 10 country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Guatemala 2 -6 -5 3 1 -3 -7 3 3 Now let’s produce a time-series plot showing how the democracy scores have changed over the 40 years from 1952 to 1992 for Guatemala. Recall that we saw time-series plot in Section 3.4 on creating linegraphs using geom_line(). Let’s lay out the Grammar of Graphics we saw in Section 3.1. First we know we need to set data = guat_dem and use a geom_line() layer, but what is the aesthetic mapping of variables. We’d like to see how the democracy score has changed over the years, so we need to map: year to the x-position aesthetic and democracy_score to the y-position aesthetic Now we are stuck in a predicament, much like with our drinks_smaller example in Section 5.2. We see that we have a variable named country, but its only value is &quot;Guatemala&quot;. We have other variables denoted by different year values. Unfortunately, the guat_dem data frame is not “tidy” and hence is not in the appropriate format to apply the Grammar of Graphics and thus we cannot use the ggplot2 package. We need to take the values of the columns corresponding to years in guat_dem and convert them into a new “key” variable called year. Furthermore, we’d like to take the democracy scores on the inside of the table and turn them into a new “value” variable called democracy_score. Our resulting data frame will thus have three columns: country, year, and democracy_score. Recall that the gather() function in the tidyr package can complete this task for us: guat_dem_tidy &lt;- guat_dem %&gt;% gather(key = year, value = democracy_score, -country) guat_dem_tidy # A tibble: 9 x 3 country year democracy_score &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 Guatemala 1952 2 2 Guatemala 1957 -6 3 Guatemala 1962 -5 4 Guatemala 1967 3 5 Guatemala 1972 1 6 Guatemala 1977 -3 7 Guatemala 1982 -7 8 Guatemala 1987 3 9 Guatemala 1992 3 We set the arguments to gather() as follows: key is the name of the column/variable in the new “tidy” frame that contains the column names of the original data frame that you want to tidy. Observe how we set key = year and in the resulting guat_dem_tidy the column year contains the years where the Guatemala’s democracy score were measured. value is the name of the column/variable in the “tidy” frame that contains the rows and columns of values in the original data frame you want to tidy. Observe how we set value = democracy_score and in the resulting guat_dem_tidy the column democracy_score contains the 1 \\(\\times\\) 9 = 9 democracy scores. The third argument are the columns you either want to or don’t want to tidy. Observe how we set this to -country indicating that we don’t want to tidy the country variable in guat_dem and rather only 1952 through 1992. However, observe in the output for guat_dem_tidy that the year variable is of type chr or character. Before we can plot this variable on the x-axis, we need to convert it into a numerical variable using the as.numeric() function within the mutate() function, which we saw in Section 4.5 on mutating existing variables to create new ones. guat_dem_tidy &lt;- guat_dem_tidy %&gt;% mutate(year = as.numeric(year)) We can now create the plot to show how the democracy score of Guatemala changed from 1952 to 1992 using a geom_line(): ggplot(guat_dem_tidy, aes(x = year, y = democracy_score)) + geom_line() + labs(x = &quot;Year&quot;, y = &quot;Democracy Score&quot;, title = &quot;Democracy score in Guatemala 1952-1992&quot;) Learning check (LC5.4) Convert the dem_score data frame into a tidy data frame and assign the name of dem_score_tidy to the resulting long-formatted data frame. (LC5.5) Read in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a tidy data frame. 5.4 Conclusion 5.4.1 tidyverse package Notice at the beginning of the chapter we loaded the following four packages, which are among the four of the most frequently used R packages for data science: library(dplyr) library(ggplot2) library(readr) library(tidyr) There is a much quicker way to load these packages than by individually loading them as we did above: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you. So after installing the tidyverse package as you would a normal package, running this: library(tidyverse) would be the same as running this: library(ggplot2) library(dplyr) library(tidyr) library(readr) library(purrr) library(tibble) library(stringr) library(forcats) You’ve seen the first 4 of the these packages: ggplot2 for data visualization, dplyr for data wrangling, tidyr for converting data to “tidy” format, and readr for importing spreadsheet data into R. The remaining packages (purrr, tibble, stringr, and forcats) are left for a more advanced book; check out R for Data Science to learn about these packages. The tidyverse “umbrella” package gets its name from the fact that all functions in all its constituent packages are designed to that all inputs/argument data frames are in “tidy” format and all output data frames are in “tidy” format as well. This standardization of input and output data frames makes transitions between the various functions in these packages as seamless as possible. 5.4.2 Additional resources An R script file of all R code used in this chapter is available here. If you want to learn more about using the readr and tidyr package, we suggest you that you check out RStudio’s “Data Import” cheatsheet. You can access this cheatsheet by going to RStudio’s cheatsheet page and searching for “Data Import Cheat Sheet”. FIGURE 5.3: Data Import cheatsheat 5.4.3 What’s to come? Congratulations! We’ve completed the “Data Science via the tidyverse” portion of this book! We’ll now move to the “data modeling” portion in Chapters 6 and 7, where you’ll leverage your data visualization and wrangling skills to model relationships between different variables in data frames. However, we’re going to leave the Chapter ?? on “Inference for Regression” until after we’ve covered statistical inference. FIGURE 5.4: ModernDive flowchart - On to Part II! References "],
["6-regression.html", "Chapter 6 Basic Regression 6.1 One numerical explanatory variable 6.2 One categorical explanatory variable 6.3 Related topics 6.4 Conclusion", " Chapter 6 Basic Regression Now that we are equipped with data visualization skills from Chapter 3, an understanding of the “tidy” data format from Chapter 5, and data wrangling skills from Chapter 4, we now proceed with data modeling. The fundamental premise of data modeling is to make explicit the relationship between: an outcome variable \\(y\\), also called a dependent variable and an explanatory/predictor variable \\(x\\), also called an independent variable or covariate. Another way to state this is using mathematical terminology: we will model the outcome variable \\(y\\) as a function of the explanatory/predictor variable \\(x\\). Why do we have two different labels, explanatory and predictor, for the variable \\(x\\)? That’s because roughly speaking data modeling can be used for two purposes: Modeling for prediction: You want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables. You don’t care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about \\(y\\), you’re fine. For example, if we know many individuals’ risk factors for lung cancer, such as smoking habits and age, can we predict whether or not they will develop lung cancer? Here we wouldn’t care so much about distinguishing the degree to which the different risk factors contribute to lung cancer, but instead only on whether or not they could be put together to make reliable predictions. Modeling for explanation: You want to explicitly describe the relationship between an outcome variable \\(y\\) and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these. Continuing our example from above, we would now be interested in describing the individual effects of the different risk factors and quantifying the magnitude of these effects. One reason could be to design an intervention to reduce lung cancer cases in a population, such as targeting smokers of a specific age group with an advertisement for smoking cessation programs. In this book, we’ll focus more on this latter purpose. Data modeling is used in a wide variety of fields, including statistical inference, causal inference, artificial intelligence, and machine learning. There are many techniques for data modeling, such as tree-based models, neural networks and deep learning, and supervised learning. In this chapter, we’ll focus on one particular technique: linear regression, one of the most commonly-used and easy-to-understand approaches to modeling. Recall our discussion in Subsection 2.4.3 on numerical and categorical variables. Linear regression involves: an outcome variable \\(y\\) that is numerical and explanatory variables \\(\\vec{x}\\) that are either numerical or categorical. With linear regression there is always only one numerical outcome variable \\(y\\) but we have choices on both the number and the type of explanatory variables \\(\\vec{x}\\) to use. We’re going to cover the following regression scenarios: In this current chapter on basic regression, we’ll always have only one explanatory variable. In Section 6.1, this explanatory variable will be a single numerical explanatory variable \\(x\\). This scenario is known as simple linear regression. In Section 6.2, this explanatory variable will be a categorical explanatory variable \\(x\\). In the next chapter, Chapter 7 on multiple regression, we’ll have more than one explanatory variable: We’ll focus on two numerical explanatory variables \\(x_1\\) and \\(x_2\\) in Section 7.1. This can be denoted as \\(\\vec{x}\\) as well since we have more than one explanatory variable. We’ll use one numerical and one categorical explanatory variable in Section 7.1. We’ll also introduce interaction models here; there, the effect of one explanatory variable depends on the value of another. We’ll study all four of these regression scenarios using real data, all easily accessible via R packages! Needed packages In this chapter we introduce a new package, moderndive, that is an accompaniment package to this ModernDive book. It includes useful functions for linear regression and other functions as well as data used later in the book. Let’s now load all the packages needed for this chapter. If needed, read Section 2.3 for information on how to install and load R packages. library(ggplot2) library(dplyr) library(moderndive) library(gapminder) library(skimr) 6.1 One numerical explanatory variable Why do some professors and instructors at universities and colleges get high teaching evaluations from students while others don’t? What factors can explain these differences? Are there biases? These are questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which professors and instructors should get promotions. Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer this question: what factors can explain differences in instructor’s teaching evaluation scores? To this end, they collected information on \\(n = 463\\) instructors. A full description of the study can be found at openintro.org. We’ll keep things simple for now and try to explain differences in instructor evaluation scores as a function of one numerical variable: their “beauty score.” The specifics on how this score was calculated will be described shortly. Could it be that instructors with higher beauty scores also have higher teaching evaluations? Could it be instead that instructors with higher beauty scores tend to have lower teaching evaluations? Or could it be there is no relationship between beauty score and teaching evaluations? We’ll achieve ways to address these questions by modeling the relationship between these two variables with a particular kind of linear regression called simple linear regression. Simple linear regression is the most basic form of linear regression. With it we have A numerical outcome variable \\(y\\). In this case, their teaching score. A single numerical explanatory variable \\(x\\). In this case, their beauty score. 6.1.1 Exploratory data analysis A crucial step before doing any kind of modeling or analysis is performing an exploratory data analysis, or EDA, of all our data. Exploratory data analysis can give you a sense of the distribution of the data, and whether there are outliers and/or missing values. Most importantly, it can inform how to build your model. There are many approaches to exploratory data analysis; here are three: Most fundamentally: just looking at the raw values, in a spreadsheet for example. While this may seem trivial, many people ignore this crucial step! Computing summary statistics like means, medians, and standard deviations. Creating data visualizations. Let’s load the data, select only a subset of the variables, and look at the raw values. Recall you can look at the raw values by running View() in the console in RStudio to pop-up the spreadsheet viewer with the data frame of interest as the argument to View(). Here, however, we present only a snapshot of five randomly chosen rows: evals_ch6 &lt;- evals %&gt;% select(score, bty_avg, age) evals_ch6 %&gt;% sample_n(5) TABLE 6.1: Random sample of 5 instructors score bty_avg age 3.7 3.00 62 4.7 4.33 46 4.8 5.50 62 2.8 2.00 62 4.0 2.33 64 While a full description of each of these variables can be found at openintro.org, let’s summarize what each of these variables represents. score: Numerical variable of the average teaching score based on students’ evaluations between 1 and 5. This is the outcome variable \\(y\\) of interest. bty_avg: Numerical variable of average “beauty” rating based on a panel of 6 students’ scores between 1 and 10. This is the numerical explanatory variable \\(x\\) of interest. Here 1 corresponds to a low beauty rating and 10 to a high beauty rating. age: A numerical variable of age in years as an integer value. Another way to look at the raw values is using the glimpse() function, which gives us a slightly different view of the data. We see Observations: 463, indicating that there are 463 observations in evals, each corresponding to a particular instructor at UT Austin. Expressed differently, each row in the data frame evals corresponds to one of 463 instructors. glimpse(evals_ch6) Observations: 463 Variables: 3 $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4… $ bty_avg &lt;dbl&gt; 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, 3.17, 3… $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 4… Since both the outcome variable score and the explanatory variable bty_avg are numerical, we can compute summary statistics about them such as the mean, median, and standard deviation. Let’s take evals_ch6 and select only the two variables of interest for now. However, let’s instead pipe this into the skim() function from the skimr package. This function quickly uses a “skim” of the data to return the following summary information about each variable. evals_ch6 %&gt;% select(score, bty_avg) %&gt;% skim() Skim summary statistics n obs: 463 n variables: 2 Variable type: numeric variable missing complete n mean sd p0 p25 p50 p75 p100 hist bty_avg 0 463 463 4.42 1.53 1.67 3.17 4.33 5.5 8.17 ▂▅▅▇▃▃▂▁ score 0 463 463 4.17 0.54 2.3 3.8 4.3 4.6 5 ▁▁▂▃▅▇▇▆ In this case for our two numerical variables bty_avg beauty score and teaching score score it returns: missing: the number of missing values complete: the number of non-missing or complete values n: the total number of values mean: the average sd: the standard deviation p0: the 0th percentile: the value at which 0% of observations are smaller than it. This is also known as the minimum p25: the 25th percentile: the value at which 25% of observations are smaller than it. This is also known as the 1st quartile p50: the 50th percentile: the value at which 50% of observations are smaller than it. This is also know as the 2nd quartile and more commonly the median p75: the 75th percentile: the value at which 75% of observations are smaller than it. This is also known as the 3rd quartile p100: the 100th percentile: the value at which 100% of observations are smaller than it. This is also known as the maximum A quick snapshot of the histogram We get an idea of how the values in both variables are distributed. For example, the mean teaching score was 4.17 out of 5 whereas the mean beauty score was 4.42 out of 10. Furthermore, the middle 50% of teaching scores were between 3.80 and 4.6 (the first and third quartiles) while the middle 50% of beauty scores were between 3.17 and 5.5 out of 10. The skim() function however only returns what are called univariate summaries, i.e. summaries about single variables at a time. Since we are considering the relationship between two numerical variables, it would be nice to have a summary statistic that simultaneously considers both variables. The correlation coefficient is a bivariate summary statistic that fits this bill. Coefficients in general are quantitative expressions of a specific property of a phenomenon. A correlation coefficient is a quantitative expression between -1 and 1 that summarizes the strength of the linear relationship between two numerical variables: -1 indicates a perfect negative relationship: as the value of one variable goes up, the value of the other variable tends to go down. 0 indicates no relationship: the values of both variables go up/down independently of each other. +1 indicates a perfect positive relationship: as the value of one variable goes up, the value of the other variable tends to go up as well. Figure 6.1 gives examples of different correlation coefficient values for hypothetical numerical variables \\(x\\) and \\(y\\). We see that while for a correlation coefficient of -0.75 there is still a negative relationship between \\(x\\) and \\(y\\), it is not as strong as the negative relationship between \\(x\\) and \\(y\\) when the correlation coefficient is -1. FIGURE 6.1: Different correlation coefficients The correlation coefficient is computed using the get_correlation() function in the moderndive package, where in this case the inputs to the function are the two numerical variables from which we want to calculate the correlation coefficient. We place the name of the response variable on the left hand side of the ~ and the explanatory variable on the right hand side of the “tilde.” We will use this same “formula” syntax with regression later in this chapter. evals_ch6 %&gt;% get_correlation(formula = score ~ bty_avg) # A tibble: 1 x 1 correlation &lt;dbl&gt; 1 0.187 The correlation coefficient can also be computed using the cor() function, where in this case the inputs to the function are the two numerical variables from which we want to calculate the correlation coefficient. Recall from Subsection 2.4.3 that the $ pulls out specific variables from a data frame: cor(x = evals_ch6$bty_avg, y = evals_ch6$score) [1] 0.187 In our case, the correlation coefficient of 0.187 indicates that the relationship between teaching evaluation score and beauty average is “weakly positive.” There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to -1, 0, and 1. For help developing such intuition and more discussion on the correlation coefficient see Subsection 6.3.1 below. Let’s now proceed by visualizing this data. Since both the score and bty_avg variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using geom_point() and set informative axes labels and title and display the result in Figure 6.2. ggplot(evals_ch6, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship of teaching and beauty scores&quot;) FIGURE 6.2: Instructor evaluation scores at UT Austin Observe the following: Most “beauty” scores lie between 2 and 8. Most teaching scores lie between 3 and 5. Recall our earlier computation of the correlation coefficient, which describes the strength of the linear relationship between two numerical variables. Looking at Figure 6.3, it is not immediately apparent that these two variables are positively related. This is to be expected given the positive, but rather weak (close to 0), correlation coefficient of 0.187. Before we continue, we bring to light an important fact about this dataset: it suffers from overplotting. Recall from the data visualization Subsection 3.3.2 that overplotting occurs when several points are stacked directly on top of each other thereby obscuring the number of points. For example, let’s focus on the 6 points in the top-right of the plot with a beauty score of around 8 out of 10: are there truly only 6 points, or are there many more just stacked on top of each other? You can think of these as ties. Let’s break up these ties with a little random “jitter” added to the points in Figure 6.3. FIGURE 6.3: Instructor evaluation scores at UT Austin: Jittered Jittering adds a little random bump to each of the points to break up these ties: just enough so you can distinguish them, but not so much that the plot is overly altered. Furthermore, jittering is strictly a visualization tool; it does not alter the original values in the dataset. Let’s compare side-by-side the regular scatterplot in Figure 6.2 with the jittered scatterplot in Figure 6.3 in Figure 6.4. FIGURE 6.4: Comparing regular and jittered scatterplots. We make several further observations: Focusing our attention on the top-right of the plot again, as noted earlier where there seemed to only be 6 points in the regular scatterplot, we see there were in fact really 9 as seen in the jittered scatterplot. A further interesting trend is that the jittering revealed a large number of instructors with beauty scores of between 3 and 4.5, towards the lower end of the beauty scale. To keep things simple in this chapter, we’ll present regular scatterplots rather than the jittered scatterplots, though we’ll keep the overplotting in mind whenever looking at such plots. Going back to scatterplot in Figure 6.2, let’s improve on it by adding a “regression line” in Figure 6.5. This is easily done by adding a new layer to the ggplot code that created Figure 6.3: + geom_smooth(method = &quot;lm&quot;). A regression line is a “best fitting” line in that of all possible lines you could draw on this plot, it is “best” in terms of some mathematical criteria. We discuss the criteria for “best” in Subsection 6.3.3 below, but we suggest you read this only after covering the concept of a residual coming up in Subsection 6.1.3. ggplot(evals_ch6, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship of teaching and beauty scores&quot;) + geom_smooth(method = &quot;lm&quot;) FIGURE 6.5: Regression line When viewed on this plot, the regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable score and the explanatory variable bty_avg. The positive slope of the blue line is consistent with our observed correlation coefficient of 0.187 suggesting that there is a positive relationship between score and bty_avg. We’ll see later however that while the correlation coefficient is not equal to the slope of this line, they always have the same sign: positive or negative. What are the grey bands surrounding the blue line? These are standard error bands, which can be thought of as error/uncertainty bands. Let’s skip this idea for now and suppress these grey bars by adding the argument se = FALSE to geom_smooth(method = &quot;lm&quot;). We’ll introduce standard errors in Chapter 9 on sampling, use them for constructing confidence intervals and conducting hypothesis tests in Chapters ?? and ??, and consider them when we revisit regression in Chapter ??. ggplot(evals_ch6, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship of teaching and beauty scores&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 6.6: Regression line without error bands Learning check (LC6.1) Conduct a new exploratory data analysis with the same outcome variable \\(y\\) being score but with age as the new explanatory variable \\(x\\). Remember, this involves three things: Looking at the raw values. Computing summary statistics of the variables of interest. Creating informative visualizations. What can you say about the relationship between age and teaching scores based on this exploration? 6.1.2 Simple linear regression You may recall from secondary school / high school algebra, in general, the equation of a line is \\(y = a + bx\\), which is defined by two coefficients. Recall we defined this earlier as “quantitative expressions of a specific property of a phenomenon.” These two coefficients are: the intercept coefficient \\(a\\), or the value of \\(y\\) when \\(x = 0\\), and the slope coefficient \\(b\\), or the increase in \\(y\\) for every increase of one in \\(x\\). However, when defining a line specifically for regression, like the blue regression line in Figure 6.6, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) where the intercept coefficient is \\(b_0\\), or the value of \\(\\widehat{y}\\) when \\(x=0\\), and the slope coefficient \\(b_1\\), or the increase in \\(\\widehat{y}\\) for every increase of one in \\(x\\). Why do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression, which we’ll introduce in the next Subsection 6.1.3 when we discuss fitted values. For now, let’s ignore the hat and treat the equation of the line as you would from secondary school / high school algebra recognizing the slope and the intercept. We know looking at Figure 6.6 that the slope coefficient corresponding to bty_avg should be positive. Why? Because as bty_avg increases, professors tend to roughly have larger teaching evaluation scores. However, what are the specific values of the intercept and slope coefficients? Let’s not worry about computing these by hand, but instead let the computer do the work for us. Specifically let’s use R! Let’s get the value of the intercept and slope coefficients by outputting something called the linear regression table. We will fit the linear regression model to the data using the lm() function and save this to score_model. lm stands for “linear model”, given that we are dealing with lines. When we say “fit”, we are saying find the best fitting line to this data. The lm() function that “fits” the linear regression model is typically used as lm(y ~ x, data = data_frame_name) where: y is the outcome variable, followed by a tilde (~). This is likely the key to the left of “1” on your keyboard. In our case, y is set to score. x is the explanatory variable. In our case, x is set to bty_avg. We call the combination y ~ x a model formula. Recall the use of this notation when we computed the correlation coefficient using the get_correlation() function in Subsection 6.1.1. data_frame_name is the name of the data frame that contains the variables y and x. In our case, data_frame_name is the evals_ch6 data frame. score_model &lt;- lm(score ~ bty_avg, data = evals_ch6) score_model Call: lm(formula = score ~ bty_avg, data = evals_ch6) Coefficients: (Intercept) bty_avg 3.8803 0.0666 This output is telling us that the Intercept coefficient \\(b_0\\) of the regression line is 3.8803 and the slope coefficient for by_avg is 0.0666. Therefore the blue regression line in Figure 6.6 is \\[\\widehat{\\text{score}} = b_0 + b_{\\text{bty avg}} \\cdot\\text{bty avg} = 3.8803 + 0.0666\\cdot\\text{ bty avg}\\] where The intercept coefficient \\(b_0 = 3.8803\\) means for instructors that had a hypothetical beauty score of 0, we would expect them to have on average a teaching score of 3.8803. In this case however, while the intercept has a mathematical interpretation when defining the regression line, there is no practical interpretation since score is an average of a panel of 6 students’ ratings from 1 to 10, a bty_avg of 0 would be impossible. Furthermore, no instructors had a beauty score anywhere near 0 in this data. Of more interest is the slope coefficient associated with bty_avg: \\(b_{\\text{bty avg}} = +0.0666\\). This is a numerical quantity that summarizes the relationship between the outcome and explanatory variables. Note that the sign is positive, suggesting a positive relationship between beauty scores and teaching scores, meaning as beauty scores go up, so also do teaching scores go up. The slope’s precise interpretation is: For every increase of 1 unit in bty_avg, there is an associated increase of, on average, 0.0666 units of score. Such interpretations need be carefully worded: We only stated that there is an associated increase, and not necessarily a causal increase. For example, perhaps it’s not that beauty directly affects teaching scores, but instead individuals from wealthier backgrounds tend to have had better education and training, and hence have higher teaching scores, but these same individuals also have higher beauty scores. Avoiding such reasoning can be summarized by the adage “correlation is not necessarily causation.” In other words, just because two variables are correlated, it doesn’t mean one directly causes the other. We discuss these ideas more in Subsection 6.3.2. We say that this associated increase is on average 0.0666 units of teaching score and not that the associated increase is exactly 0.0666 units of score across all values of bty_avg. This is because the slope is the average increase across all points as shown by the regression line in Figure 6.6. Now that we’ve learned how to compute the equation for the blue regression line in Figure 6.6 and interpreted all its terms, let’s take our modeling one step further. This time after fitting the model using the lm(), let’s get something called the regression table using the get_regression_table() function from the moderndive package: # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch6) # Get regression table: get_regression_table(score_model) TABLE 6.2: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Note how we took the output of the model fit saved in score_model and used it as an input to the subsequent get_regression_table() function. The output now looks like a table: in fact it is a data frame. The values of the intercept and slope of 3.880 and 0.0666 are now in the estimate column. But what are the remaining 5 columns: std_error, statistic, p_value, lower_ci and upper_ci? What do they tell us? They tell us about both the statistical significance and practical significance of our model results. You can think of this loosely as the “meaningfulness” of the results from a statistical perspective. We are going to put aside these ideas for now and revisit them in Chapter ?? on (statistical) inference for regression, after we’ve had a chance to cover: Standard errors in Chapter 9 (std_error) Confidence intervals in Chapter ?? (lower_ci and upper_ci) Hypothesis testing in Chapter ?? (statistic and p_value). For now, we’ll only focus on the term and estimate columns of any regression table. The get_regression_table() from the moderndive is an example of what’s known as a wrapper function in computer programming, which takes other pre-existing functions and “wraps” them into a single function. This concept is illustrated in Figure 6.7. FIGURE 6.7: The concept of a ‘wrapper’ function. So all you need to worry about is the what the inputs look like and what the outputs look like; you leave all the other details “under the hood of the car.” In our regression modeling example, the get_regression_table() has Input: A saved lm() linear regression Output: A data frame with information on the intercept and slope of the regression line. If you’re interested in learning more about the get_regression_table() function’s construction and thinking, see Subsection 6.3.4 below. Learning check (LC6.2) Fit a new simple linear regression using lm(score ~ age, data = evals_ch6) where age is the new explanatory variable \\(x\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your exploratory data analysis above? 6.1.3 Observed/fitted values and residuals We just saw how to get the value of the intercept and the slope of the regression line from the regression table generated by get_regression_table(). Now instead, say we want information on individual points. In this case, we focus on one of the \\(n = 463\\) instructors in this dataset, corresponding to a single row of evals_ch6. For example, say we are interested in the 21st instructor in this dataset: TABLE 6.3: Data for 21st instructor score bty_avg age 4.9 7.33 31 What is the value on the blue line corresponding to this instructor’s bty_avg of 7.333? In Figure 6.8 we mark three values in particular corresponding to this instructor. Red circle: This is the observed value \\(y\\) = 4.9 and corresponds to this instructor’s actual teaching score. Red square: This is the fitted value \\(\\widehat{y}\\) and corresponds to the value on the regression line for \\(x\\) = 7.333. This value is computed using the intercept and slope in the regression table above: \\[\\widehat{y} = b_0 + b_1 \\cdot x = 3.88 + 0.067 * 7.333 = 4.369\\] Blue arrow: The length of this arrow is the residual and is computed by subtracting the fitted value \\(\\widehat{y}\\) from the observed value \\(y\\). The residual can be thought of as the error or “lack of fit” of the regression line. In the case of this instructor, it is \\(y - \\widehat{y}\\) = 4.9 - 4.369 = 0.531. In other words, the model was off by 0.531 teaching score units for this instructor. FIGURE 6.8: Example of observed value, fitted value, and residual What if we want both the fitted value \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) and the residual \\(y - \\widehat{y}\\) not only the 21st instructor but for all 463 instructors in the study? Recall that each instructor corresponds to one of the 463 rows in the evals_ch6 data frame and also one of the 463 points in the regression plot in Figure 6.6. We could repeat the above calculations by hand 463 times, but that would be tedious and time consuming. Instead, let’s use the get_regression_points() function that we’ve included in the moderndive R package. Note that in the table below we only present the results for the 21st through the 24th instructors. regression_points &lt;- get_regression_points(score_model) regression_points TABLE 6.4: Regression points (for only 21st through 24th instructor) ID score bty_avg score_hat residual 21 4.9 7.33 4.37 0.531 22 4.6 7.33 4.37 0.231 23 4.5 7.33 4.37 0.131 24 4.4 5.50 4.25 0.153 Just as with the get_regression_table() function, the inputs to the get_regression_points() function are the same, however the outputs are different. Let’s inspect the individual columns: The score column represents the observed value of the outcome variable \\(y\\). The bty_avg column represents the values of the explanatory variable \\(x\\). The score_hat column represents the fitted values \\(\\widehat{y}\\). The residual column represents the residuals \\(y - \\widehat{y}\\). get_regression_points() is another example of a wrapper function we described in Figure 6.7. If you’re curious about this function as well, check out Subsection 6.3.4. Just as we did for the 21st instructor in the evals_ch6 dataset (in the first row of the table above), let’s repeat the above calculations for the 24th instructor in the evals_ch6 dataset (in the fourth row of the table above): score = 4.4 is the observed value \\(y\\) for this instructor. bty_avg = 5.50 is the value of the explanatory variable \\(x\\) for this instructor. score_hat = 4.25 = 3.88 + 0.067 * \\(x\\) = 3.88 + 0.067 * 5.50 is the fitted value \\(\\widehat{y}\\) for this instructor. residual = 0.153 = 4.4 - 4.25 is the value of the residual for this instructor. In other words, the model was off by 0.153 teaching score units for this instructor. More development of this idea appears in Section 6.3.3 and we encourage you to read that section after you investigate residuals. 6.2 One categorical explanatory variable It’s an unfortunate truth that life expectancy is not the same across various countries in the world; there are a multitude of factors that are associated with how long people live. International development agencies are very interested in studying these differences in the hope of understanding where governments should allocate resources to address this problem. In this section, we’ll explore differences in life expectancy in two ways: Differences between continents: Are there significant differences in life expectancy, on average, between the five continents of the world: Africa, the Americas, Asia, Europe, and Oceania? Differences within continents: How does life expectancy vary within the world’s five continents? For example, is the spread of life expectancy among the countries of Africa larger than the spread of life expectancy among the countries of Asia? To answer such questions, we’ll study the gapminder dataset in the gapminder package. Recall we mentioned this dataset in Subsection 3.1.2 when we first studied the “Grammar of Graphics” introduced in Figure 3.1. This dataset has international development statistics such as life expectancy, GDP per capita, and population by country (\\(n\\) = 142) for 5-year intervals between 1952 and 2007. We’ll use this data for linear regression again, but note that our explanatory variable \\(x\\) is now categorical, and not numerical like when we covered simple linear regression in Section 6.1. More precisely, we have: A numerical outcome variable \\(y\\). In this case, life expectancy. A single categorical explanatory variable \\(x\\), In this case, the continent the country is part of. When the explanatory variable \\(x\\) is categorical, the concept of a “best-fitting” line is a little different than the one we saw previously in Section 6.1 where the explanatory variable \\(x\\) was numerical. We’ll study these differences shortly in Subsection 6.2.2, but first we conduct our exploratory data analysis. 6.2.1 Exploratory data analysis Let’s load the gapminder data and filter() for only observations in 2007. Next we select() only the variables we’ll need along with gdpPercap, which is each country’s gross domestic product per capita (GDP). GDP is a rough measure of that country’s economic performance. (This will be used for the upcoming Learning Check). Lastly, we save this in a data frame with name gapminder2007: library(gapminder) gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% select(country, continent, lifeExp, gdpPercap) You should look at the raw data values both by bringing up RStudio’s spreadsheet viewer and the glimpse() function. In Table 6.5 we only show 5 randomly selected countries out of 142: View(gapminder2007) TABLE 6.5: Random sample of 5 countries country continent lifeExp gdpPercap Togo Africa 58.4 883 Sao Tome and Principe Africa 65.5 1598 Congo, Dem. Rep. Africa 46.5 278 Lesotho Africa 42.6 1569 Bulgaria Europe 73.0 10681 glimpse(gapminder2007) Observations: 142 Variables: 4 $ country &lt;fct&gt; Afghanistan, Albania, Algeria, Angola, Argentina, Australia… $ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, As… $ lifeExp &lt;dbl&gt; 43.8, 76.4, 72.3, 42.7, 75.3, 81.2, 79.8, 75.6, 64.1, 79.4,… $ gdpPercap &lt;dbl&gt; 975, 5937, 6223, 4797, 12779, 34435, 36126, 29796, 1391, 33… We see that the variable continent is indeed categorical, as it is encoded as fct which stands for “factor.” This is R’s way of storing categorical variables. Let’s once again apply the skim() function from the skimr package to our two variables of interest: continent and lifeExp: gapminder2007 %&gt;% select(continent, lifeExp) %&gt;% skim() Skim summary statistics n obs: 142 n variables: 2 ── Variable type:factor ───────────────────────── variable missing complete n n_unique top_counts continent 0 142 142 5 Afr: 52, Asi: 33, Eur: 30, Ame: 25 ordered FALSE ── Variable type:numeric ──────────────────────── variable missing complete n mean sd p0 p25 p50 p75 p100 lifeExp 0 142 142 67.01 12.07 39.61 57.16 71.94 76.41 82.6 hist ▂▂▂▂▂▃▇▇ The output now reports summaries for categorical variables (the variable type: factor) separately from the numerical variables. For the categorical variable continent it now reports: missing, complete, n as before which are the number of missing, complete, and total number of values. n_unique: The unique number of levels to this variable, corresponding to Africa, Asia, Americas, Europe, and Oceania top_counts: In this case the top four counts: Africa has 52 entries each corresponding to a country, Asia has 33, Europe has 30, and Americans has 25. Not displayed is Oceania with 2 countries ordered: Reporting whether the variable is “ordinal.” In this case, it is not ordered. Given that the global median life expectancy is 71.94, half of the world’s countries (71 countries) will have a life expectancy less than 71.94. Further, half will have a life expectancy greater than this value. The mean life expectancy of 67.01 is lower however. Why are these two values different? Let’s look at a histogram of lifeExp in Figure 6.9 to see why. FIGURE 6.9: Histogram of Life Expectancy in 2007 We see that this data is left-skewed/negatively skewed: there are a few countries with very low life expectancy that are bringing down the mean life expectancy. However, the median is less sensitive to the effects of such outliers. Hence the median is greater than the mean in this case. Let’s proceed by comparing median and mean life expectancy between continents by adding a group_by(continent) to the above code: lifeExp_by_continent &lt;- gapminder2007 %&gt;% group_by(continent) %&gt;% summarize(median = median(lifeExp), mean = mean(lifeExp)) TABLE 6.6: Life expectancy by continent continent median mean Africa 52.9 54.8 Americas 72.9 73.6 Asia 72.4 70.7 Europe 78.6 77.6 Oceania 80.7 80.7 We see now that there are differences in life expectancy between the continents. For example let’s focus on only medians. While the median life expectancy across all \\(n = 142\\) countries in 2007 was 71.935, the median life expectancy across the \\(n =52\\) countries in Africa was only 52.927. Let’s create a corresponding visualization. One way to compare the life expectancy of countries in different continents would be via a faceted histogram. Recall we saw back in the Data Visualization chapter, specifically Section 3.6, that facets allow us to split a visualization by the different levels of a categorical variable or factor variable. In Figure 6.10, the variable we facet by is continent, which is categorical with five levels, each corresponding to the five continents of the world. ggplot(gapminder2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Life expectancy&quot;, y = &quot;Number of countries&quot;, title = &quot;Life expectancy by continent&quot;) + facet_wrap(~ continent, nrow = 2) FIGURE 6.10: Life expectancy in 2007 Another way would be via a geom_boxplot where we map the categorical variable continent to the \\(x\\)-axis and the different life expectancy within each continent on the \\(y\\)-axis; we do this in Figure 6.11. ggplot(gapminder2007, aes(x = continent, y = lifeExp)) + geom_boxplot() + labs(x = &quot;Continent&quot;, y = &quot;Life expectancy (years)&quot;, title = &quot;Life expectancy by continent&quot;) FIGURE 6.11: Life expectancy in 2007 Some people prefer comparing a numerical variable between different levels of a categorical variable, in this case comparing life expectancy between different continents, using a boxplot over a faceted histogram as we can make quick comparisons with single horizontal lines. For example, we can see that even the country with the highest life expectancy in Africa is still lower than all countries in Oceania. It’s important to remember however that the solid lines in the middle of the boxes correspond to the medians (i.e. the middle value) rather than the mean (the average). So, for example, if you look at Asia, the solid line denotes the median life expectancy of around 72 years, indicating to us that half of all countries in Asia have a life expectancy below 72 years whereas half of all countries in Asia have a life expectancy above 72 years. Furthermore, note that: Africa and Asia have much more spread/variation in life expectancy as indicated by the interquartile range (the height of the boxes). Oceania has almost no spread/variation, but this might in large part be due to the fact there are only two countries in Oceania: Australia and New Zealand. Now, let’s start making comparisons of life expectancy between continents. Let’s use Africa as a baseline for comparison. Why Africa? Only because it happened to be first alphabetically, we could have just as appropriately used the Americas as the baseline for comparison. Using the “eyeball test” (just using our eyes to see if anything stands out), we make the following observations about differences in median life expectancy compared to the baseline of Africa: The median life expectancy of the Americas is roughly 20 years greater. The median life expectancy of Asia is roughly 20 years greater. The median life expectancy of Europe is roughly 25 years greater. The median life expectancy of Oceania is roughly 27.8 years greater. Let’s remember these four differences vs Africa corresponding to the Americas, Asia, Europe, and Oceania: 20, 20, 25, 27.8. Learning check (LC6.3) Conduct a new exploratory data analysis with the same explanatory variable \\(x\\) being continent but with gdpPercap as the new outcome variable \\(y\\). Remember, this involves three things: Looking at the raw values Computing summary statistics of the variables of interest. Creating informative visualizations What can you say about the differences in GDP per capita between continents based on this exploration? 6.2.2 Linear regression In Subsection 6.1.2 we introduced simple linear regression, which involves modeling the relationship between a numerical outcome variable \\(y\\) as a function of a numerical explanatory variable \\(x\\), in our life expectancy example, we now have a categorical explanatory variable \\(x\\) continent. While we still can fit a regression model, given our categorical explanatory variable we no longer have a concept of a “best-fitting” line, but rather “differences relative to a baseline for comparison.” Before we fit our regression model, let’s create a table similar to Table 6.6, but Report the mean life expectancy for each continent. Report the difference in mean life expectancy relative to Africa’s mean life expectancy of 54.806 in the column “mean vs Africa”; this column is simply the “mean” column minus 54.806. Think back to your observations from the eyeball test of Figure 6.11 at the end of the last subsection. The column “mean vs Africa” is the same idea of comparing a summary statistic to a baseline for comparison, in this case the countries of Africa, but using means instead of medians. TABLE 6.7: Mean life expectancy by continent continent mean mean vs Africa Africa 54.8 0.0 Americas 73.6 18.8 Asia 70.7 15.9 Europe 77.6 22.8 Oceania 80.7 25.9 Now, let’s use the get_regression_table() function we introduced in Section 6.1.2 to get the regression table for gapminder2007 analysis: lifeExp_model &lt;- lm(lifeExp ~ continent, data = gapminder2007) get_regression_table(lifeExp_model) TABLE 6.8: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 54.8 1.02 53.45 0 52.8 56.8 continentAmericas 18.8 1.80 10.45 0 15.2 22.4 continentAsia 15.9 1.65 9.68 0 12.7 19.2 continentEurope 22.8 1.70 13.47 0 19.5 26.2 continentOceania 25.9 5.33 4.86 0 15.4 36.5 Just as before, we have the term and estimates columns of interest, but unlike before, we now have 5 rows corresponding to 5 outputs in our table: an intercept like before, but also continentAmericas, continentAsia, continentEurope, and continentOceania. What are these values? First, we must describe the equation for fitted value \\(\\widehat{y}\\), which is a little more complicated when the \\(x\\) explanatory variable is categorical: \\[\\begin{align} \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x) \\end{align}\\] Let’s break this down. First, \\(\\mathbb{1}_{A}(x)\\) is what’s known in mathematics as an “indicator function” that takes one of two possible values: \\[ \\mathbb{1}_{A}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } x \\text{ is in } A \\\\ 0 &amp; \\text{if } \\text{otherwise} \\end{array} \\right. \\] In a statistical modeling context this is also known as a “dummy variable”. In our case, let’s consider the first such indicator variable: \\[ \\mathbb{1}_{\\mbox{Amer}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{country } x \\text{ is in the Americas} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Now let’s interpret the terms in the estimate column of the regression table. First \\(b_0 =\\) intercept = 54.8 corresponds to the mean life expectancy for countries in Africa, since for country \\(x\\) in Africa we have the following equation: \\[\\begin{align} \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 \\end{align}\\] i.e. All four of the indicator variables are equal to 0. Recall we stated earlier that we would treat Africa as the baseline for comparison group. Furthermore, this value corresponds to the group mean life expectancy for all African countries in Table 6.7. Next, \\(b_{\\text{Amer}}\\) = continentAmericas = 18.8 is the difference in mean life expectancy of countries in the Americas relative to Africa, or in other words, on average countries in the Americas had life expectancy 18.8 years greater. The fitted value yielded by this equation is: \\[\\begin{align} \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 1 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 18.8\\\\ &amp;= 72.9 \\end{align}\\] i.e. in this case, only the indicator function \\(\\mathbb{1}_{\\mbox{Amer}}(x)\\) is equal to 1, but all others are 0. Recall that 72.9 corresponds to the group mean life expectancy for all countries in the Americas in Table 6.7. Similarly, \\(b_{\\text{Asia}}\\) = continentAsia = 15.9 is the difference in mean life expectancy of Asian countries relative to Africa countries, or in other words, on average countries in the Asia had life expectancy 18.8 years greater than Africa. The fitted value yielded by this equation is: \\[\\begin{align} \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\mbox{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\mbox{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\mbox{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\mbox{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 1 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 15.9\\\\ &amp;= 70.7 \\end{align}\\] i.e. in this case, only the indicator function \\(\\mathbb{1}_{\\mbox{Asia}}(x)\\) is equal to 1, but all others are 0. Recall that 70.7 corresponds to the group mean life expectancy for all countries in Asia in Table 6.7. The same logic applies to \\(b_{\\text{Euro}} = 22.8\\) and \\(b_{\\text{Ocean}} = 25.9\\); they correspond to the “offset” in mean life expectancy for countries in Europe and Oceania, relative to the mean life expectancy of the baseline group for comparison of African countries. Let’s generalize this idea a bit. If we fit a linear regression model using a categorical explanatory variable \\(x\\) that has \\(k\\) levels, a regression model will return an intercept and \\(k - 1\\) “slope” coefficients. When \\(x\\) is a numerical explanatory variable the interpretation is of a “slope” coefficient, but when \\(x\\) is categorical the meaning is a little trickier. They are offsets relative to the baseline. In our case, since there are \\(k = 5\\) continents, the regression model returns an intercept corresponding to the baseline for comparison Africa and \\(k - 1 = 4\\) slope coefficients corresponding to the Americas, Asia, Europe, and Oceania. Africa was chosen as the baseline by R for no other reason than it is first alphabetically of the 5 continents. You can manually specify which continent to use as baseline instead of the default choice of whichever comes first alphabetically, but we leave that to a more advanced course. (The forcats package is particularly nice for doing this and we encourage you to explore using it.) Learning check (LC6.4) Fit a new linear regression using lm(gdpPercap ~ continent, data = gapminder2007) where gdpPercap is the new outcome variable \\(y\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your exploratory data analysis above? 6.2.3 Observed/fitted values and residuals Recall in Subsection 6.1.3 when we had a numerical explanatory variable \\(x\\), we defined: Observed values \\(y\\), or the observed value of the outcome variable Fitted values \\(\\widehat{y}\\), or the value on the regression line for a given \\(x\\) value Residuals \\(y - \\widehat{y}\\), or the error between the observed value and the fitted value What do fitted values \\(\\widehat{y}\\) and residuals \\(y - \\widehat{y}\\) correspond to when the explanatory variable \\(x\\) is categorical? Let’s investigate these values for the first 10 countries in the gapminder2007 dataset: TABLE 6.9: First 10 out of 142 countries country continent lifeExp gdpPercap Afghanistan Asia 43.8 975 Albania Europe 76.4 5937 Algeria Africa 72.3 6223 Angola Africa 42.7 4797 Argentina Americas 75.3 12779 Australia Oceania 81.2 34435 Austria Europe 79.8 36126 Bahrain Asia 75.6 29796 Bangladesh Asia 64.1 1391 Belgium Europe 79.4 33693 Recall the get_regression_points() function we used in Subsection 6.1.3 to return the observed value of the outcome variable, all explanatory variables, fitted values, and residuals for all points in the regression. Recall that each “point”. In this case, each row corresponds to one of 142 countries in the gapminder2007 dataset. They are also the 142 observations used to construct the boxplots in Figure 6.11. regression_points &lt;- get_regression_points(lifeExp_model) regression_points TABLE 6.10: Regression points (First 10 out of 142 countries) ID lifeExp continent lifeExp_hat residual 1 43.8 Asia 70.7 -26.900 2 76.4 Europe 77.6 -1.226 3 72.3 Africa 54.8 17.495 4 42.7 Africa 54.8 -12.075 5 75.3 Americas 73.6 1.712 6 81.2 Oceania 80.7 0.515 7 79.8 Europe 77.6 2.180 8 75.6 Asia 70.7 4.907 9 64.1 Asia 70.7 -6.666 10 79.4 Europe 77.6 1.792 Notice The fitted values lifeExp_hat \\(\\widehat{\\text{lifeexp}}\\). Countries in Africa have the same fitted value of 54.8, which is the mean life expectancy of Africa. Countries in Asia have the same fitted value of 70.7, which is the mean life expectancy of Asia. This similarly holds for countries in the Americas, Europe, and Oceania. The residual column is simply \\(y - \\widehat{y}\\) = lifeexp - lifeexp_hat. These values can be interpreted as that particular country’s deviation from the mean life expectancy of the respective continent’s mean. For example, the first row of this dataset corresponds to Afghanistan, and the residual of \\(-26.9 = 43.8 - 70.7\\) is Afghanistan’s mean life expectancy minus the mean life expectancy of all Asian countries. 6.3 Related topics 6.3.1 Correlation coefficient Let’s re-plot Figure 6.1, but now consider a broader range of correlation coefficient values in Figure 6.12. FIGURE 6.12: Different Correlation Coefficients As we suggested in Subsection 6.1.1, interpreting coefficients that are not close to the extreme values of -1 and 1 can be subjective. To develop your sense of correlation coefficients, we suggest you play the following 80’s-style video game called “Guess the correlation”! Click on the image below to do so: 6.3.2 Correlation is not necessarily causation You’ll note throughout this chapter we’ve been very cautious in making statements of the “associated effect” of explanatory variables on the outcome variables, for example our statement from Subsection 6.1.2 that “for every increase of 1 unit in bty_avg, there is an associated increase of, on average, 18.802 units of score.” We stay this because we are careful not to make causal statements. So while beauty score bty_avg is positively correlated with teaching score, does it directly cause effects on teaching score. For example, let’s say an instructor has their bty_avg reevaluated, but only after taking steps to try to boost their beauty score. Does this mean that they will suddenly be a better instructor? Or will they suddenly get higher teaching scores? Maybe? Here is another example, a not-so-great medical doctor goes through their medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares “Sleeping with shoes on cause headaches!” FIGURE 6.13: Does sleeping with shoes on cause headaches? However as some of you might have guessed, if someone is sleeping with their shoes on its probably because they are intoxicated. Furthermore, drinking more tends to cause more hangovers, and hence more headaches. In this instance, alcohol is what’s known as a confounding/lurking variable. It “lurks” behind the scenes, confounding or making less apparent, the causal effect (if any) of “sleeping with shoes on” with waking up with a headache. We can summarize this notion in Figure 6.14 with a causal graph where: Y: Is an outcome variable, here “waking up with a headache.” X: Is a treatment variable whose causal effect we are interested in, here “sleeping with shoes on.” FIGURE 6.14: Causal graph. So for example, many such studies use regression modeling where the outcome variable is set to Y and the explanatory/predictor variable is X, much as you’ve started learning how to do in this chapter. However, Figure 6.14 also includes a third variable with arrows pointing at both X and Y. Z: Is a confounding variable that affects both X &amp; Y, thus “confounding” their relationship. So as we said, alcohol will both cause people to be more likely to sleep with their shoes on as well as more likely to wake up with a headache. Thus when evaluating what causes one to wake up with a headache, its hard to tease out the effect of sleeping with shoes on versus just the alcohol. Thus our model needs to also use Z as an explanatory/predictor variable as well, in other words our doctor needs to take into account who had been drinking the night before. We’ll start covering multiple regression models that allows us to incorporate more than one variable in the next chapter. Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of potential confounding variables. Both these approaches attempt either to remove all confounding variables or take them into account as best they can, and only focus on the behavior of an outcome variable in the presence of the levels of the other variable(s). Be careful as you read studies to make sure that the writers aren’t falling into this fallacy of correlation implying causation. If you spot one, you may want to send them a link to Spurious Correlations. 6.3.3 Best fitting line Regression lines are also known as “best fitting lines”. But what do we mean by best? Let’s unpack the criteria that is used by regression to determine best. Recall the plot in Figure 6.8 where for a instructor with a beauty average score of \\(x=7.333\\) The observed value \\(y=4.9\\) was marked with a red circle The fitted value \\(\\widehat{y} = 4.369\\) on the regression line was marked with a red square The residual \\(y-\\widehat{y} = 4.9-4.369 = 0.531\\) was the length of the blue arrow. Let’s do this for another arbitrarily chosen instructor whose beauty score was \\(x=2.333\\). The residual in this case is \\(2.7 - 4.036 = -1.336\\). Another arbitrarily chosen instructor whose beauty score was \\(x=3.667\\) results in the residual in this case being \\(4.4 - 4.125 = 0.2753\\). Let’s do this one more time for another arbitrarily chosen instructor. This instructor had a beauty score of \\(x = 6\\). The residual in this case is \\(3.8 - 4.28 = -0.4802\\). Now let’s say we repeated this process for all 463 instructors in our dataset. Regression minimizes the sum of all 463 arrow lengths squared. In other words, it minimizes the sum of the squared residuals: \\[ \\sum_{i=1}^{n}(y_i - \\widehat{y}_i)^2 \\] We square the arrow lengths so that positive and negative deviations of the same amount are treated equally. That’s why alternative names for the simple linear regression line are the least-squares line and the best fitting line. It can be proven via calculus and linear algebra that this line uniquely minimizes the sum of the squared arrow lengths. For the regression line in the plot, the sum of the squared residuals is 131.879. This is the lowest possible value of the sum of the squared residuals of all possible lines we could draw on this scatterplot? How do we know this? We can mathematically prove this fact, but this requires some calculus and linear algebra, so let’s leave this proof for another course! 6.3.4 get_regression_x() functions What is going on behind the scenes with the get_regression_table() get_regression_points() from the moderndive package? Recall we introduced In Subsection 6.1.2, the get_regression_table() function that returned a regression table. In Subsection 6.1.3, the get_regression_points() function that returned information on all \\(n\\) points/observations involved in a regression? and that these were examples of wrapper functions that takes other pre-existing functions and “wraps” them in a single function. This way all the user needs to worry about is the input and the output format, and ignore what’s “under the hood.” In this subsection we “lift the hood” and see how the engine of these wrapper functions work. First, the get_regression_table() wrapper function leverages the the tidy() function in the broom package and the clean_names() function in the janitor package to generate tidy data frames with information about a regression model. Here is what the regression table from Subsection 6.1.2 looks like: score_model &lt;- lm(score ~ bty_avg, data = evals_ch6) get_regression_table(score_model) term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 The get_regression_table() function takes the above two functions that already existed in other R packages, uses them, and hides the details as seen below. This was on the editorial decision on our part as we felt the following code was unfortunately out of the reach for some new coders, so the following wrapper function was written so that users need only focus on the output. library(broom) library(janitor) score_model %&gt;% tidy(conf.int = TRUE) %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% rename(lower_ci = conf_low, upper_ci = conf_high) term estimate std_error statistic p_value lower_ci upper_ci (Intercept) 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Note that the mutate_if() function is from the dplyr package and applies the round() function with 3 significant digits precision only to those variables that are numerical. Similarly, the second get_regression_points() function is another wrapper function, but this time returning information about the points in a regression rather than the regression table. It uses the augment() function in the broom package instead of tidy() as with get_regression_points(). library(broom) library(janitor) score_model %&gt;% augment() %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% select(-c(&quot;se_fit&quot;, &quot;hat&quot;, &quot;sigma&quot;, &quot;cooksd&quot;, &quot;std_resid&quot;)) score bty_avg fitted resid 4.7 5.00 4.21 0.486 4.1 5.00 4.21 -0.114 3.9 5.00 4.21 -0.314 4.8 5.00 4.21 0.586 4.6 3.00 4.08 0.520 4.3 3.00 4.08 0.220 2.8 3.00 4.08 -1.280 4.1 3.33 4.10 -0.002 3.4 3.33 4.10 -0.702 4.5 3.17 4.09 0.409 In this case, it outputs only variables of interest to us as new regression modelers: the outcome variable \\(y\\) (score), all explanatory/predictor variables (bty_avg), all resulting fitted values \\(\\hat{y}\\) used by applying the equation of the regression line to bty_avg, and the residual \\(y - \\hat{y}\\). If you’re even more curious, take a look at the source code for these functions on GitHub. 6.4 Conclusion 6.4.1 Additional resources An R script file of all R code used in this chapter is available here. 6.4.2 What’s to come? In this chapter, you’ve seen what we call “basic regression” when you only have one explanatory variable. In Chapter 7, we’ll study multiple regression where we have more than one explanatory variable! In particular, we’ll see why we’ve been conducting the residual analyses from Subsections ?? and ??. We are actually verifying some very important assumptions that must be met for the std_error (standard error), p_value, lower_ci and upper_ci (the end-points of the confidence intervals) columns in our regression tables to have valid interpretation. Again, don’t worry for now if you don’t understand what these terms mean. After the next chapter on multiple regression, we’ll dive in! "],
["7-multiple-regression.html", "Chapter 7 Multiple Regression 7.1 Two numerical explanatory variables 7.2 One numerical &amp; one categorical explanatory variable 7.3 Related topics 7.4 Conclusion", " Chapter 7 Multiple Regression In Chapter 6 we introduced ideas related to modeling, in particular that the fundamental premise of modeling is to make explicit the relationship between an outcome variable \\(y\\) and an explanatory/predictor variable \\(x\\). Recall further the synonyms that we used to also denote \\(y\\) as the dependent variable and \\(x\\) as an independent variable or covariate. There are many modeling approaches one could take, among the most well-known being linear regression, which was the focus of the last chapter. Whereas in the last chapter we focused solely on regression scenarios where there is only one explanatory/predictor variable, in this chapter, we now focus on modeling scenarios where there is more than one. This case of regression more than one explanatory variable is known as multiple regression. You can imagine when trying to model a particular outcome variable, like teaching evaluation score as in Section 6.1 or life expectancy as in Section 6.2, it would be very useful to incorporate more than one explanatory variable. Since our regression models will now consider more than one explanatory/predictor variable, the interpretation of the associated effect of any one explanatory/predictor variables must be made in conjunction with the others. For example, say we are modeling individuals’ incomes as a function of their number of years of education and their parents’ wealth. When interpreting the effect of education on income, one has to consider the effect of their parents’ wealth at the same time, as these two variables are almost certainly related. Make note of this throughout this chapter and as you work on interpreting the results of multiple regression models into the future. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Read Section 2.3 for information on how to install and load R packages. library(ggplot2) library(dplyr) library(moderndive) library(ISLR) library(skimr) 7.1 Two numerical explanatory variables Let’s now attempt to identify factors that are associated with how much credit card debt an individual will have. The textbook An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani is an intermediate-level textbook on statistical and machine learning freely available here. It has an accompanying R package called ISLR with datasets that the authors use to demonstrate various machine learning methods. One dataset that is frequently used by the authors is the Credit dataset where predictions are made on the credit card balance held by \\(n = 400\\) credit card holders. These predictions are based on information about them like income, credit limit, and education level. Note that this dataset is not based on actual individuals, it is a simulated dataset used for educational purposes. Since no information was provided as to who these \\(n\\) = 400 individuals are and how they came to be included in this dataset, it will be hard to make any scientific claims based on this data. Recall our discussion from the previous chapter that correlation does not necessarily imply causation. That being said, we’ll still use Credit to demonstrate multiple regression with: A numerical outcome variable \\(y\\), in this case credit card balance. Two explanatory variables: A first numerical explanatory variable \\(x_1\\). In this case, their credit limit. A second numerical explanatory variable \\(x_2\\). In this case, their income (in thousands of dollars). In the forthcoming Learning Checks, we’ll consider a different scenario: The same numerical outcome variable \\(y\\): credit card balance. Two new explanatory variables: A first numerical explanatory variable \\(x_1\\): their credit rating. A second numerical explanatory variable \\(x_2\\): their age. 7.1.1 Exploratory data analysis Let’s load the Credit data and select() only the needed subset of variables. library(ISLR) Credit &lt;- Credit %&gt;% select(Balance, Limit, Income, Rating, Age) Let’s look at the raw data values both by bringing up RStudio’s spreadsheet viewer and the glimpse() function. Although in Table 7.1 we only show 5 randomly selected credit card holders out of 400: View(Credit) TABLE 7.1: Random sample of 5 credit card holders Balance Limit Income Rating Age 1259 8376 123.3 610 89 227 6033 108.0 449 64 467 4534 32.8 333 44 846 7576 94.2 527 44 436 4866 45.0 347 30 glimpse(Credit) Observations: 400 Variables: 5 $ Balance &lt;int&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0, … $ Limit &lt;int&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, 8… $ Income &lt;dbl&gt; 14.9, 106.0, 104.6, 148.9, 55.9, 80.2, 21.0, 71.4, 15.1, 71.1… $ Rating &lt;int&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, 3… $ Age &lt;int&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, 5… Let’s look at some summary statistics, again using the skim() function from the skimr package: Credit %&gt;% select(Balance, Limit, Income) %&gt;% skim() Skim summary statistics n obs: 400 n variables: 3 ── Variable type:integer ──────────────────────── variable missing complete n mean sd p0 p25 p50 p75 p100 Balance 0 400 400 520.01 459.76 0 68.75 459.5 863 1999 Limit 0 400 400 4735.6 2308.2 855 3088 4622.5 5872.75 13913 hist ▇▃▃▃▂▁▁▁ ▅▇▇▃▂▁▁▁ ── Variable type:numeric ──────────────────────── variable missing complete n mean sd p0 p25 p50 p75 p100 Income 0 400 400 45.22 35.24 10.35 21.01 33.12 57.47 186.63 hist ▇▃▂▁▁▁▁▁ We observe for example: The mean and median credit card balance are $520.01 and $459.50 respectively. 25% of card holders had debts of $68.75 or less. The mean and median credit card limit are $4735.6 and $4622.50 respectively. 75% of these card holders had incomes of $57,470 or less. Since our outcome variable Balance and the explanatory variables Limit and Income are numerical, we can compute the correlation coefficient between pairs of these variables. First, we could run the get_correlation() command as seen in Subsection 6.1.1 twice, once for each explanatory variable: Credit %&gt;% get_correlation(Balance ~ Limit) Credit %&gt;% get_correlation(Balance ~ Income) Or we can simultaneously compute them by returning a correlation matrix in Table 7.2. We can read off the correlation coefficient for any pair of variables by looking them up in the appropriate row/column combination. Credit %&gt;% select(Balance, Limit, Income) %&gt;% cor() TABLE 7.2: Correlations between credit card balance, credit limit, and income Balance Limit Income Balance 1.000 0.862 0.464 Limit 0.862 1.000 0.792 Income 0.464 0.792 1.000 For example, the correlation coefficient of: Balance with itself is 1 as we would expect based on the definition of the correlation coefficient. Balance with Limit is 0.862. This indicates a strong positive linear relationship, which makes sense as only individuals with large credit limits can accrue large credit card balances. Balance with Income is 0.464. This is suggestive of another positive linear relationship, although not as strong as the relationship between Balance and Limit. As an added bonus, we can read off the correlation coefficient of the two explanatory variables, Limit and Income of 0.792. In this case, we say there is a high degree of collinearity between these two explanatory variables. Collinearity (or multicollinearity) is a phenomenon in which one explanatory variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. So in this case, if we knew someone’s credit card Limit and since Limit and Income are highly correlated, we could make a fairly accurate guess as to that person’s Income. Or put loosely, these two variables provided redundant information. For now let’s ignore any issues related to collinearity and press on. Let’s visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots: ggplot(Credit, aes(x = Limit, y = Balance)) + geom_point() + labs(x = &quot;Credit limit (in $)&quot;, y = &quot;Credit card balance (in $)&quot;, title = &quot;Relationship between balance and credit limit&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(Credit, aes(x = Income, y = Balance)) + geom_point() + labs(x = &quot;Income (in $1000)&quot;, y = &quot;Credit card balance (in $)&quot;, title = &quot;Relationship between balance and income&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 7.1: Relationship between credit card balance and credit limit/income First, there is a positive relationship between credit limit and balance, since as credit limit increases so also does credit card balance; this is to be expected given the strongly positive correlation coefficient of 0.862. In the case of income, the positive relationship doesn’t appear as strong, given the weakly positive correlation coefficient of 0.464. However the two plots in Figure 7.1 only focus on the relationship of the outcome variable with each of the explanatory variables independently. To get a sense of the joint relationship of all three variables simultaneously through a visualization, let’s display the data in a 3-dimensional (3D) scatterplot, where The numerical outcome variable \\(y\\) Balance is on the z-axis (vertical axis) The two numerical explanatory variables form the “floor” axes. In this case The first numerical explanatory variable \\(x_1\\) Income is on of the floor axes. The second numerical explanatory variable \\(x_2\\) Limit is on the other floor axis. Click on the following image to open an interactive 3D scatterplot in your browser: Previously in Figure 6.6, we plotted a “best-fitting” regression line through a set of points where the numerical outcome variable \\(y\\) was teaching score and a single numerical explanatory variable \\(x\\) was bty_avg. What is the analogous concept when we have two numerical predictor variables? Instead of a best-fitting line, we now have a best-fitting plane, which is a 3D generalization of lines which exist in 2D. Click here to open an interactive plot of the regression plane shown below in your browser. Move the image around, zoom in, and think about how this plane generalizes the concept of a linear regression line to three dimensions. FIGURE 7.2: Regression plane Learning check (LC7.1) Conduct a new exploratory data analysis with the same outcome variable \\(y\\) being Balance but with Rating and Age as the new explanatory variables \\(x_1\\) and \\(x_2\\). Remember, this involves three things: Looking at the raw values Computing summary statistics of the variables of interest. Creating informative visualizations What can you say about the relationship between a credit card holder’s balance and their credit rating and age? 7.1.2 Multiple regression Just as we did when we had a single numerical explanatory variable \\(x\\) in Subsection 6.1.2 and when we had a single categorical explanatory variable \\(x\\) in Subsection 6.2.2, we fit a regression model and obtained the regression table in our two numerical explanatory variable scenario. To fit a regression model and get a table using get_regression_table(), we now use a + to consider multiple explanatory variables. In this case since we want to perform a regression of Limit and Income simultaneously, we input Balance ~ Limit + Income. Balance_model &lt;- lm(Balance ~ Limit + Income, data = Credit) get_regression_table(Balance_model) TABLE 7.3: Multiple regression table term estimate std_error statistic p_value lower_ci upper_ci intercept -385.179 19.465 -19.8 0 -423.446 -346.912 Limit 0.264 0.006 45.0 0 0.253 0.276 Income -7.663 0.385 -19.9 0 -8.420 -6.906 How do we interpret these three values that define the regression plane? Intercept: -$385.18 (rounded to two decimal points to represent cents). The intercept in our case represents the credit card balance for an individual who has both a credit Limit of $0 and Income of $0. In our data however, the intercept has limited practical interpretation as no individuals had Limit or Income values of $0 and furthermore the smallest credit card balance was $0. Rather, it is used to situate the regression plane in 3D space. Limit: $0.26. Now that we have multiple variables to consider, we have to add a caveat to our interpretation: taking all other variables in our model into account, for every increase of one unit in credit Limit (dollars), there is an associated increase of on average $0.26 in credit card balance. Note: Just as we did in Subsection 6.1.2, we are not making any causal statements, only statements relating to the association between credit limit and balance We need to preface our interpretation of the associated effect of Limit with the statement “taking all other variables into account”, in this case Income, to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model and not in isolation. Income: -$7.66. Similarly, taking all other variables into account, for every increase of one unit in Income (in other words, $1000 in income), there is an associated decrease of on average $7.66 in credit card balance. However, recall in Figure 7.1 that when considered separately, both Limit and Income had positive relationships with the outcome variable Balance. As card holders’ credit limits increased their credit card balances tended to increase as well, and a similar relationship held for incomes and balances. In the above multiple regression, however, the slope for Income is now -7.66, suggesting a negative relationship between income and credit card balance. What explains these contradictory results? This is known as Simpson’s Paradox, a phenomenon in which a trend appears in several different groups of data but disappears or reverses when these groups are combined. We expand on this in Subsection 7.3.2 where we’ll look at the relationship between credit Limit and credit card balance but split by different income bracket groups. Learning check (LC7.2) Fit a new simple linear regression using lm(Balance ~ Rating + Age, data = Credit) where Rating and Age are the new numerical explanatory variables \\(x_1\\) and \\(x_2\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your exploratory data analysis above? 7.1.3 Observed/fitted values and residuals As we did previously in Table 7.4, let’s unpack the output of the get_regression_points() function for our model for credit card balance for all 400 card holders in the dataset. Recall that each card holder corresponds to one of the 400 rows in the Credit data frame and also for one of the 400 3D points in the 3D scatterplots in Subsection 7.1.1. regression_points &lt;- get_regression_points(Balance_model) regression_points TABLE 7.4: Regression points (first 5 rows of 400) ID Balance Limit Income Balance_hat residual 1 333 3606 14.9 454 -120.8 2 903 6645 106.0 559 344.3 3 580 7075 104.6 683 -103.4 4 964 9504 148.9 986 -21.7 5 331 4897 55.9 481 -150.0 Recall the format of the output: Balance corresponds to \\(y\\) (the observed value) Balance_hat corresponds to \\(\\widehat{y}\\) (the fitted value) residual corresponds to \\(y - \\widehat{y}\\) (the residual) 7.2 One numerical &amp; one categorical explanatory variable Let’s revisit the instructor evaluation data introduced in Section 6.1, where we studied the relationship between instructor evaluation scores and their beauty scores. This analysis suggested that there is a positive relationship between bty_avg and score, in other words as instructors had higher beauty scores, they also tended to have higher teaching evaluation scores. Now let’s say instead of bty_avg we are interested in the numerical explanatory variable \\(x_1\\) age and furthermore we want to use a second explanatory variable \\(x_2\\), the (binary) categorical variable gender. Note: This study only focused on the gender binary of &quot;male&quot; or &quot;female&quot; when the data was collected and analyzed years ago. It has been tradition to use gender as an “easy” binary variable in the past in statistical analyses. We have chosen to include it here because of the interesting results of the study, but we also understand that a segment of the population is not included in this dichotomous assignment of gender and we advocate for more inclusion in future studies to show representation of groups that do not identify with the gender binary. We now resume our analyses using this evals data and hope that others find these results interesting and worth further exploration. Our modeling scenario now becomes A numerical outcome variable \\(y\\). As before, instructor evaluation score. Two explanatory variables: A numerical explanatory variable \\(x_1\\): in this case, their age. A categorical explanatory variable \\(x_2\\): in this case, their binary gender. 7.2.1 Exploratory data analysis Let’s reload the evals data and select() only the needed subset of variables. Note that these are different than the variables chosen in Chapter 6. Let’s given this the name evals_ch7. evals_ch7 &lt;- evals %&gt;% select(score, age, gender) Let’s look at the raw data values both by bringing up RStudio’s spreadsheet viewer and the glimpse() function, although in Table 7.5 we only show 5 randomly selected instructors out of 463: View(evals_ch7) TABLE 7.5: Random sample of 5 instructors score age gender 3.7 62 male 4.7 46 female 4.8 62 male 2.8 62 male 4.0 64 male Let’s look at some summary statistics using the skim() function from the skimr package: evals_ch7 %&gt;% skim() Skim summary statistics n obs: 463 n variables: 3 ── Variable type:factor ───────────────────────── variable missing complete n n_unique top_counts ordered gender 0 463 463 2 mal: 268, fem: 195, NA: 0 FALSE ── Variable type:integer ──────────────────────── variable missing complete n mean sd p0 p25 p50 p75 p100 hist age 0 463 463 48.37 9.8 29 42 48 57 73 ▅▅▅▇▅▇▂▁ ── Variable type:numeric ──────────────────────── variable missing complete n mean sd p0 p25 p50 p75 p100 hist score 0 463 463 4.17 0.54 2.3 3.8 4.3 4.6 5 ▁▁▂▃▅▇▇▆ Furthermore, let’s compute the correlation between two numerical variables we have score and age. Recall that correlation coefficients only exist between numerical variables. We observe that they are weakly negatively correlated. evals_ch7 %&gt;% get_correlation(formula = score ~ age) # A tibble: 1 x 1 correlation &lt;dbl&gt; 1 -0.107 In Figure 7.3, we plot a scatterplot of score over age. Given that gender is a binary categorical variable in this study, we can make some interesting tweaks: We can assign a color to points from each of the two levels of gender: female and male. Furthermore, the geom_smooth(method = &quot;lm&quot;, se = FALSE) layer automatically fits a different regression line for each since we have provided color = gender at the top level in ggplot(). This allows for all geom_etries that follow to have the same mapping of aes()thetics to variables throughout the plot. ggplot(evals_ch7, aes(x = age, y = score, color = gender)) + geom_jitter() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 7.3: Instructor evaluation scores at UT Austin split by gender (jittered) We notice some interesting trends: There are almost no women faculty over the age of 60. We can see this by the lack of red dots above 60. Fitting separate regression lines for men and women, we see they have different slopes. We see that the associated effect of increasing age seems to be much harsher for women than men. In other words, as women age, the drop in their teaching score appears to be faster. 7.2.2 Multiple regression: Parallel slopes model Much like we started to consider multiple explanatory variables using the + sign in Subsection 7.1.2, let’s fit a regression model and get the regression table. This time we provide the name of score_model_2 to our regression model fit, in so as to not overwrite the model score_model from Section 6.1.2. score_model_2 &lt;- lm(score ~ age + gender, data = evals_ch7) get_regression_table(score_model_2) TABLE 7.6: Regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 4.484 0.125 35.79 0.000 4.238 4.730 age -0.009 0.003 -3.28 0.001 -0.014 -0.003 gendermale 0.191 0.052 3.63 0.000 0.087 0.294 The modeling equation for this scenario is: \\[\\begin{align} \\widehat{y} &amp;= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ \\widehat{\\mbox{score}} &amp;= b_0 + b_{\\mbox{age}} \\cdot \\mbox{age} + b_{\\mbox{male}} \\cdot \\mathbb{1}_{\\mbox{is male}}(x) \\end{align}\\] where \\(\\mathbb{1}_{\\mbox{is male}}(x)\\) is an indicator function for sex == male. In other words, \\(\\mathbb{1}_{\\mbox{is male}}(x)\\) equals one if the current observation corresponds to a male professor, and 0 if the current observation corresponds to a female professor. This model can be visualized in Figure 7.4. FIGURE 7.4: Instructor evaluation scores at UT Austin by gender: same slope We see that: Females are treated as the baseline for comparison for no other reason than “female” is alphabetically earlier than “male.” The \\(b_{male} = 0.1906\\) is the vertical “bump” that men get in their teaching evaluation scores. Or more precisely, it is the average difference in teaching score that men get relative to the baseline of women. Accordingly, the intercepts (which in this case make no sense since no instructor can have an age of 0) are : for women: \\(b_0\\) = 4.484 for men: \\(b_0 + b_{male}\\) = 4.484 + 0.191 = 4.675 Both men and women have the same slope. In other words, in this model the associated effect of age is the same for men and women. So for every increase of one year in age, there is on average an associated change of \\(b_{age}\\) = -0.009 (a decrease) in teaching score. But wait, why is Figure 7.4 different than Figure 7.3! What is going on? What we have in the original plot is known as an interaction effect between age and gender. Focusing on fitting a model for each of men and women, we see that the resulting regression lines are different. Thus, gender appears to interact in different ways for men and women with the different values of age. 7.2.3 Multiple regression: Interaction model We say a model has an interaction effect if the associated effect of one variable depends on the value of another variable. These types of models usually prove to be tricky to view on first glance because of their complexity. In this case, the effect of age will depend on the value of gender. Put differently, the effect of age on teaching scores will differ for men and for women, as was suggested by the different slopes for men and women in our visual exploratory data analysis in Figure 7.3. Let’s fit a regression with an interaction term. Instead of using the + sign in the enumeration of explanatory variables, we use the * sign. Let’s fit this regression and save it in score_model_3, then we get the regression table using the get_regression_table() function as before. score_model_interaction &lt;- lm(score ~ age * gender, data = evals_ch7) get_regression_table(score_model_interaction) TABLE 7.7: Regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 4.883 0.205 23.80 0.000 4.480 5.286 age -0.018 0.004 -3.92 0.000 -0.026 -0.009 gendermale -0.446 0.265 -1.68 0.094 -0.968 0.076 age:gendermale 0.014 0.006 2.45 0.015 0.003 0.024 The modeling equation for this scenario is: \\[\\begin{align} \\widehat{y} &amp;= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2\\\\ \\widehat{\\mbox{score}} &amp;= b_0 + b_{\\mbox{age}} \\cdot \\mbox{age} + b_{\\mbox{male}} \\cdot \\mathbb{1}_{\\mbox{is male}}(x) + b_{\\mbox{age,male}} \\cdot \\mbox{age} \\cdot \\mathbb{1}_{\\mbox{is male}}(x) \\end{align}\\] Oof, that’s a lot of rows in the regression table output and a lot of terms in the model equation. The fourth term being added on the right hand side of the equation corresponds to the interaction term. Let’s simplify things by considering men and women separately. First, recall that \\(\\mathbb{1}_{\\mbox{is male}}(x)\\) equals 1 if a particular observation (or row in evals_ch7) corresponds to a male instructor. In this case, using the values from the regression table the fitted value of \\(\\widehat{\\mbox{score}}\\) is: \\[\\begin{align} \\widehat{\\mbox{score}} &amp;= b_0 + b_{\\mbox{age}} \\cdot \\mbox{age} + b_{\\mbox{male}} \\cdot \\mathbb{1}_{\\mbox{is male}}(x) + b_{\\mbox{age,male}} \\cdot \\mbox{age} \\cdot \\mathbb{1}_{\\mbox{is male}}(x) \\\\ &amp;= b_0 + b_{\\mbox{age}} \\cdot \\mbox{age} + b_{\\mbox{male}} \\cdot 1 + b_{\\mbox{age,male}} \\cdot \\mbox{age} \\cdot 1 \\\\ &amp;= \\left(b_0 + b_{\\mbox{male}}\\right) + \\left(b_{\\mbox{age}} + b_{\\mbox{age,male}} \\right) \\cdot \\mbox{age} \\\\ &amp;= \\left(4.883 + -0.446\\right) + \\left(-0.018 + 0.014 \\right) \\cdot \\mbox{age} \\\\ &amp;= 4.437 -0.004 \\cdot \\mbox{age} \\end{align}\\] Second, recall that \\(\\mathbb{1}_{\\mbox{is male}}(x)\\) equals 0 if a particular observation corresponds to a female instructor. Again, using the values from the regression table the fitted value of \\(\\widehat{\\mbox{score}}\\) is: \\[\\begin{align} \\widehat{\\mbox{score}} &amp;= b_0 + b_{\\mbox{age}} \\cdot \\mbox{age} + b_{\\mbox{male}} \\cdot \\mathbb{1}_{\\mbox{is male}}(x) + b_{\\mbox{age,male}} \\cdot \\mbox{age} \\cdot \\mathbb{1}_{\\mbox{is male}}(x) \\\\ &amp;= b_0 + b_{\\mbox{age}} \\cdot \\mbox{age} + b_{\\mbox{male}} \\cdot 0 + b_{\\mbox{age,male}}\\mbox{age} \\cdot 0 \\\\ &amp;= b_0 + b_{\\mbox{age}} \\cdot \\mbox{age}\\\\ &amp;= 4.883 -0.018 \\cdot \\mbox{age} \\end{align}\\] Let’s summarize these values in a table: TABLE 7.8: Comparison of male and female intercepts and age slopes Gender Intercept Slope for age Male instructors 4.44 -0.004 Female instructors 4.88 -0.018 We see that while male instructors have a lower intercept, as they age, they have a less steep associated average decrease in teaching scores: 0.004 teaching score units per year as opposed to -0.018 for women. This is consistent with the different slopes and intercepts of the red and blue regression lines fit in Figure 7.3. Recall our definition of a model having an interaction effect: when the associated effect of one variable, in this case age, depends on the value of another variable, in this case gender. But how do we know when it’s appropriate to include an interaction effect? For example, which is the more appropriate model? The regular multiple regression model without an interaction term we saw in Section 7.2.2 or the multiple regression model with the interaction term we just saw? We’ll revisit this question in Chapter ?? on “inference for regression.” 7.2.4 Observed/fitted values and residuals Now say we want to apply the above calculations for male and female instructors for all 463 instructors in the evals_ch7 dataset. As our multiple regression models get more and more complex, computing such values by hand gets more and more tedious. The get_regression_points() function spares us this tedium and returns all fitted values and all residuals. For simplicity, let’s focus only on the fitted interaction model, which is saved in score_model_interaction. regression_points &lt;- get_regression_points(score_model_interaction) regression_points TABLE 7.9: Regression points (first 5 rows of 463) ID score age gender score_hat residual 1 4.7 36 female 4.25 0.448 2 4.1 36 female 4.25 -0.152 3 3.9 36 female 4.25 -0.352 4 4.8 36 female 4.25 0.548 5 4.6 59 male 4.20 0.399 Recall the format of the output: score corresponds to \\(y\\) the observed value score_hat corresponds to \\(\\widehat{y} = \\widehat{\\mbox{score}}\\) the fitted value residual corresponds to the residual \\(y - \\widehat{y}\\) 7.3 Related topics 7.3.1 More on the correlation coefficient Recall from Table 7.2 that we saw the correlation coefficient between Income in thousands of dollars and credit card Balance was 0.464. What if in instead we looked at the correlation coefficient between Income and credit card Balance, but where Income was in dollars and not thousands of dollars? This can be done by multiplying Income by 1000. library(ISLR) data(Credit) Credit %&gt;% select(Balance, Income) %&gt;% mutate(Income = Income * 1000) %&gt;% cor() TABLE 7.10: Correlation between income (in dollars) and credit card balance Balance Income Balance 1.000 0.464 Income 0.464 1.000 We see it is the same! We say that the correlation coefficient is invariant to linear transformations! In other words, the correlation between \\(x\\) and \\(y\\) will be the same as the correlation between \\(a\\times x + b\\) and \\(y\\) where \\(a\\) and \\(b\\) are numerical values (real numbers in mathematical terms). 7.3.2 Simpson’s Paradox Recall in Section 7.1, we saw the two following seemingly contradictory results when studying the relationship between credit card balance, credit limit, and income. On the one hand, the right hand plot of Figure 7.1 suggested that credit card balance and income were positively related: FIGURE 7.5: Relationship between credit card balance and credit limit/income On the other hand, the multiple regression in Table 7.3, suggested that when modeling credit card balance as a function of both credit limit and income at the same time, credit limit has a negative relationship with balance, as evidenced by the slope of -7.66. How can this be? First, let’s dive a little deeper into the explanatory variable Limit. Figure 7.6 shows a histogram of all 400 values of Limit, along with vertical red lines that cut up the data into quartiles, meaning: 25% of credit limits were between $0 and $3088. Let’s call this the “low” credit limit bracket. 25% of credit limits were between $3088 and $4622. Let’s call this the “medium-low” credit limit bracket. 25% of credit limits were between $4622 and $5873. Let’s call this the “medium-high” credit limit bracket. 25% of credit limits were over $5873. Let’s call this the “high” credit limit bracket. `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. FIGURE 7.6: Histogram of credit limits and quartiles Let’s now display The scatterplot showing the relationship between credit card balance and limit (the right-hand plot of Figure 7.1). The scatterplot showing the relationship between credit card balance and limit now with a color aesthetic added corresponding to the credit limit bracket. FIGURE 7.7: Relationship between credit card balance and income for different credit limit brackets In the right-hand plot, the Red points (bottom-left) correspond to the low credit limit bracket. Green points correspond to the medium-low credit limit bracket. Blue points correspond to the medium-high credit limit bracket. Purple points (top-right) correspond to the high credit limit bracket. The left-hand plot focuses of the relationship between balance and income in aggregate, but the right-hand plot focuses on the relationship between balance and income broken down by credit limit bracket. Whereas in aggregate there is an overall positive relationship, when broken down we now see that for the low (red points), medium-low (green points), and medium-high (blue points) income bracket groups, the strong positive relationship between credit card balance and income disappears! Only for the high bracket does the relationship stay somewhat positive. In this example, credit limit is a confounding variable for credit card balance and income. 7.4 Conclusion 7.4.1 Additional resources An R script file of all R code used in this chapter is available here. 7.4.2 What’s to come? Congratulations! We’re ready to proceed to the third portion of this book: “statistical inference” using a new package called infer. Once we’ve covered Chapters 9 on sampling, ?? on confidence intervals, and ?? on hypothesis testing, we’ll come back to the models we’ve seen in “data modeling” in Chapter ?? on inference for regression. As we said at the end of Chapter 6, we’ll see why we’ve been conducting the residual analyses from Subsections ?? and ??. We are actually verifying some very important assumptions that must be met for the std_error (standard error), p_value, conf_low and conf_high (the end-points of the confidence intervals) columns in our regression tables to have valid interpretation. Up next: "],
["8-populations.html", "Chapter 8 Populations 8.1 Terminology &amp; Notation 8.2 Populations &amp; Sampling 8.3 Samples &amp; Populations", " Chapter 8 Populations In this chapter we kick off the third segment of this book, statistical inference, by learning about samples and populations. Until now, this book has focused on how to analyze data in a sample. In many instances, your goal is not to understand insights and trends in the sample but instead to make inferences from such insights and trends observed to trends in a larger population. This chapter provides a broad overview of the concepts of samples and populations and the links between them. In Chapters 9 and 10 we will provide a more theoretical connection between these two based on the theory of repeated samples and properties of sampling distributions, respectively. As mentioned before, the concepts throughout this text all build into a culmination allowing you to “think with data.” 8.1 Terminology &amp; Notation In Parts I and II of this book, you were provided with sample data, methods for exploring this data visually, and methods for summarizing trends in this data. These methods are what we call descriptive statistics, as they are focused on describing the observed sample. In science and policy, the goal of analysis is typically not just to understand trends in a sample but instead to make inferences from this sample to trends in a population. For example, in order to understand the relationship between party affiliation and voting, you might conduct a poll in a sample of 100 (or 1,000) voters by mail, phone-call, or by stopping them on the street. Or, in order to determine if a new drug effectively reduces high blood pressure, you might conduct a randomized experiment in a sample of 200 patients experiencing high blood pressure. Why not use population data instead? While a census of every person (or unit) in a population would be ideal, it’s not hard to see that doing so is costly in many regards — financially and in terms of time and personnel. In order to understand the relationship between samples and populations, we begin by providing some vocabulary and notation that we will use throughout the remainder of the book. Population: A population is a collection of individuals or units about which we are interested. We mathematically denote the population’s size using upper-case \\(N\\). Sample: A sample is a collection of individuals or units from a population. These are the individuals or units about which we have data. We mathematically denote the sample’s size, the number of people or units we have data on, using lower-case \\(n\\). Population parameter: A population parameter is a numerical value that summarizes the population. In almost all cases the population parameter is unknown, but we wish to know it. Population parameters are typically denoted mathematically using Greek letters. For example, you may want to know the population mean, which is typically written as \\(\\mu\\) (pronounced “mu”). Or, you might want to know the population proportion, which is typically written as \\(\\pi\\) (pronounced “pi”). Population size, \\(N\\), is a case where we do not follow the Greek letter convention to denote a population parameter. Sample statistic / estimate : A sample statistic is a numerical value that summarizes the sample and can be used to estimate an unknown population parameter. It is common for a sample statistic to sometimes be called an estimate or point estimate. These are sometimes denoted mathematically using Roman letters (that correspond to Greek letters) or via inclusion of a “hat” above the population parameter (called hat-notation). For example, the population proportion \\(\\pi\\) can be estimated using a sample proportion which is denoted with \\(\\hat{\\pi}\\) or \\(p\\). The population mean \\(\\mu\\) can be estimated using the sample mean which is denoted \\(\\hat{\\mu}\\) or \\(\\overline{x}\\). Obviously \\(\\overline{x}\\) doesn’t follow either the Greek letter or hat-notation conventions, but it is the standard notation for the sample mean. Census: A census is an exhaustive collection of a measurement on all \\(N\\) individuals or units in the population in order to compute the exact value of a population parameter for the given measure. Random sampling: Random sampling is the act of using a random procedure to select individuals or units from a population that we will collect measurements on. Random sampling is extremely useful when we don’t have the means to perform a census. Here “random” means that every individual or unit in the population has a chance of being selected and that the process of selection is uncorrelated with the data itself. For example, a random procedure might involve rolling dice, selecting slips of paper out of a hat, or using a random number generator. Importantly, procedures that involve selecting the sample based upon observed characteristics (e.g., age, gender) are not random. The first two of these definitions makes clear that the data you have in hand (sample) is being used to make inferences to a larger set of data you don’t have (population). Definitions 3 and 4 refine these further, focusing on the specific numbers you wish you knew (population parameter) and the ones you are able to calculate using your data (estimate / sample statistic). Definitions 5 and 6 refer to how a sample is related to a population — either they are the same (census) or the mechanism through which they are related needs to be clear (random sampling). The goal is to use data in the sample to make inferences to a value in the population. The act of “inferring” is to deduce or conclude (information) from evidence and reasoning. Statistical inference is the theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling (Wikipedia). In other words, statistical inference is the act of inference via sampling. 8.2 Populations &amp; Sampling Recall that a population is a collection of individuals or observations that you would like to make inferences about. Some examples of populations are: Citizens of voting age (18 or older) in the United States. Students in public elementary schools in Texas. Private hospitals receiving Medicaid funding in California. Fish in Lake Michigan. In each case, the definition of the population includes clear inclusion / exclusion criteria. These help to clarify where inferences are appropriate to be made and where they are not. In order to select a sample from a population, a population frame must be created. This frame includes a list of all possible individuals or observations within the population. Sometimes this frame is difficult to make - and the result is that the population frame may not be exactly the same as the population. For example, for the above populations, population frames might be: A list of phone numbers registered to individuals in the United States. (Once contacted, only those that are citizens 18 and older would be able to be included.) A list of public elementary schools (not students), available for the prior year in the Texas public education state longitudinal data system. A list of private hospitals made available from the state of California government in a database collected every five years. (Once contacted, only those receiving &gt; $0 Medicaid would be included). Areas of Lake Michigan where it is possible to fish (e.g, excluding coves). When this population frame differs from the population, undercoverage can occur - i.e., there are parts of the population that may not be able to be studied. For example, citizens over 18 without phone numbers would have a 0% chance of being included in the sample even though they are part of the population of interest. It is important in research to make this clear and to understand how these differences might impact results. Once a population frame is defined, a sampling process is developed that, based upon a random procedure, allows for making clear inferences from the sample to the population. There are many possible sampling procedures, some of which include: Simple random sampling: Individuals or observations are selected randomly from the population, each having an equal chance of being selected. Random sampling with unequal probability: Individuals or observations are selected randomly, but the probability of selection varies proportional to size or some other relevant characteristic. Cluster sampling: In order to reach individuals or observations, first clusters are selected (e.g. schools, neighborhoods, hospitals, etc.), and then within these clusters, individuals or observations are randomly selected. Stratified sampling: In order to represent the population well, first the population is divided into sub-groups (strata) that are similar to one another, and then within these sub-groups (strata), individuals or observations are randomly selected. Observations or clusters can be selected with equal probability or unequal probability — the most important feature is that the probability of being selected is known and defined in advance of selection. In the above examples, these procedures might be used: Simple random sampling: Phone numbers are randomly selected with equal probability. Cluster sampling: First schools (clusters) are randomly selected with unequal probability (e.g., larger schools have a bigger chance of being selected), and then within those schools selected, students are randomly selected with equal probability. Random sampling with unequal probability: Hospitals are selected randomly with unequal probability (e.g., larger hospitals have a bigger chance of being selected). Stratified sampling: Lake Michigan is geographically divided into four regions (strata): those nearby to the shore in urban areas, those nearby the shore in non-urban areas, those in the middle north, and those in the middle south. It is expected that the number and kinds of fish differ across these regions. Within each region, fish are selected randomly based upon a catch-and-release procedure. In all of these cases, because the sample is selected randomly from the population, estimates from the sample can be used to make inferences regarding values of the population parameters. For example, a sample mean calculated in a random sample of a population can be used to make inferences regarding the value of the population mean. Without this random selection, these inferences would be unwarranted. Finally, note that in the examples and data we use in this book and course, we focus on random sampling with equal probabilities of selection (i.e. simple random sampling). Methods to account for clustering, stratification, and unequal selection probabilities require use of weights and, sometimes, more complicated models. Courses on survey sampling, regression, and hierarchical linear models will provide more background and details on these cases. 8.3 Samples &amp; Populations As an analyst, you will often encounter samples of data that come from unspecified or unclear populations. For example, you: developed a survey regarding relationship preferences on SurveyMonkey and then promoted completing this survey on Facebook. Now you have a sample of \\(n = 200\\) completed surveys - but what population do they come from? conducted an experiment in a psychology lab. The experiment is advertised to students in Introductory Psychology courses and in fliers around campus. You now have a sample of \\(n = 50\\) participants in the experiment - but what population do they come from? scraped some data off the web regarding movie reviews on Rotten Tomatoes. You now have a huge sample of \\(n = 10,000\\) reviews by people - but what population do these reviews come from? In these situations, statistically you do two things: You can assume that the sample you have is a random sample from some population. You can thus make inferences to this larger population using the sampling theory we will develop in the next chapters. You need to define as clearly as you can what population this sample is from. This involves using clear inclusion / exclusion criteria. Finally, keep in mind that no study generalizes everywhere. It is your job as analyst to make clear where results might be useful for making inferences and where they may not. To do this requires describing characteristics of the sample clearly when interpreting results and making inferences. In general: Ask: Do the results apply to all individuals or observations? If not, think through how the results might be dependent upon the types of individuals or observations in your sample. Report clearly the characteristics that might affect interpretation and inferences (e.g., race, ethnicity, gender, age, education). Report clearly how the data was generated. Was it from on online survey? Report this. Was it from a lab study advertised in a college? Report this. "],
["9-sampling.html", "Chapter 9 Sampling Distributions 9.1 Theory of Repeated Samples 9.2 Sampling Activity 9.3 Computer simulation 9.4 Properties of Sampling Distributions 9.5 Sample Size and Sampling Distributions 9.6 Central Limit Theorem (CLT)", " Chapter 9 Sampling Distributions In Chapter 8 we introduced inferential statistics by explaining that a sample can be treated as a random sample from some population and that estimates calculated in samples can be used to make inferences regarding parameter values in populations. In this chapter we focus on how these inferences can be made using the theory of repeated sampling. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 2.3 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(moderndive) library(dslabs) 9.1 Theory of Repeated Samples Imagine that you want to know the proportion of individuals at a football game that are cheering for the home team, so you take a random sample of \\(n=100\\) people and find that \\(88\\) of them are home team fans. From this sample you calculate \\(\\hat{\\pi} = 88/100 = 0.88\\), which is an estimate of the population proportion \\(\\pi\\). Does this mean that the population proportion is \\(\\pi = 0.88\\)? The sample was selected randomly, so inferences are straightforward, right? It’s not this simple! Statisticians approach this problem by focusing not on the sample in hand, but instead on possible other samples. We will call this counter-factual thinking. This means that in order to understand the relationship between a sample estimate we are able to compute in our data and the population parameter we have to understand how results might differ if instead of our sample, we had another possible (equally likely) sample. That is, How would our estimate of the proportion of home fans at the football game change if instead of these \\(n = 100\\) individuals we had randomly selected a different \\(n = 100\\) individuals? While it is not possible to travel back in time for this particular question, we can instead generate a way to study this exact question by creating a simulation. To do so, we begin by creating a situation in which we know the outcome (e.g., what team each person is cheering for) for every individual or observation in a population. As a result, we know what the population parameter value is (e.g., \\(\\pi = 0.91\\)). Now we randomly select a sample of \\(n = 100\\) observations and calculate the sample proportion (e.g. \\(\\hat{\\pi}=0.88\\)). And then we repeat this sampling process – each time selecting a different possible sample of \\(n = 100\\) – many, many, many times (e.g., \\(10,000\\) different samples of \\(n = 100\\)), each time calculating the sample proportion. At the end of this process, we now have many different estimates of the population proportion, each equally likely. set.seed(76) ##simulate a population of 40000 fans that have a 91% chance of being a home fan using the rbinom function ##for the variable home_fan, 1 = Yes and 0 = No football_fans &lt;- data.frame(home_fan = rbinom(40000, 1, 0.91)) ##take 10000 samples of 100 fans samples_football_fans &lt;- rep_sample_n(football_fans, size = 100, reps = 10000) ##compute the proportion of home fans for each of the 10,000 samples home_fan_results &lt;- samples_football_fans %&gt;% group_by(replicate) %&gt;% summarise(num_home_fans = sum(home_fan), prop_home_fans = num_home_fans / 100) TABLE 9.1: Proportion of home fans for 10 out of 10,000 samples of size 100 replicate num_home_fans prop_home_fans 1 89 0.89 2 90 0.90 3 92 0.92 4 94 0.94 5 90 0.90 6 90 0.90 7 88 0.88 8 95 0.95 9 92 0.92 10 90 0.90 Table 9.1 shows the simulation results for the first 10 samples, and Figure 9.1 shows a histogram of all 10,000 sample estimates. Later on in this chapter we will go through the code step-by-step, and you will learn how to conduct this type of simulation yourself. We do this simulation in order to understand how close any one sample’s estimate is to the true population proportion. For example, it may be that we are usually within \\(\\pm 2\\%\\)? Or maybe \\(\\pm 5\\%\\)? We can ask further questions, like: on average, is the sample proportion the same as the population proportion? FIGURE 9.1: Distribution of 10,000 proportions based on 10,000 samples of size 100 [1] 0.91 It is important to emphasize here that this process is theoretical. In real life, you do not know the values for all individuals or observations in a population. (If you did, why sample?) And in real life, you will not take repeated samples. Instead, you will have in front of you a single sample that you will need to use to make inferences to a population. What this simulation exercise provides is properties that can help you understand how far off the sample estimate you have in hand might be for the population parameter you care about. 9.2 Sampling Activity In the previous section, we provided an overview of repeated sampling and why the theoretical exercise is useful for understanding how to make inferences. This way of thinking, however, can be hard in the abstract. In this section, we provide a concrete example. 9.2.1 What proportion of this bowl’s balls are red? Take a look at the bowl in Figure 9.2. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand as there does not seem to be any particular pattern to the spatial distribution of red and white balls. Let’s now ask ourselves, what proportion of this bowl’s balls are red? FIGURE 9.2: A bowl with red and white balls. One way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However this would be a long and tedious process. 9.2.2 Using the shovel once Instead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 9.3. FIGURE 9.3: Inserting a shovel into the bowl. Using the shovel we remove a number of balls as seen in Figure 9.4. FIGURE 9.4: Fifty balls from the bowl. Observe that 17 of the balls are red and there are a total of 5 x 10 = 50 balls and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count, our guess of 34% took much less time and energy to obtain. However, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe? What if we repeated this exercise several times? Would I obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s actually do and observe the results with the help of 33 of our friends. 9.2.3 Using the shovel 33 times Each of our 33 friends will do the following: use the shovel to remove 50 balls each, count the number of red balls, use this number to compute the proportion of the 50 balls they removed that are red, return the balls into the bowl, and mix the contents of the bowl a little to not let a previous group’s results influence the next group’s set of results. FIGURE 9.5: Repeating sampling activity 33 times. However, before returning the balls into the bowl, they are going to mark the proportion of the 50 balls they removed that are red in a histogram as seen in Figure 9.6. FIGURE 9.6: Constructing a histogram of proportions. Recall from Section 3.5 that histograms allow us to visualize the distribution of a numerical variable: where the values center and in particular how they vary. The resulting hand-drawn histogram can be seen in Figure 9.7. FIGURE 9.7: Hand-drawn histogram of 33 proportions. Observe the following about the histogram in Figure 9.7: At the low end, one group removed 50 balls from the bowl with proportion between 0.20 = 20% and 0.25 = 25% At the high end, another group removed 50 balls from the bowl with proportion between 0.45 = 45% and 0.5 = 50% red. However the most frequently occurring proportions were between 0.30 = 30% and 0.35 = 35% red, right in the middle of the distribution. The shape of this distribution is somewhat bell-shaped. Let’s construct this same hand-drawn histogram in R using your data visualization skills that you honed in Chapter 3. We saved our 33 group of friends’ proportion red in a data frame tactile_prop_red which is included in the moderndive package you loaded earlier. tactile_prop_red View(tactile_prop_red) Let’s display only the first 10 out of 33 rows of tactile_prop_red’s contents in Table 9.2. TABLE 9.2: First 10 out of 33 groups’ proportion of 50 balls that are red. group replicate red_balls prop_red Ilyas, Yohan 1 21 0.42 Morgan, Terrance 2 17 0.34 Martin, Thomas 3 21 0.42 Clark, Frank 4 21 0.42 Riddhi, Karina 5 18 0.36 Andrew, Tyler 6 19 0.38 Julia 7 19 0.38 Rachel, Lauren 8 11 0.22 Daniel, Caroline 9 15 0.30 Josh, Maeve 10 17 0.34 Observe for each group we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. Observe, we also have a variable replicate enumerating each of the 33 groups; we chose this name because each row can be viewed as one instance of a replicated activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red. We visualize the distribution of these 33 proportions using a geom_histogram() with binwidth = 0.05 in Figure 9.8, which is appropriate since the variable prop_red is numerical. This computer-generated histogram matches our hand-drawn histogram from the earlier Figure 9.7. ggplot(tactile_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 9.8: Distribution of 33 proportions based on 33 samples of size 50 9.2.4 What are we doing here? What we just demonstrated in this activity is the statistical concept of repeated sampling. We would like to know the proportion of the bowl’s balls that are red, but because the bowl has a very large number of balls performing an exhaustive count of the number of red and white balls in the bowl would be very costly in terms of both time and energy. We therefore extract a sample of 50 balls using the shovel. Using this sample of 50 balls, we estimate the proportion of the bowl’s balls that are red using the proportion of the shovel’s balls that are red. This estimate in our earlier example was 17 red balls out of 50 balls = 34%. Moreover, because we mixed the balls before each use of the shovel, the samples were randomly selected. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Table 9.2. This is known as the concept of sampling variation. In Section 9.3 we’ll mimic the hands-on sampling activity we just performed in a computer simulation; using a computer will allow us to repeat the above sampling activity much more than 33 times. Using a computer, not only will we be able to repeat the hands-on activity a very large number of times, but we will also be able to repeat it using different sized shovels. The purpose of these simulations is to develop an understanding of two key concepts relating to repeated sampling: understanding the concept of sampling variation and the role that sample size plays in this variation. 9.3 Computer simulation What we performed in Section 9.2 is a simulation of sampling. In other words, we were not in a real-life sampling scenario in order to answer a real-life question, but rather we were mimicking such a scenario with our bowl and shovel. The crowd-sourced Wikipedia definition of a simulation states: “A simulation is an approximate imitation of the operation of a process or system.”1 One example of simulations in practice are a flight simulators: before pilots in training are allowed to fly an actual plane, they first practice on a computer that attempts to mimic the reality of flying an actual plane as best as possible. Now you might be thinking that simulations must necessarily take place on computer. However, this is not necessarily true. Take for example crash test dummies: before cars are made available to the market, automobile engineers test their safety by mimicking the reality for passengers of being in an automobile crash. To distinguish between these two simulation types, we’ll term a simulation performed in real-life as a “tactile” simulation done with your hands and to the touch as opposed to a “virtual” simulation performed on a computer. Example of a “tactile” simulation Example of “virtual” simulation So while in Section 9.2 we performed a “tactile” simulation of sampling using an actual bowl and an actual shovel with our hands, in this section we’ll perform a “virtual” simulation using a “virtual” bowl and a “virtual” shovel with our computers. 9.3.1 Using the virtual shovel once Let’s start by performing the virtual analogue of the tactile sampling simulation we performed in 9.2. We first need a virtual analogue of the bowl seen in Figure 9.2. To this end, we included a data frame bowl in the moderndive package whose rows correspond exactly with the contents of the actual bowl. bowl # A tibble: 2,400 x 2 ball_ID color &lt;int&gt; &lt;chr&gt; 1 1 white 2 2 white 3 3 white 4 4 red 5 5 white 6 6 white 7 7 red 8 8 white 9 9 red 10 10 white # … with 2,390 more rows Observe in the output that bowl has 2400 rows, telling us that the bowl contains 2400 equally-sized balls. The first variable ball_ID is used merely as an “identification variable” for this data frame as discussed in Subsection ??; none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white. View the contents of the bowl in RStudio’s data viewer and scroll through the contents to convince yourselves that bowl is indeed a virtual version of the actual bowl in Figure 9.2. Now that we have a virtual analogue of our bowl, we now need a virtual analogue for the shovel seen in Figure 9.3; we’ll use this virtual shovel to generate our virtual random samples of 50 balls. We’re going to use the rep_sample_n() function included in the moderndive package. This function allows us to take repeated, or replicated, samples of size n. Run the following and explore virtual_shovel’s contents in the RStudio viewer. virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) View(virtual_shovel) Let’s display only the first 10 out of 50 rows of virtual_shovel’s contents in Table 9.3. TABLE 9.3: First 10 sampled balls of 50 in virtual sample replicate ball_ID color 1 1422 white 1 828 white 1 234 red 1 1038 red 1 598 white 1 1254 white 1 1354 red 1 1430 red 1 572 white 1 338 white The ball_ID variable identifies which of the balls from bowl are included in our sample of 50 balls and color denotes its color. However what does the replicate variable indicate? In virtual_shovel’s case, replicate is equal to 1 for all 50 rows. This is telling us that these 50 rows correspond to a first repeated/replicated use of the shovel, in our case our first sample. We’ll see below when we “virtually” take 33 samples, replicate will take values between 1 and 33. Before we do this, let’s compute the proportion of balls in our virtual sample of size 50 that are red using the dplyr data wrangling verbs you learned in Chapter 4. Let’s breakdown the steps individually: First, for each of our 50 sampled balls, identify if it is red using a test for equality using ==. For every row where color == &quot;red&quot;, the Boolean TRUE is returned and for every row where color is not equal to &quot;red&quot;, the Boolean FALSE is returned. Let’s create a new Boolean variable is_red using the mutate() function from Section 4.5: virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) # A tibble: 50 x 4 # Groups: replicate [1] replicate ball_ID color is_red &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; 1 1 1422 white FALSE 2 1 828 white FALSE 3 1 234 red TRUE 4 1 1038 red TRUE 5 1 598 white FALSE 6 1 1254 white FALSE 7 1 1354 red TRUE 8 1 1430 red TRUE 9 1 572 white FALSE 10 1 338 white FALSE # … with 40 more rows Second, we compute the number of balls out of 50 that are red using the summarize() function. Recall from Section 4.3 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics that you specify, like mean() and median(). In this case we use the sum(): virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) %&gt;% summarize(num_red = sum(is_red)) # A tibble: 1 x 2 replicate num_red &lt;int&gt; &lt;int&gt; 1 1 20 Why does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUE’s and FALSE’s is equivalent to summing 1’s and 0’s, which in the end counts the number of balls where color is red. In our case, 17 of the 50 balls were red. Third and last, we compute the proportion of the 50 sampled balls that are red by dividing num_red by 50: virtual_shovel %&gt;% mutate(is_red = color == &quot;red&quot;) %&gt;% summarize(num_red = sum(is_red)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 x 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 20 0.4 In other words, this “virtual” sample’s balls were 34% red. Let’s make the above code a little more compact and succinct by combining the first mutate() and the summarize() as follows: virtual_shovel %&gt;% summarize(num_red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 x 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 20 0.4 Great! 34% of virtual_shovel’s 50 balls were red! So based on this particular sample, our guess at the proportion of the bowl’s balls that are red is 34%. But remember from our earlier tactile sampling activity that if we repeated this sampling, we would not necessarily obtain a sample of 50 balls with 34% of them being red again; there will likely be some variation. In fact in Table 9.3 we displayed 33 such proportions based on 33 tactile samples and then in Figure 9.7 we visualized the distribution of the 33 proportions in a histogram. Let’s now perform the virtual analogue of having 33 groups of students use the sampling shovel! 9.3.2 Using the virtual shovel 33 times Recall that in our tactile sampling exercise in Section 9.2 we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls, which we then used to compute 33 proportions. In other words we repeated/replicated using the shovel 33 times. We can perform this repeated/replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument, indicating we want to repeat the sampling 33 times. Be sure to scroll through the contents of virtual_samples in RStudio’s viewer. virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 33) View(virtual_samples) Observe that while the first 50 rows of replicate are equal to 1, the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\times\\) 50 = 1650 rows. Let’s now take the data frame virtual_samples with 33 \\(\\times\\) 50 = 1650 rows corresponding to 33 samples of size 50 balls and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as we did in the previous section, but this time with an additional group_by() of the replicate variable. Recall from Section 4.4 that by assigning the grouping variable “meta-data” before summarizing(), we’ll obtain 33 different proportions red: virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) View(virtual_prop_red) Let’s display only the first 10 out of 33 rows of virtual_prop_red’s contents in Table 9.2. As one would expect, there is variation in the resulting prop_red proportions red for the first 10 out 33 repeated/replicated samples. TABLE 9.4: First 10 out of 33 virtual proportion of 50 balls that are red. replicate red prop_red 1 25 0.50 2 17 0.34 3 22 0.44 4 17 0.34 5 15 0.30 6 17 0.34 7 19 0.38 8 19 0.38 9 17 0.34 10 20 0.40 Let’s visualize the distribution of these 33 proportions red based on 33 virtual samples using a histogram with binwidth = 0.05 in Figure 9.9. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 9.9: Distribution of 33 proportions based on 33 samples of size 50 Observe that occasionally we obtained proportions red that are less than 0.3 = 30%, while on the other hand we occasionally we obtained proportions that are greater than 0.45 = 45%. However, the most frequently occurring proportions red out of 50 balls were between 35% and 40% (for 11 out 33 samples). Why do we have these differences in proportions red? Because of sampling variation. Let’s now compare our virtual results with our tactile results from the previous section in Figure 9.10. We see that both histograms, in other words the distribution of the 33 proportions red, are somewhat similar in their center and spread although not identical. These slight differences are again due to random variation. Furthermore both distributions are somewhat bell-shaped. FIGURE 9.10: Comparing 33 virtual and 33 tactile proportions red. 9.3.3 Using the virtual shovel 1000 times Now say we want study the variation in proportions red not based on 33 repeated/replicated samples, but rather a very large number of samples say 1000 samples. We have two choices at this point. We could have our students manually take 1000 samples of 50 balls and compute the corresponding 1000 proportion red out 50 balls. This would be cruel and unusual however, as this would be very tedious and time-consuming. This is where computers excel: automating long and repetitive tasks while performing them very quickly. Therefore at this point we will abandon tactile sampling in favor of only virtual sampling. Let’s once again use the rep_sample_n() function with sample size set to 50 once again, but this time with the number of replicates reps = 1000. Be sure to scroll through the contents of virtual_samples in RStudio’s viewer. virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) View(virtual_samples) Observe that now virtual_samples has 1000 \\(\\times\\) 50 = 50,000 rows, instead of the 33 \\(\\times\\) 50 = 1650 rows from earlier. Using the same code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\times\\) 50 = 50,000 and compute the resulting 1000 proportions red. virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) View(virtual_prop_red) Observe that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 9.11. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 1000 proportions red&quot;) FIGURE 9.11: Distribution of 1000 proportions based on 33 samples of size 50 Once again, the most frequently occurring proportions red occur between 35% and 40%. Every now and then, we obtain proportions as low as between 20% and 25%, and others as high as between 55% and 60%. These are rare however. Furthermore observe that we now have a much more symmetric and smoother bell-shaped distribution. 9.4 Properties of Sampling Distributions Once you have simulated many possible sample values of an estimator across repeated samples, an important question is how you can summarize these. The way you summarize a sampling distribution is the same way you’d summarize any other data: How can you characterize its distribution? What is it’s average? What is its standard deviation? 9.4.1 Distribution The sampling distribution of an estimator refers to the general shape of distribution of estimates across repeated samples. Recall from Chapter X that we can, in general, characterize a distribution in terms of its modality (unimodal, bimodal), symmetry/ skew (none, right, or left), and kurtosis (heavy tails or not). Most estimators follow one of a few common distributions. These include: Normal distribution: e.g., sample mean t-distribution: e.g., sample mean standardized by standard error Chi-squared distribution: e.g., sample variance F-distribution: e.g., ratio of sample variances in two groups Of these, the most common and important is the Normal Distribution. As a result of the Central Limit Theorem (CLT), when sample sizes are large, most sampling distributions will be approximated well by a Normal Distribution. In fact, we can see in Figures 9.11 and 9.1 that the sampling distribution of \\(\\hat{\\pi}\\) follows a Normal distribution. We will discuss the CLT further in Section 9.6 9.4.2 Mean of the sampling distribution If we were to summarize a dataset beyond the distribution, the first statistic we would likely report is the mean of the distribution. This is true with sampling distributions as well. With sampling distributions, however, we do not simply want to know what the mean is – we want to know how similar or different this is from the population parameter value the sample statistic is estimating. Any difference in these two values is called bias: A sample statistic is a biased estimator of a population parameter if its average value is more or less than the population parameter it is meant to estimate. A sample statistic is an unbiased estimator of a population parameter if in the average sample it equals the population parameter value. Note that the mean of our simulated sampling distribution of proportion of red balls is 0.374, which is a good approximation to the true proportion of red balls in the bowl (\\(\\pi =\\) 0.375). This is because the sample proportion \\(\\hat{\\pi} = \\frac{\\# \\ of \\ successes}{\\# \\ of \\ trials}\\) is an unbiased estimator of the population proportion \\(\\pi\\). #mean of sampling distribution of pi_hat mean(virtual_prop_red$prop_red) [1] 0.374 #true population proportion sum(bowl$color == &quot;red&quot;)/dim(bowl)[1] [1] 0.375 The difficulty with introducing this idea of bias in an introductory course is that most statistics used at this level (e.g., proportions, means, regression coefficients) are unbiased. Examples of biased statistics are more common in more complex models. One example, however, that illustrates this bias concept is that of sample variance estimator. Remember that we estimate the sample variance using \\(s^2= \\frac{\\sum(x_i - \\bar{x})^2}{(n-1)}\\), where \\(n\\) is the number of observations. Until now, we have simply provided this to you as the estimator without reason. You might ask: why is this divided by \\(n – 1\\) instead of simply by \\(n\\)? To see why, examine the following case. The gapminder dataset in the dslabs package has life expectancy data on 185 countries in 2016. We will consider these 185 countries to be our population. The true variance of life expectancy in this population is \\(\\sigma^2 = 57.5\\). data(&quot;gapminder&quot;, package = &quot;dslabs&quot;) gapminder_2016 &lt;- filter(gapminder, year == 2016) var(gapminder_2016$life_expectancy) [1] 57.5 Let’s draw 10,000 repeated samples of n = 5 countries from this population. The data for the first 2 samples (replicates) is shown in 9.5. set.seed(76) samples &lt;- rep_sample_n(gapminder_2016, size = 5, reps = 10000) %&gt;% select(replicate, country, year, life_expectancy, continent, region) TABLE 9.5: Life expectancy data for 2 out of 10,000 samples of size n = 5 countries replicate country year life_expectancy continent region 1 Panama 2016 78.5 Americas Central America 1 Mauritania 2016 69.8 Africa Western Africa 1 Cambodia 2016 69.7 Asia South-Eastern Asia 1 Uzbekistan 2016 72.1 Asia Central Asia 1 Hong Kong, China 2016 83.9 Asia Eastern Asia 2 Belgium 2016 80.5 Europe Western Europe 2 Kazakhstan 2016 70.2 Asia Central Asia 2 Malta 2016 82.2 Europe Southern Europe 2 Singapore 2016 82.1 Asia South-Eastern Asia 2 Indonesia 2016 71.4 Asia South-Eastern Asia We can then calculate the variance for each sample (replicate) using two different formulas: \\(s_n^2= \\frac{\\sum(x_i - \\bar{x})^2}{n}\\) \\(s^2= \\frac{\\sum(x_i - \\bar{x})^2}{(n-1)}\\) variances &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(s2_n = sum((life_expectancy - mean(life_expectancy))^2) / 5, #calculate the variance for each sample with n = 5 on the denominator s2 = sum((life_expectancy - mean(life_expectancy))^2) / 4) #calculate the variance for each sample with n - 1 = 4 on the denominator variances[1:10,] %&gt;% kable(digits = 3, caption = &quot;Sample variances of life expectancy for first 10 samples&quot;, booktabs = TRUE) %&gt;% kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16), latex_options = c(&quot;HOLD_position&quot;)) TABLE 9.6: Sample variances of life expectancy for first 10 samples replicate s2_n s2 1 31.0 38.7 2 28.5 35.6 3 33.4 41.7 4 72.1 90.1 5 13.9 17.4 6 39.0 48.7 7 52.9 66.1 8 18.8 23.6 9 40.3 50.3 10 19.1 23.9 Table 9.6 shows the results for the first 10 samples. If we look at the average of \\(s_n^2\\) and \\(s^2\\) across all 10,000 samples, we find that \\(s_n^2\\) is biased; on average it is equal to 46.151. Remember that the true value of the variance in this population is \\(\\sigma^2 = 57.5\\). By dividing by \\(n – 1\\) instead of \\(n\\), however, the bias is removed; the average value of \\(s^2\\) = 57.689. Therefore we use \\(s^2= \\frac{\\sum(x_i - \\bar{x})^2}{(n-1)}\\) as our usual estimator for \\(\\sigma^2\\) because it is unbiased. mean(variances$s2_n) [1] 46.2 mean(variances$s2) [1] 57.7 ggplot(variances) + geom_histogram(aes(x = s2_n, fill = &quot;red&quot;), color = &quot;white&quot;, alpha = 0.6) + geom_histogram(aes(x = s2, fill = &quot;green&quot;), color = &quot;white&quot;, alpha = 0.6) + geom_vline(xintercept = mean(variances$s2_n), color = &quot;red&quot;) + geom_vline(xintercept = mean(variances$s2), color = &quot;green&quot;) + geom_vline(xintercept = var(gapminder_2016$life_expectancy), linetype = 2) + #ggtitle(&quot;Sample variance estimates for 10,000 samples of size n = 5&quot;) + scale_fill_manual(name=&quot;Estimator&quot;, values = c(&#39;red&#39; = &#39;red&#39;,&#39;green&#39; = &#39;green&#39;), labels = expression(s_n^2, s^2)) + xlab(&quot;Sample variance estimate&quot;) + ylab(&quot;Number of samples&quot;) FIGURE 9.12: Sample variance estimates for 10,000 samples of size n = 5 Notice that the sampling distribution of the sample variance shown in Figure 9.12 is not Normal but rather is skewed right; in fact, it follows a chi-square distribution with \\(n-1\\) degrees of freedom. 9.4.3 Standard deviation of the sampling distribution While a sample statistic might on average be equal to the population parameter, any one sample estimate might be far from the population parameter. An estimator is precise when the estimate is close to the population parameter in most samples. If we were analyzing a dataset in general, we might characterize this precision by a measure of the distribution’s spread, such as the standard deviation. We can do this with sampling distributions, too. The standard error of an estimator is the standard deviation of its sampling distribution: A large standard error means that an estimate (e.g., in the sample you have) may be far from the true population parameter. This means the estimate is imprecise. A small standard error means an estimate (e.g., in the sample you have) is likely to be close to the true population parameter. This means the estimate is precise. In statistics, we prefer estimators that are precise over those that are not. Again, this is tricky to understand at an introductory level, since nearly all sample statistics at this level can be proven to be the most precise estimators (out of all possible estimators) of the population parameters they are estimating. In more complex models, however, there are often competing estimators, and statisticians spend time studying the behavior of these estimators in comparison to one another. 9.4.4 Confusing concepts On one level, sampling distributions should seem straightforward and like simple extensions to methods you’ve learned already in this course. That is, just like sample data you have in front of you, we can summarize these sampling distributions in terms of their shape (distribution), mean (bias), and standard deviation (standard error). But this similarity to data analysis is exactly what makes this tricky. It is imperative to remember that sampling distributions are inherently theoretical constructs: Even if your estimator is unbiased, the number you see in your data (the value of the estimator) may not be the value of the parameter in the population. The standard deviation is a measure of spread in your data. The standard error is a property of an estimator across repeated samples. The distribution of a variable is something you can directly examine in your data. The sampling distribution is a property of an estimator across repeated samples. Remember, a sample statistic is a tool we use to estimate a parameter value in a population. The sampling distribution tells us how good this tool is: Does it work on average (bias)? Does it work most of the time (standard error)? Does it tend to over- or under- estimate (distribution)? 9.5 Sample Size and Sampling Distributions Let’s return to our football fan example. Let’s say you could estimate the proportions of attendees that are home fans by selecting a sample of \\(n = 10\\) or by selecting a sample of \\(n = 100\\). Which would be better? Why? A larger sample will certainly cost more – is this worth it? What about a sample of \\(n = 500\\)? Is that worth it? This question of appropriate sample size drives much of statistics. For example, you might be conducting an experiment in a psychology lab and ask: how many participants do I need to estimate this treatment effect precisely? Or you might be conducting a survey and need to know: how many respondents do I need in order to estimate the relationship between income and education well? These questions are inherently about how sample size affects sampling distributions, in general, and in particular, how sample size affects standard errors (precision). 9.5.1 Sampling balls with different sized shovels Returning to our ball example, now say instead of just one shovel, you had three choices of shovels to extract a sample of balls with. A shovel with 25 slots A shovel with 50 slots A shovel with 100 slots If your goal was still to estimate the proportion of the bowl’s balls that were red, which shovel would you choose? In our experience, most people would choose the shovel with 100 slots since it has the biggest sample size and hence would yield the “best” guess of the proportion of the bowl’s 2400 balls that are red. Using our newly developed tools for virtual sampling simulations, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size = 25, size = 50, and size = 100, while keeping the number of repeated/replicated samples at 1000: Virtually use the appropriate shovel to generate 1000 samples with size balls. Compute the resulting 1000 replicated of the proportion of the shovel’s balls that are red. Visualize the distribution of these 1000 proportion red using a histogram. Run each of the following code segments individually and then compare the three resulting histograms. # Segment 1: sample size = 25 ------------------------------ # 1.a) Virtually use shovel 1000 times virtual_samples_25 &lt;- bowl %&gt;% rep_sample_n(size = 25, reps = 1000) # 1.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 25) # 1.c) Plot distribution via a histogram ggplot(virtual_prop_red_25, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 25 balls that were red&quot;, title = &quot;25&quot;) # Segment 2: sample size = 50 ------------------------------ # 2.a) Virtually use shovel 1000 times virtual_samples_50 &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) # 2.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) # 2.c) Plot distribution via a histogram ggplot(virtual_prop_red_50, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;50&quot;) # Segment 3: sample size = 100 ------------------------------ # 3.a) Virtually using shovel with 100 slots 1000 times virtual_samples_100 &lt;- bowl %&gt;% rep_sample_n(size = 100, reps = 1000) # 3.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 100) # 3.c) Plot distribution via a histogram ggplot(virtual_prop_red_100, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 100 balls that were red&quot;, title = &quot;100&quot;) For easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure 9.13. What do you observe? FIGURE 9.13: Comparing the distributions of proportion red for different sample sizes Observe that as the sample size increases, the spread of the 1000 replicates of the proportion red decreases. In other words, as the sample size increases, there are less differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure 9.13, things appear to center tightly around roughly 40%. We can be numerically explicit about the amount of spread in our 3 sets of 1000 values of prop_red by computing the standard deviation for each of the three sampling distributions. For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function. # n = 25 virtual_prop_red_25 %&gt;% summarize(sd = sd(prop_red)) # n = 50 virtual_prop_red_50 %&gt;% summarize(sd = sd(prop_red)) # n = 100 virtual_prop_red_100 %&gt;% summarize(sd = sd(prop_red)) Let’s compare these 3 measures of spread of the distributions in Table 9.7. TABLE 9.7: Comparing standard deviations of proportions red for 3 different shovels. Number of slots in shovel Standard deviation of proportions red 25 0.098 50 0.068 100 0.047 As we observed visually in Figure 9.13, as the sample size increases our numerical measure of spread (i.e. our standard error) decreases; there is less variation in our proportions red. In other words, as the sample size increases, our guesses at the true proportion of the bowl’s balls that are red get more consistent and precise. Remember that because we are computing the standard deviation of an estimator \\(\\hat{\\pi}\\)’s sampling distribution, we call this the standard error of \\(\\hat{\\pi}\\). Overall, this simulation shows that compared to a smaller sample size (e.g., \\(n = 10\\)), with a larger sample size (e.g., \\(n = 100\\)), the sampling distribution has less spread and a smaller standard error. This means that an estimate from a larger sample is likely closer to the population parameter value than one from a smaller sample. 9.6 Central Limit Theorem (CLT) There is a very useful result in statistics called the Central Limit Theorem which tells us that the sampling distribution of the sample mean is well approximated by the normal distribution. While not all variables follow a normal distribution, many estimators have sampling distributions that are normal. We have already seen this to be true with the sample proportion. More formally, the CLT tells us that \\[\\bar{x} \\sim N(mean = \\mu, SE = \\frac{\\sigma}{\\sqrt{n}}),\\] where \\(\\mu\\) is the population mean of X, \\(\\sigma\\) is the population standard deviation of X, and \\(n\\) is the sample size. ###CLT conditions Certain conditions must be met for the CLT to apply: Independence: Sampled observations must be independent. This is difficult to verify, but is more likely if * random sampling / assignment is used, and * Sample size n &lt; 10% of the population Sample size / skew: Either the population distribution is normal, or if the population distribution is skewed, the sample size is large. * the more skewed the population distribution, the larger sample size we need for the CLT to apply * for moderately skewed distributions n &gt; 30 is a widely used rule of thumb This is also difficult to verify for the population, but we can check it using the sample data, and assume that the sample mirrors the population. ###CLT example Let’s return to the gapminder dataset, this time looking at the variable infant_mortality. We’ll first subset our data to only include the year 2015, and we’ll exclude the 7 countries that have missing data for infant_mortality. Figure 9.14 shows the distribution of infant_mortality, which is skewed right. data(&quot;gapminder&quot;, package = &quot;dslabs&quot;) gapminder_2015 &lt;- filter(gapminder, year == 2015) infant_mortality_noNA &lt;- filter(gapminder_2015, !is.na(infant_mortality)) ggplot(infant_mortality_noNA) + geom_histogram(aes(x = infant_mortality), color = &quot;black&quot;) + xlab(&quot;Infant mortality per 1,000 live births&quot;) + ylab(&quot;Number of countries&quot;) FIGURE 9.14: Infant mortality rates per 1,000 live births across 178 countries in 2015 Let’s run 3 simulations where we take 10,000 samples of size \\(n = 5\\), \\(n = 30\\) and \\(n = 100\\) and plot the sampling distribution of the mean for each. sample_5 &lt;- rep_sample_n(infant_mortality_noNA, size = 5, reps = 10000) %&gt;% group_by(replicate) %&gt;% summarise(mean_infant_mortality = mean(infant_mortality)) %&gt;% mutate(n = 5) sample_30 &lt;- rep_sample_n(infant_mortality_noNA, size = 30, reps = 10000) %&gt;% group_by(replicate) %&gt;% summarise(mean_infant_mortality = mean(infant_mortality)) %&gt;% mutate(n = 30) sample_100 &lt;- rep_sample_n(infant_mortality_noNA, size = 100, reps = 10000) %&gt;% group_by(replicate) %&gt;% summarise(mean_infant_mortality = mean(infant_mortality)) %&gt;% mutate(n = 100) all_samples &lt;- bind_rows(sample_5, sample_30, sample_100) ggplot(all_samples) + geom_histogram(aes(x = mean_infant_mortality), color = &quot;white&quot;) + facet_wrap(~n) + xlab(&quot;Mean infant mortality&quot;) + ylab(&quot;Number of samples&quot;) FIGURE 9.15: Sampling distributions of the mean infant mortality for various sample sizes Figure 9.15 shows that for samples of size \\(n = 5\\), the sampling distribution is still skewed slightly right. However, with even a moderate sample size of \\(n = 30\\), the Central Limit Theorem kicks in, and we see that the sampling distribution of the mean (\\(\\bar{x}\\)) is normal, even though the underlying data was skewed. We again see that the standard error of the estimate decreases as the sample size increases. Overall, this simulation shows that not only might the precision of an estimate differ as a result of a larger sample size, but also the sampling distribution might be different for a smaller sample size (e.g., \\(n = 5\\)) than for a larger sample size (e.g., \\(n=100\\)). Wikipedia entry for simulation↩ "],
["10-CIs.html", "Chapter 10 Confidence Intervals 10.1 Theory based upon assumptions and models 10.2 Bootstrap 10.3 The infer package for statistical inference 10.4 Combining an estimate with its precision 10.5 Confidence Interval via Bootstrap 10.6 Interpreting a Confidence Interval 10.7 Example: One proportion 10.8 Example: Comparing two proportions", " Chapter 10 Confidence Intervals In Chapter 9, we developed a theory of repeated samples. But what does this mean for your data analysis? If you only have one sample in front of you, how are you supposed to understand properties of the sampling distribution and your estimators? In this chapter we tackle these questions using two approaches: 1) based upon formulas provided to us via mathematical theory, and 2) based upon approximations available computationally (bootstrap). Needed Packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 2.3 for information on how to install and load R packages. library(tidyverse) library(janitor) library(moderndive) library(infer) 10.1 Theory based upon assumptions and models In the table below, we provide some basic properties of common sampling statistics you are likely to encounter in this course. (How do we know these properties are true? These have been proven mathematically by statisticians.) TABLE 10.1: Properties of Sample Statistics Statistic Population parameter Estimator Biased? SE of estimator Distribution (small n) Distribution (large n) Proportion \\(\\pi\\) \\(\\widehat{\\pi}\\) Unbiased \\(\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{\\sqrt{n}}\\) Normal Normal Mean \\(\\mu\\) \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) Unbiased \\(\\frac{s}{\\sqrt{n}}\\) Normal Normal Variance \\(\\sigma^2\\) \\(s^2\\) Unbiased ? Chi-squared Normal Difference in proportions \\(\\pi_1 -\\pi_2\\) \\(\\widehat{\\pi}_1 - \\widehat{\\pi}_2\\) Unbiased \\(\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1} + \\frac{\\hat{\\pi}_2(1 - \\hat{\\pi}_2)}{n_2}}\\) Normal Normal Difference in means \\(\\mu_1 - \\mu_2\\) \\(\\overline{x}_1 - \\overline{x}_2\\) Unbiased \\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) Normal Normal Regression intercept \\(\\beta_0\\) \\(b_0\\) or \\(\\widehat{\\beta}_0\\) Unbiased \\(s_y^2[\\frac{1}{n} + \\frac{\\bar{x}^2}{(n-1)s_x^2}]\\) Normal Normal Regression slope \\(\\beta_1\\) \\(b_1\\) or \\(\\widehat{\\beta}_1\\) Unbiased \\(\\sqrt{\\frac{s_y^2}{(n-1)s_x^2}}\\) Normal Normal As an example, let’s say you are trying to estimate the population average age at the football game \\(\\mu\\). Let’s use our simulated football_fans dataset again that has information on the whole population of 40,000 fans. set.seed(2018) football_fans &lt;- data.frame(home_fan = rbinom(40000, 1, 0.91), age = rnorm(40000, 30, 8)) %&gt;% mutate(age = case_when(age &lt; 0 ~ 0, age &gt;=0 ~ age)) sample_10 &lt;- sample_n(football_fans, size = 10) mean(sample_10$age) [1] 28.5 var(sample_10$age) [1] 133 To estimate the average age, you take a sample of \\(n = 10\\) participants, producing an estimate of \\(\\bar{x} =\\) 28.5 years old. Your question is simply: is this a good estimate? The estimator here is the sample mean, \\(\\bar{x}\\). We know that the sample mean provides an unbiased estimate of the population mean. In the average possible sample, this statistic will be right. We know that the sampling distribution of the sample mean is the normal distribution. This is unimodal and symmetric, meaning that our estimate is just as likely larger than this population mean as it is smaller. The standard error of a sample mean is \\(SE(\\bar{x}) = s/\\sqrt{n}\\). In this sample, since \\(s^2 =\\) 133.2, we can calculate this standard error to be \\(SE(\\bar{x}) =\\) 133.2\\(/\\sqrt{10} =\\) 3.65. Note that a standard error is always in the same units as the original variable, so in this case the standard error is 3.65 years. Comparing this to the sample mean (\\(\\bar{x} =\\) 28.5) this gives us a sense that we are somewhat close to the true parameter value. Note that we can extend the above properties to differences between groups as well. In general, \\(Bias(X_1 - X_2) = Bias(X_1) - Bias(X_2) \\dashrightarrow\\) If \\(X_1, X_2\\) are unbiased, then \\(X_1 - X_2\\) is also unbiased. So long as \\(X_1\\) and \\(X_2\\) are independent, $ SE(X_1 – X_2) = $ \\(Dist(X_1 - X_2) \\dashrightarrow\\) if \\(X_1, X_2\\) are independent, then \\(X_1 – X_2\\) are independent. This may seem really abstract. As an example, let’s focus on estimating the difference means between two groups in the population \\((\\mu_1 - \\mu_2)\\). In our football_fans example, we could consider whether there is a difference in average age between home fans and away fans, so \\(\\mu_1\\) refers here to the average age of all home fans in the population, and \\(\\mu_2\\) refers to the average age of all away fans in the population. We can estimate this difference in average age using \\(\\bar{x}_1 - \\bar{x}_2\\) in our sample, where \\(\\bar{x}_1\\) is the average age of home fans in our sample and \\(\\bar{x}_2\\) is the average age of away fans in our sample. Using information in Table 10.1 above, because \\(\\bar{x}_1\\) is and unbiased estimator for \\(\\mu_1\\) and \\(\\bar{x}_2\\) is an unbiased estimator for \\(mu_2\\), we have: \\[Bias(\\bar{x}_1 - \\bar{x}_2) = Bias(\\bar{x}_1) - Bias(\\bar{x}_2) = 0\\] That is, on average, \\(\\bar{x}_1 - \\bar{x}_2\\) will give us an unbiased estimate of \\(\\mu_1 - \\mu_2\\). Also, because the age of home fans is independent of the age of away fans, we have: \\[SE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{[SE(\\bar{x}_1)]^2 + [SE(\\bar{x}_2)]^2} = \\sqrt{(s_1^2/n_1 + s_2^2/n_2)}\\] \\[\\bar{x}_1 - \\bar{x}_2 \\sim normally \\ \\ distributed\\] Let’s demonstrate this with our football_fans data by drawing a random sample of \\(n=100\\) fans and computing \\(\\bar{x}_1 - \\bar{x}_2\\). set.seed(2018) sample1_football_fans &lt;- rep_sample_n(football_fans, size = 100, reps = 1) mean_age_by_fan_type &lt;- sample1_football_fans %&gt;% group_by(home_fan) %&gt;% summarise(mean_age = mean(age), sd_age = sd(age), n = n(), SE_xbar = sd_age / sqrt(n)) mean_age_by_fan_type # A tibble: 2 x 5 home_fan mean_age sd_age n SE_xbar &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 0 29.3 8.59 15 2.22 2 1 31.0 7.82 85 0.848 diff_means &lt;- as.numeric(mean_age_by_fan_type[1,2]) - as.numeric(mean_age_by_fan_type[2,2]) SE_diff_means &lt;- as.numeric(sqrt(mean_age_by_fan_type[1, &quot;SE_xbar&quot;]^2 + mean_age_by_fan_type[2, &quot;SE_xbar&quot;]^2)) SE_diff_means [1] 2.38 We see here that \\(\\bar{x}_1 - \\bar{x}_2 =\\) -1.642 and \\(SE(\\bar{x}_1 - \\bar{x}_2) =\\) 2.375. In our particular sample, home fans are older, but because the magnitude of our standard error is roughly 4 times the size of our point estimate, this indicates we have a relatively imprecise estimate. We could also look at the sampling distribution of \\(\\bar{x}_1 - \\bar{x}_2 =\\) by taking 10,000 repeated samples and computing the difference in means in each. set.seed(2018) samples_football_fans &lt;- rep_sample_n(football_fans, size = 100, reps = 10000) mean_age_by_fan_type &lt;- samples_football_fans %&gt;% group_by(home_fan, replicate) %&gt;% summarise(mean_age = mean(age)) %&gt;% spread(key = home_fan, value = mean_age) %&gt;% rename(&quot;mean_age_away_fan&quot; = `0`, &quot;mean_age_home_fan&quot; = `1`) %&gt;% mutate(diff_means = mean_age_home_fan - mean_age_away_fan) ggplot(mean_age_by_fan_type, aes(x = diff_means)) + geom_histogram(color = &quot;white&quot;) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. FIGURE 10.1: Samping distribution of difference in means pop_mean_age_by_fan_type &lt;- football_fans %&gt;% group_by(home_fan) %&gt;% summarise(mean_age = mean(age), sd_age = sd(age)) pop_mean_age_by_fan_type # A tibble: 2 x 3 home_fan mean_age sd_age &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 29.8 7.97 2 1 30.1 8.02 We see that the sampling distribution of \\(\\bar{x}_1 - \\bar{x}_2 =\\) follows a normal distribution centered around the true population mean \\(\\mu_1 - \\mu_2 =\\) -0.277, as expected based on the properties presented in Table ??. What does this mean for your data analysis? You need to think about sample size and how it might affect the sampling distribution. If you have a large enough sample (e.g., \\(n &gt; 50\\)), you can probably assume the sampling distribution is normally distributed. If you have a smaller sample (e.g., \\(n &lt; 50\\)), you should look up what the appropriate sampling distribution would be (e.g., t-distribution, F-distribution). Importantly, what this does not mean is that you need to get a larger sample! 10.2 Bootstrap The properties developed above are based on mathematical proofs. These proofs require assumptions, and it is sometimes hard to know if the assumptions have been met. Also, Table ?? only lists a small number of statistics. What if the parameter you are estimating requires a more complex estimator and you aren’t sure of its sampling distribution? This problem provides the motivation for the bootstrap. The idea of the bootstrap is to approximate the sampling distribution of an estimator based upon the sample of data you have in front of you. The procedure is as follows: You have a sample size of \\(n\\) (e.g., \\(n = 100\\)) Randomly select 1 observation and record its value. Return the observation to the sample. Now repeat this \\(n\\) times. This is called sampling with replacement – note that in this procedure, the same observation may be selected multiple times, and other observations may not be selected at all. For that simulated sample of size \\(n\\), calculate the sample statistic. Repeat this process many, many, many (e.g., 10,000) times. Clearly, if you were to do this by hand this would be impossible. But this procedure can be conducted using a computer very quickly. At the end of this bootstrap procedure, you are able to see: An estimate of the sampling distribution A bootstrap estimate of the standard error In cases in which there are also mathematical formulas available, bootstrap results should be very similar to those based off the formulas. When they are very different, however, this suggests that some of the assumptions used to develop the mathematical results might have been violated in the data. 10.2.1 Bootstrapping Example Suppose we are interested in understanding some properties of the mean age of all US pennies from this data collected in 2011. How might we go about that? The moderndive package contains a sample of 40 pennies collected and minted in the United States. Let’s explore this sample data first: pennies_sample # A tibble: 40 x 2 year age_in_2011 &lt;int&gt; &lt;int&gt; 1 1997 14 2 2006 5 3 1997 14 4 2000 11 5 2000 11 6 2007 4 7 1986 25 8 1980 31 9 1992 19 10 1993 18 # … with 30 more rows The pennies_sample data frame has rows corresponding to a single penny with two variables: year of minting as shown on the penny and age_in_2011 giving the years the penny had been in circulation from 2011 as an integer, e.g. 15, 2, etc. Let’s begin by understanding some of the properties of pennies_sample using data wrangling tools from Chapter 4 and data visualization from Chapter 3. First, let’s visualize the values in this sample as a histogram: ggplot(pennies_sample, aes(x = age_in_2011)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) We see a slightly skewed distribution here that has quite a few values near 10 years in age with only a few larger than 35 years or smaller than 5 years. If pennies_sample is a representative sample from the population, we’d expect the age of all US pennies collected in 2011 to have a similar shape, a similar spread, and similar measures of central tendency like the mean. So where does the mean value fall for this sample? This point will be known as our point estimate and provides us with a single number that could serve as the guess to what the true population mean age might be. Recall how to find this using the dplyr package: x_bar &lt;- pennies_sample %&gt;% summarize(stat = mean(age_in_2011)) x_bar # A tibble: 1 x 1 stat &lt;dbl&gt; 1 16.5 Our point estimate is, thus, \\(\\bar{x} = 16.5\\). Note that this is just one sample, though, providing just one guess at the population mean. What if we’d like to have another guess? This should all sound similar to what we did in Chapter 9. There instead of collecting just a single scoop of balls we had many different students use the shovel to scoop different samples of red and white balls. We then calculated a sample statistic (the sample proportion) from each sample. But, we don’t have a population to pull from here with the pennies. We only have this one sample. The process of bootstrapping allows us to use a single sample to generate many different samples that will act as our way of approximating a sampling distribution using a created bootstrap distribution instead. 10.2.2 The Bootstrapping Process Remember that bootstrapping uses a process of random sampling with replacement from our original sample to create new bootstrap samples of the same size as our original sample. We can again make use of the rep_sample_n() function to explore what one such bootstrap sample would look like. bootstrap_sample1 &lt;- pennies_sample %&gt;% rep_sample_n(size = 40, replace = TRUE, reps = 1) bootstrap_sample1 # A tibble: 40 x 3 # Groups: replicate [1] replicate year age_in_2011 &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 1996 15 2 1 2000 11 3 1 1985 26 4 1 1977 34 5 1 1986 25 6 1 1977 34 7 1 1985 26 8 1 2007 4 9 1 1971 40 10 1 1985 26 # … with 30 more rows Let’s visualize what this new bootstrap sample looks like: ggplot(bootstrap_sample1, aes(x = age_in_2011)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) We now have another sample from what we could assume comes from the population of interest. We can similarly calculate the sample mean of this bootstrap sample, called a bootstrap statistic. bootstrap_sample1 %&gt;% summarize(stat = mean(age_in_2011)) # A tibble: 1 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 17.8 We can see that this sample mean is larger than the x_bar value we calculated earlier for the pennies_sample data. We’ll come back to analyzing the different bootstrap statistic values shortly. Let’s recap what was done to get to this bootstrap sample using a tactile explanation: First, pretend that each of the 40 values of age_in_2011 in pennies_sample were written on a small piece of paper. Recall that these values were 6, 30, 34, 19, 6, etc. Now, put the 40 small pieces of paper into a receptacle such as a baseball cap. Shake up the pieces of paper. Draw “at random” from the cap to select one piece of paper. Write down the value on this piece of paper. Say that it is 28. Now, place this piece of paper containing 28 back into the cap. Draw “at random” again from the cap to select a piece of paper. Note that this is the sampling with replacement part since you may draw 28 again. Repeat this process until you have drawn 40 pieces of paper and written down the values on these 40 pieces of paper. Completing this repetition produces ONE bootstrap sample. If you look at the values in bootstrap_sample1, you can see how this process plays out. We originally drew 28, then we drew 11, then 7, and so on. Of course, we didn’t actually use pieces of paper and a cap here. We just had the computer perform this process for us to produce bootstrap_sample1 using rep_sample_n() with replace = TRUE set. The process of sampling with replacement is how we can use the original sample to take a guess as to what other values in the population may be. Sometimes in these bootstrap samples, we will select lots of larger values from the original sample, sometimes we will select lots of smaller values, and most frequently we will select values that are near the center of the sample. Let’s explore what the distribution of values of age_in_2011 for six different bootstrap samples looks like to further understand this variability. six_bootstrap_samples &lt;- pennies_sample %&gt;% rep_sample_n(size = 40, replace = TRUE, reps = 6) ggplot(six_bootstrap_samples, aes(x = age_in_2011)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + facet_wrap(~ replicate) We can also look at the six different means using dplyr syntax: six_bootstrap_samples %&gt;% group_by(replicate) %&gt;% summarize(stat = mean(age_in_2011)) # A tibble: 6 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 16.6 2 2 14.4 3 3 17.9 4 4 14.9 5 5 14.2 6 6 18.8 Instead of doing this six times, we could do it 1000 times and then look at the distribution of stat across all 1000 of the replicates. This sets the stage for the infer R package (Bray et al. 2019) that was created to help users perform statistical inference such as confidence intervals and hypothesis tests using verbs similar to what you’ve seen with dplyr. We’ll walk through setting up each of the infer verbs for creating bootstrap sampling distributions using this pennies_sample example, while also explaining the purpose of the verbs in a general framework. 10.3 The infer package for statistical inference The infer package makes great use of the %&gt;% to create a pipeline for statistical inference. The goal of the package is to provide a way for its users to explain the computational process of confidence intervals and hypothesis tests using the code as a guide. The verbs build in order here, so you’ll want to start with specify() and then continue through the others as needed. 10.3.1 Specify variables The specify() function is used primarily to choose which variables will be the focus of the statistical inference. In addition, a setting of which variable will act as the explanatory and which acts as the response variable is done here. For proportion problems similar to those in Chapter 9, we can also give which of the different levels we would like to have as a success. We’ll see further examples of these options in this chapter, Chapter ??, and in Appendix B. To begin to create a bootstrap sampling distribution for the population mean age of US pennies in 2011, we start by using specify() to choose which variable in our pennies_sample data we’d like to work with. This can be done in one of two ways: Using the response argument: pennies_sample %&gt;% specify(response = age_in_2011) Response: age_in_2011 (integer) # A tibble: 40 x 1 age_in_2011 &lt;int&gt; 1 14 2 5 3 14 4 11 5 11 6 4 7 25 8 31 9 19 10 18 # … with 30 more rows Using formula notation: pennies_sample %&gt;% specify(formula = age_in_2011 ~ NULL) Response: age_in_2011 (integer) # A tibble: 40 x 1 age_in_2011 &lt;int&gt; 1 14 2 5 3 14 4 11 5 11 6 4 7 25 8 31 9 19 10 18 # … with 30 more rows Note that the formula notation uses the common R methodology to include the response \\(y\\) variable on the left of the ~ and the explanatory \\(x\\) variable on the right of the “tilde.” Recall that you used this notation frequently with the lm() function in Chapters 6 and 7 when fitting regression models. Either notation works just fine, but a preference is usually given here for the formula notation to further build on the ideas from earlier chapters. 10.3.2 Generate replicates After specify()ing the variables we’d like in our inferential analysis, we next feed that into the generate() verb. The generate() verb’s main argument is reps, which is used to give how many different repetitions one would like to perform. Another argument here is type, which is automatically determined by the kinds of variables passed into specify(). We can also be explicit and set this type to be type = &quot;bootstrap&quot;. This type argument will be further used in hypothesis testing in Chapter ?? as well. Make sure to check out ?generate to see the options here and use the ? operator to better understand other verbs as well. Let’s generate() 1000 bootstrap samples: thousand_bootstrap_samples &lt;- pennies_sample %&gt;% specify(response = age_in_2011) %&gt;% generate(reps = 1000) Setting `type = &quot;bootstrap&quot;` in `generate()`. We can use the dplyr count() function to help us understand what the thousand_bootstrap_samples data frame looks like: thousand_bootstrap_samples %&gt;% count(replicate) # A tibble: 1,000 x 2 # Groups: replicate [1,000] replicate n &lt;int&gt; &lt;int&gt; 1 1 40 2 2 40 3 3 40 4 4 40 5 5 40 6 6 40 7 7 40 8 8 40 9 9 40 10 10 40 # … with 990 more rows Notice that each replicate has 40 entries here. Now that we have 1000 different bootstrap samples, our next step is to calculate the bootstrap statistics for each sample. 10.3.3 Calculate summary statistics After generate()ing many different samples, we next want to condense those samples down into a single statistic for each replicated sample. As seen in the diagram, the calculate() function is helpful here. As we did at the beginning of this chapter, we now want to calculate the mean age_in_2011 for each bootstrap sample. To do so, we use the stat argument and set it to &quot;mean&quot; below. The stat argument has a variety of different options here and we will see further examples of this throughout the remaining chapters. bootstrap_distribution &lt;- pennies_sample %&gt;% specify(response = age_in_2011) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = &quot;mean&quot;) Setting `type = &quot;bootstrap&quot;` in `generate()`. bootstrap_distribution # A tibble: 1,000 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 15.6 2 2 17.4 3 3 18.1 4 4 17.8 5 5 19.6 6 6 15 7 7 18.2 8 8 18.6 9 9 20.1 10 10 17.4 # … with 990 more rows We see that the resulting data has 1000 rows and 2 columns corresponding to the 1000 replicates and the mean for each bootstrap sample. Observed statistic / point estimate calculations Just as group_by() %&gt;% summarize() produces a useful workflow in dplyr, we can also use specify() %&gt;% calculate() to compute summary measures on our original sample data. It’s often helpful both in confidence interval calculations, but also in hypothesis testing to identify what the corresponding statistic is in the original data. For our example on penny age, we computed above a value of x_bar using the summarize() verb in dplyr: pennies_sample %&gt;% summarize(stat = mean(age_in_2011)) # A tibble: 1 x 1 stat &lt;dbl&gt; 1 16.5 This can also be done by skipping the generate() step in the pipeline feeding specify() directly into calculate(): pennies_sample %&gt;% specify(response = age_in_2011) %&gt;% calculate(stat = &quot;mean&quot;) # A tibble: 1 x 1 stat &lt;dbl&gt; 1 16.5 This shortcut will be particularly useful when the calculation of the observed statistic is tricky to do using dplyr alone. This is particularly the case when working with more than one variable as will be seen in Chapter ??. 10.3.4 Visualize the results The visualize() verb provides a simple way to view the bootstrap distribution as a histogram of the stat variable values. It has many other arguments that one can use as well including the shading of the histogram values corresponding to confidence interval values, which we will demonstrate later. bootstrap_distribution %&gt;% visualize() In this case, we can see that the bootstrap distribution provides us a guess as to what the variability in different sample means may look like only using the original sample as our guide. The shape of this resulting distribution appears to be normal, which is what we would expect given what we learned about the Central Limit Theorem in Section 9.6. The following diagram recaps the infer pipeline for creating a bootstrap distribution. 10.4 Combining an estimate with its precision A confidence interval gives a range of plausible values for a parameter. It depends on a specified confidence level with higher confidence levels corresponding to wider confidence intervals and lower confidence levels corresponding to narrower confidence intervals. Common confidence levels include 90%, 95%, and 99%. Usually we don’t just begin sections with a definition, but confidence intervals are simple to define and play an important role in the sciences and any field that uses data. You can think of a confidence interval as playing the role of a net when fishing. Using a single point-estimate to estimate an unknown parameter is like trying to catch a fish in a murky lake with a single spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. If we toss a net in that area, we have a good chance of catching the fish. Analogously, if we report a point estimate, we probably won’t hit the exact population parameter, but if we report a range of plausible values based around our statistic, we have a good shot at catching the parameter. 10.4.1 Confidence Interval with the Normal distribution If the sampling distribution of an estimator is normally distributed, then we can use properties of the standard normal distribution to create a confidence interval. Recall that in the standard normal distribution: 95% of the values are between -1.96 and +1.96. 90% of values are between -1.68 and +1.68. Using this, we can define a 95% confidence interval for a population parameter as, \\[(Estimate \\ \\ – \\ 1.96*SE(Estimate),\\ \\ \\ Estimate + 1.96*SE(Estimate))\\] For example, a 95% confidence interval for the population mean \\(\\mu\\) can be constructed based upon the sample mean as, \\[(\\bar{x} - 1.96 * SE(\\bar{x}), \\ \\bar{x} + 1.96*SE(\\bar{x}))\\] In our penny example, our original sample pennies_sample had \\(\\bar{x} =\\) 16.5 and \\(SE(\\bar{x}) = \\frac{s}{\\sqrt{n}} =\\) 1.606, which gives a confidence interval of (13.35, 19.65). A few properties are worth keeping in mind: This interval is symmetric. This symmetry follows from the fact that the normal distribution is a symmetric distribution. If the sampling distribution is not normal, the confidence interval may not be symmetric. The multiplier 1.96 used in this interval corresponding to 95% comes directly from properties of the normal distribution. If the sampling distribution is not normal, this multiplier might be different. For example, this multiplier is larger when the distribution has heavy tails, as with the t-distribution. The multiplier will also be different if you want to use a level of confidence other than 95%. 10.5 Confidence Interval via Bootstrap When you are not sure what the appropriate sampling distribution is, or when you are worried that your assumptions might be wrong, confidence intervals can be created using a bootstrapping process. The bootstrapping process will provide bootstrap statistics that have a bootstrap distribution with the center at (or extremely close to) the mean of the original sample. This can be seen by giving the observed statistic obs_stat argument the value of the point estimate x_bar. bootstrap_distribution %&gt;% visualize(obs_stat = x_bar) We can also compute the mean of the bootstrap distribution of means to see how it compares to x_bar: bootstrap_distribution %&gt;% summarize(mean_of_means = mean(stat)) # A tibble: 1 x 1 mean_of_means &lt;dbl&gt; 1 16.6 We can calculate a confidence interval for the unknown mean age of coins in 2011 via the bootsrap by using the middle 95% of the bootstrap_distribution to determine our endpoints. Our endpoints are thus at the 2.5th and 97.5th percentiles of the 1,000 bootsrapped sample means. This can be done with infer using the get_ci() function. (You can also use the conf_int() or get_confidence_interval() functions here as they are aliases that work the exact same way.) bootstrap_distribution %&gt;% get_ci(level = 0.95, type = &quot;percentile&quot;) # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 13.4 20 These options are the default values for level and type so we can also just do: percentile_ci &lt;- bootstrap_distribution %&gt;% get_ci() percentile_ci # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 13.4 20 Using this method, our range of plausible values for the mean age of US pennies in circulation in 2011 is 13.425 years to 20 years. We can use the visualize() function to view this using the endpoints and direction arguments, setting direction to &quot;between&quot; (between the values) and endpoints to be those stored with name percentile_ci. bootstrap_distribution %&gt;% visualize(endpoints = percentile_ci, direction = &quot;between&quot;) You can see that 95% of the data stored in the stat variable in bootstrap_distribution falls between the two endpoints with 2.5% to the left outside of the shading and 2.5% to the right outside of the shading. The cut-off points that provide our range are shown with the darker lines. Note that the function get_ci() in the infer package can be used to compute the confidence interval using the \\(Estimate \\ \\pm \\ multiplier*SE(Estimate)\\) formula, but the function is currently dependent on statistics being stored in the stat column created in the calculate() function, so you can only use get_ci() if you have used infer to create a bootstrap sampling distribution. bootstrap_distribution %&gt;% get_ci(type = &quot;se&quot;, point_estimate = x_bar) # A tibble: 1 x 2 lower upper &lt;dbl&gt; &lt;dbl&gt; 1 13.2 19.8 10.6 Interpreting a Confidence Interval Like many statistics, while a confidence interval is fairly straightforward to construct, it is very easy to interpret incorrectly. In fact, many researchers – statisticians included – get the interpretation of confidence intervals wrong. This goes back to the idea of counterfactual thinking that we introduced previously: a confidence interval is a property of a population and estimator, not a particular sample. It asks: if I constructed this interval in every possible sample, in what percentage of samples would I correctly include the true population parameter? To see this, let’s return to the sampling distribution of the sample mean for our age of football fans example. Recall that we have simulated population data for all 40,000 fans and we took 10,000 repeated samples of size 100. Figure 10.2 shows the sampling distribution of the sample mean. Recall that the true population mean is 30.064 set.seed(2018) samp_means_football_fans &lt;- samples_football_fans %&gt;% group_by(replicate) %&gt;% summarise(mean = mean(age), sd = sd(age), n = n(), se = sd / sqrt(n)) mu &lt;- mean(football_fans$age) samp_dist_plot &lt;- ggplot(samp_means_football_fans) + geom_histogram(aes(x = mean), color = &quot;white&quot;) + geom_vline(xintercept = mu, color = &quot;blue&quot;) samp_dist_plot FIGURE 10.2: Sampling Distribution of Average Age of Fans at a Football Game Assume that the sample we actually observed was replicate = 437, which had \\(\\bar{x} =\\) 29.838. If we used this sample mean to construct a 95% confidence interval, the population mean would be in this interval, right? Figure 10.3 shows a confidence interval shaded around \\(\\bar{x} =\\) 29.838, which is indicated by the red line. This confidence interval successfully includes the true population mean. xbar &lt;- filter(samp_means_football_fans, replicate == 437)$mean xbar [1] 29.8 SE &lt;- filter(samp_means_football_fans, replicate == 437)$se endpoints &lt;- c(xbar - 1.96*SE, xbar + 1.96*SE) samp_dist_plot + shade_ci(endpoints) + geom_vline(xintercept = xbar, color = &quot;red&quot;) FIGURE 10.3: Confidence Interval shaded for an observed sample mean of 29.8 Assume now that we were unlucky and drew a sample with a mean far from the population mean. One such case is replicate = 545, which had \\(\\bar{x} =\\) 32.275. In this case, is the population mean in this interval? Figure ?? displays this scenario. xbar &lt;- filter(samp_means_football_fans, replicate == 545)$mean SE &lt;- filter(samp_means_football_fans, replicate == 545)$se endpoints &lt;- c(xbar - 1.96*SE, xbar + 1.96*SE) samp_dist_plot + shade_ci(endpoints) + geom_vline(xintercept = xbar, color = &quot;red&quot;) FIGURE 10.4: Confidence Interval shaded for an observed sample mean of 32.3 In this case, the confidence interval does not include the true population mean. Importantly, remember that in real life we only have the data in front of us from one sample. We don’t know what the population mean is, and we don’t know if our estimate is the value near to the mean (Figure ?? ) or far from the mean (Figure ?? ). Also recall replicate = 545 was a legitimate random sample drawn from the population of 40,000 football fans. Just by chance, it is possible to observe a sample mean that is far from the true population mean. We could compute 95% confidence intervals for all 10,000 of our repeated samples, and we would expect approximately 95% of them to contain the true mean. mu &lt;- mean(football_fans$age) CIs_football_fans &lt;- samp_means_football_fans %&gt;% mutate(lb_95 = mean - 1.96*se, ub_95 = mean + 1.96*se, captured_95 = lb_95 &lt;= mu &amp; mu &lt;= ub_95) sum(CIs_football_fans$captured_95) / 10000 [1] 0.947 In fact, 94.73% of the 10,000 do capture the true mean. For visualization purposes, we’ll take a smaller subset of 100 of these confidence intervals and display the results in Figure ??. In this smaller subset, 96 of the 100 95% confidence intervals contain the true population mean. set.seed(2018) CI_subset &lt;- sample_n(CIs_football_fans, 100) %&gt;% mutate(replicate_id = seq(1:100)) ggplot(CI_subset) + geom_point(aes(x = mean, y = replicate_id, color = captured_95)) + geom_segment(aes(y = replicate_id, yend = replicate_id, x = lb_95, xend = ub_95, color = captured_95)) + labs( x = expression(&quot;Age in 2011 (Years)&quot;), y = &quot;Replicate ID&quot;, title = expression(paste(&quot;95% percentile-based confidence intervals for &quot;, mu, sep = &quot;&quot;)) ) + scale_color_manual(values = c(&quot;blue&quot;, &quot;orange&quot;)) + geom_vline(xintercept = mu, color = &quot;red&quot;) FIGURE 10.5: Confidence Interval for Average Age from 100 repeated samples of size 100 10.7 Example: One proportion Let’s revisit our exercise of trying to estimate the proportion of red balls in the bowl from Chapter 9. We are now interested in determining a confidence interval for population parameter \\(\\pi\\), the proportion of balls that are red out of the total \\(N = 2400\\) red and white balls. We will use the first sample reported from Ilyas and Yohan in Subsection 9.2.3 for our point estimate. They observed 21 red balls out of the 50 in their shovel. This data is stored in the tactile_shovel1 data frame in the moderndive package. tactile_shovel1 # A tibble: 50 x 1 color &lt;chr&gt; 1 white 2 red 3 red 4 red 5 red 6 red 7 red 8 white 9 red 10 white # … with 40 more rows 10.7.1 Observed Statistic To compute the proportion that are red in this data we can use the specify() %&gt;% calculate() workflow. Note the use of the success argument here to clarify which of the two colors &quot;red&quot; or &quot;white&quot; we are interested in. pi_hat &lt;- tactile_shovel1 %&gt;% specify(formula = color ~ NULL, success = &quot;red&quot;) %&gt;% calculate(stat = &quot;prop&quot;) pi_hat # A tibble: 1 x 1 stat &lt;dbl&gt; 1 0.42 10.7.2 Bootstrap distribution Next we want to calculate many different bootstrap samples and their corresponding bootstrap statistic (the proportion of red balls). We’ve done 1000 in the past, but let’s go up to 10,000 now to better see the resulting distribution. Recall that this is done by including a generate() function call in the middle of our pipeline: tactile_shovel1 %&gt;% specify(formula = color ~ NULL, success = &quot;red&quot;) %&gt;% generate(reps = 10000) Setting `type = &quot;bootstrap&quot;` in `generate()`. This results in 50 rows for each of the 10,000 replicates. Lastly, we finish the infer pipeline by adding back in the calculate() step. bootstrap_props &lt;- tactile_shovel1 %&gt;% specify(formula = color ~ NULL, success = &quot;red&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;prop&quot;) Let’s visualize() what the resulting bootstrap distribution looks like as a histogram. We’ve adjusted the number of bins here as well to better see the resulting shape. bootstrap_props %&gt;% visualize(bins = 25) We see that the resulting distribution is symmetric and bell-shaped so it doesn’t much matter which confidence interval method we choose. Let’s use the standard error method to create a 95% confidence interval. standard_error_ci &lt;- bootstrap_props %&gt;% get_ci(type = &quot;se&quot;, level = 0.95, point_estimate = pi_hat) standard_error_ci # A tibble: 1 x 2 lower upper &lt;dbl&gt; &lt;dbl&gt; 1 0.284 0.556 bootstrap_props %&gt;% visualize(bins = 25) + shade_ci(endpoints = standard_error_ci, color = &quot;mediumaquamarine&quot;, fill = &quot;turquoise&quot;) We are 95% confident that the true proportion of red balls in the bowl is between 0.284 and 0.556. This level of confidence is based on the standard error-based method including the true proportion 95% of the time if many different samples (not just the one we used) were collected and confidence intervals were created. 10.8 Example: Comparing two proportions If you see someone else yawn, are you more likely to yawn? In an episode of the show Mythbusters, they tested the myth that yawning is contagious. The snippet from the show is available to view in the United States on the Discovery Network website here. More information about the episode is also available on IMDb here. Fifty adults who thought they were being considered for an appearance on the show were interviewed by a show recruiter (“confederate”) who either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the Mythbusters watched via hidden camera to see if the unaware participants yawned. The data frame containing the results is available at mythbusters_yawn in the moderndive package. Let’s check it out. mythbusters_yawn # A tibble: 50 x 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no 7 7 seed yes 8 8 control no 9 9 control no 10 10 seed no # … with 40 more rows The participant ID is stored in the subj variable with values of 1 to 50. The group variable is either &quot;seed&quot; for when a confederate was trying to influence the participant or &quot;control&quot; if a confederate did not interact with the participant. The yawn variable is either &quot;yes&quot; if the participant yawned or &quot;no&quot; if the participant did not yawn. We can use the janitor package to get a glimpse into this data in a table format: mythbusters_yawn %&gt;% tabyl(group, yawn) %&gt;% adorn_percentages() %&gt;% adorn_pct_formatting() %&gt;% # To show original counts adorn_ns() group no yes control 75.0% (12) 25.0% (4) seed 70.6% (24) 29.4% (10) We are interested in comparing the proportion of those that yawned after seeing a seed versus those that yawned with no seed interaction. We’d like to see if the difference between these two proportions is significantly larger than 0. If so, we’d have evidence to support the claim that yawning is contagious based on this study. In looking over this problem, we can make note of some important details to include in our infer pipeline: We are calling a success having a yawn value of &quot;yes&quot;. Our response variable will always correspond to the variable used in the success so the response variable is yawn. The explanatory variable is the other variable of interest here: group. To summarize, we are looking to see the examine the relationship between yawning and whether or not the participant saw a seed yawn or not. 10.8.1 Compute the point estimate mythbusters_yawn %&gt;% specify(formula = yawn ~ group) Error: A level of the response variable `yawn` needs to be specified for the `success` argument in `specify()`. Note that the success argument must be specified in situations such as this where the response variable has only two levels. mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) Response: yawn (factor) Explanatory: group (factor) # A tibble: 50 x 2 yawn group &lt;fct&gt; &lt;fct&gt; 1 yes seed 2 yes control 3 no seed 4 yes seed 5 no seed 6 no control 7 yes seed 8 no control 9 no control 10 no seed # … with 40 more rows We next want to calculate the statistic of interest for our sample. This corresponds to the difference in the proportion of successes. mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;) Error: Statistic is based on a difference; specify the `order` in which to subtract the levels of the explanatory variable. `order = c(&quot;first&quot;, &quot;second&quot;)` means `(&quot;first&quot; - &quot;second&quot;)`. Check `?calculate` for details. We see another error here. To further check to make sure that R knows exactly what we are after, we need to provide the order in which R should subtract these proportions of successes. As the error message states, we’ll want to put &quot;seed&quot; first after c() and then &quot;control&quot;: order = c(&quot;seed&quot;, &quot;control&quot;). Our point estimate is thus calculated: obs_diff &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) obs_diff # A tibble: 1 x 1 stat &lt;dbl&gt; 1 0.0441 This value represents the proportion of those that yawned after seeing a seed yawn (0.2941) minus the proportion of those that yawned with not seeing a seed (0.25). 10.8.2 Bootstrap distribution Our next step in building a confidence interval is to create a bootstrap distribution of statistics (differences in proportions of successes). We saw how it works with both a single variable in computing bootstrap means in Subsection 10.2.2 and in computing bootstrap proportions in Section 10.7, but we haven’t yet worked with bootstrapping involving multiple variables though. In the infer package, bootstrapping with multiple variables means that each row is potentially resampled. Let’s investigate this by looking at the first few rows of mythbusters_yawn: head(mythbusters_yawn) # A tibble: 6 x 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no When we bootstrap this data, we are potentially pulling the subject’s readings multiple times. Thus, we could see the entries of &quot;seed&quot; for group and &quot;no&quot; for yawn together in a new row in a bootstrap sample. This is further seen by exploring the sample_n() function in dplyr on this smaller 6 row data frame comprised of head(mythbusters_yawn). The sample_n() function can perform this bootstrapping procedure and is similar to the rep_sample_n() function in infer, except that it is not repeated but rather only performs one sample with or without replacement. set.seed(2018) head(mythbusters_yawn) %&gt;% sample_n(size = 6, replace = TRUE) # A tibble: 6 x 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 3 seed no 2 4 seed yes 3 5 seed no 4 2 control yes 5 5 seed no 6 1 seed yes We can see that in this bootstrap sample generated from the first six rows of mythbusters_yawn, we have some rows repeated. The same is true when we perform the generate() step in infer as done below. bootstrap_distribution &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) Setting `type = &quot;bootstrap&quot;` in `generate()`. bootstrap_distribution %&gt;% visualize(bins = 20) This distribution is roughly symmetric and bell-shaped. Let’s use the percentile-based method to compute a 95% confidence interval for the true difference in the proportion of those that yawn with and without a seed presented. The arguments are explicitly listed here but remember they are the defaults and simply get_ci() can be used. bootstrap_distribution %&gt;% get_ci(type = &quot;percentile&quot;, level = 0.95) # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 -0.218 0.326 The confidence interval shown here includes the value of 0. We’ll see in Chapter ?? further what this means in terms of this difference being statistically significant or not, but let’s examine a bit here first. The range of plausible values for the difference in the proportion of those that yawned with and without a seed is between -0.218 and 0.326. Therefore, we are not sure which proportion is larger. Some of the bootstrap statistics showed the proportion without a seed to be higher and others showed the proportion with a seed to be higher. If the confidence interval was entirely above zero, we would be relatively sure (about “95% confident”) that the seed group had a higher proportion of yawning than the control group. Note that this all relates to the importance of denoting the order argument in the calculate() function. Since we specified &quot;seed&quot; and then &quot;control&quot; positive values for the statistic correspond to the &quot;seed&quot; proportion being higher, whereas negative values correspond to the &quot;control&quot; group being higher. We, therefore, have evidence via this confidence interval suggesting that the conclusion from the Mythbusters show that “yawning is contagious” being “confirmed” is not statistically appropriate. References "],
["11-pvalues.html", "Chapter 11 P-values 11.1 Stochastic Proof by Contradiction 11.2 Repeated samples, the null hypothesis, and p-values 11.3 P-value and Null Distribution Example 11.4 Interpretation of P-values", " Chapter 11 P-values In Chapter 10, we covered how to use the theory of repeated samples to make inferences from a sample (your data) to a population. To do so, we introduced the counterfactual thinking that underpins statistical reasoning, wherein making inferences requires you to imagine alternative versions of your data that you might have under other possible samples selected in the same way. In this chapter, we extend this counterfactual reasoning to imagine other possible samples you might have seen if you knew the trend in the population. This way of thinking will lead us to define p-values. Needed Packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 2.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(infer) library(ggplot2movies) 11.1 Stochastic Proof by Contradiction In many scientific pursuits, the goal is not simply to estimate a population parameter. Instead, the goal is often to understand if there is a difference between two groups in the population or if there is a relationship between two (or more) variables in the population. For example, we might want to know if average SAT scores differ between men and women, or if there is a relationship between education and income in the population in the United States. Let’s take the difference in means between two groups as a motivating example. In order to prove that there is a difference between average SAT scores for men and women, we might proceed with what is in math called a proof by contradiction. Here, however, this proof is probabilistic (aka stochastic). Stochastic Proof by Contraction: There are three steps in a Proof by Contradiction. In order to illustrate these, assume we wish to prove that there is a relationship between X and Y. Negate the conclusion: Begin by assuming the opposite – that there is no relationship between X and Y. Analyze the consequences of this premise: If there is no relationship between X and Y in the population, what would the sampling distribution of the estimate of the relationship between X and Y look like? Look for a contradiction: Compare the relationship between X and Y observed in your sample to this sampling distribution. How (un)likely is this observed relationship? If likelihood of the observed relationship is small, then this is evidence that there is a relationship between X and Y in the population. 11.2 Repeated samples, the null hypothesis, and p-values 11.2.1 Null hypothesis In the example of asking if there is a difference in SAT scores between men and women, you will note that in order to prove that there is a difference, we begin by assuming that there is not a difference (Step 1). We call this the null hypothesis – it is the hypothesis we are attempting to disprove. The most common null hypotheses are: A parameter is 0 in the population. There is no difference between two or more groups in the population. There is no relationship between two variables in the population. Importantly, this hypothesis is about the value or relationship in the population, not the sample. (This is a very easy mistake to make). Remember, you have data in your sample, so you know without a doubt if there is a difference or relationship in your data (that is your estimate). What you do not know is if there is a difference or relationship in the population. Once a null hypothesis is determined, the next step is to determine what the sampling distribution of the estimator would be if this null hypothesis were true (Step 2). We can determine what this null distribution would look like in two ways, just as with sampling distributions more generally: using mathematical theory or using computational approximation (bootstrapping). 11.2.2 P-values Once the distribution of the sample statistic is determined under the null hypothesis, to complete the stochastic proof by contradiction, you simply need to ask: Given this distribution, how likely is it that I would have drawn a random sample in which the estimated value is this extreme or more extreme? This is the p-value: The probability of your observing an estimate as extreme as the one you observed if the null hypothesis is true. If this p-value is small, it means that this data is unlikely to occur under the null hypothesis, and thus the null hypothesis is unlikely to be true. (See, proof by contradiction!) FIGURE 11.1: P-value diagram In general, in order to estimate a p-value, you first need to standardize your sample statistic. This standardization makes it easier to determine the sampling distribution under the null hypothesis. This standardization is easiest when the sampling distribution of the estimator itself is normally distributed, as is the case for the sample mean, the difference in sample means, the relationship between two continuous covariates, and many others. In these cases, this standardization is conducted using the following formula: \\[t\\_stat = \\frac{Estimate - Null \\ \\ value}{SE(Estimate)}\\] This t-statistic is then used to determine the sampling distribution under the null hypothesis and the p-value based upon the observed value. There are two approaches one can use here: based on mathematical theory or based upon simulation (bootstrap). 11.3 P-value and Null Distribution Example 11.3.1 IMDB data The movies dataset in the ggplot2movies package contains information on 58,788 movies that have been rated by users of IMDB.com. movies # A tibble: 58,788 x 24 title year length budget rating votes r1 r2 r3 r4 r5 r6 &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 $ 1971 121 NA 6.4 348 4.5 4.5 4.5 4.5 14.5 24.5 2 $100… 1939 71 NA 6 20 0 14.5 4.5 24.5 14.5 14.5 3 $21 … 1941 7 NA 8.2 5 0 0 0 0 0 24.5 4 $40,… 1996 70 NA 8.2 6 14.5 0 0 0 0 0 5 $50,… 1975 71 NA 3.4 17 24.5 4.5 0 14.5 14.5 4.5 6 $pent 2000 91 NA 4.3 45 4.5 4.5 4.5 14.5 14.5 14.5 7 $win… 2002 93 NA 5.3 200 4.5 0 4.5 4.5 24.5 24.5 8 &#39;15&#39; 2002 25 NA 6.7 24 4.5 4.5 4.5 4.5 4.5 14.5 9 &#39;38 1987 97 NA 6.6 18 4.5 4.5 4.5 0 0 0 10 &#39;49-… 1917 61 NA 6 51 4.5 0 4.5 4.5 4.5 44.5 # … with 58,778 more rows, and 12 more variables: r7 &lt;dbl&gt;, r8 &lt;dbl&gt;, r9 &lt;dbl&gt;, # r10 &lt;dbl&gt;, mpaa &lt;chr&gt;, Action &lt;int&gt;, Animation &lt;int&gt;, Comedy &lt;int&gt;, # Drama &lt;int&gt;, Documentary &lt;int&gt;, Romance &lt;int&gt;, Short &lt;int&gt; We’ll focus on a random sample of 68 movies that are classified as either “action” or “romance” movies but not both. We disregard movies that are classified as both so that we can assign all 68 movies into either category. Furthermore, since the original movies dataset was a little messy, we provided a pre-wrangled version of our data in the movies_sample data frame included in the moderndive package (you can look at the code to do this data wrangling here): movies_sample # A tibble: 68 x 4 title year rating genre &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; 1 Underworld 1985 3.1 Action 2 Love Affair 1932 6.3 Romance 3 Junglee 1961 6.8 Romance 4 Eversmile, New Jersey 1989 5 Romance 5 Search and Destroy 1979 4 Action 6 Secreto de Romelia, El 1988 4.9 Romance 7 Amants du Pont-Neuf, Les 1991 7.4 Romance 8 Illicit Dreams 1995 3.5 Action 9 Kabhi Kabhie 1976 7.7 Romance 10 Electric Horseman, The 1979 5.8 Romance # … with 58 more rows The variables include the title and year the movie was filmed. Furthermore, we have a numerical variable rating, which is the IMDB rating out of 10 stars, and a binary categorical variable genre indicating if the movie was an Action or Romance movie. We are interested in whether there is a difference in average ratings between the Action and Romance genres. Let’s calculate the number of movies, the mean rating, and the standard deviation split by the binary variable genre. We’ll do this using dplyr data wrangling verbs, in particular the count of each type of movies using the n() summary statistic function. movies_sample %&gt;% group_by(genre) %&gt;% summarize(n = n(), mean_rating = mean(rating), std_dev = sd(rating)) # A tibble: 2 x 4 genre n mean_rating std_dev &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Action 32 5.28 1.36 2 Romance 36 6.32 1.61 We start by assuming there is no difference, therefore our null value is \\(\\mu_1 - \\mu_2 = 0\\). We want to calculate the t-statistic for this scenario: \\[t\\_stat = \\frac{Estimate - Null \\ \\ value}{SE(Estimate)} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\] Let’s compute all the necessary values from our sample data. So we have 36 movies with an average rating of 6.32 stars out of 10 and n_action movies with a sample mean rating of 5.28 stars out of 10. The difference in these average ratings is thus 6.32 - 5.28 = 1.05. Our resulting t_stat is 2.906. There appears to be an edge of 1.05 stars in romance movie ratings. The question is however, are these results indicative of a true difference for all romance and action movies? Or could this difference be attributable to chance and sampling variation? Computing a p-value for this t-statistic can help us to answer this. 11.3.2 Theory based p-values While we have shown that the sampling distribution of the sample mean and of the regression coefficient estimates are normally distributed, this is not always the case for the t-statistic computed above. This is because the standard errors of many estimators, which appear on the denominator of the t-statistic, are a function of an additional estimated quantity – the sample variance \\(s^2\\). When this is the case, the sampling distribution for the t-statistic is a t-distribution with a specified degrees of freedom (df). Degrees of freedom for this distribution are provided in the table below. Statistics Population parameter Estimator t-distribution df Mean \\(\\mu\\) \\(\\bar{x}\\) \\(n – 1\\) Diff in means \\(\\mu_1 -\\mu_2\\) \\(\\bar{x}_1 - \\bar{x}_2\\) \\(n_1 + n_2 – 2\\) The t-distribution is very similar to the standard normal distribution: It has a mean of 0 under the null hypothesis. It is symmetric But there is one important difference: The t-distribution has heavier tails than the normal distribution. This means that large values are more likely under the t-distribution than the normal distribution. ggplot(data.frame(x = c(-6, 6)), aes(x = x)) + stat_function(fun = dnorm, aes(colour = &quot;Standard Normal&quot;), size = 1) + stat_function(fun = dt, args = list(df = 2), aes(colour = &quot;t-distribution, df = 2&quot;), size = 1) FIGURE 11.2: Standard normal vs. t-distribution ggplot(data.frame(x = c(-6, 6)), aes(x = x)) + stat_function(fun = dnorm, aes(colour = &quot;Standard Normal&quot;), size = 1) + stat_function(fun = dt, args = list(df = 2), aes(colour = &quot;t-distribution, df = 2&quot;), size = 1) + stat_function(fun = dt, args = list(df = 30), aes(colour = &quot;t-distribution, df = 30&quot;), size = 1) FIGURE 11.3: t-distribution approximately normal in large samples We can calculate a p-value by asking: Assuming the null distribution, what is the probability that we will see a t-statistic as extreme as the one from our data? To answer this, we can calculate this using pt(t_stat, df), where t_stat is the t-statistic we calculated from our sample data, and df is the appropriate degrees of freedom for our estimator and sample size. There is a default argument lower.tail = TRUE in the pt() function, which means it returns the probability to the left of the t_stat value you enter. For example, Figure 11.4 shows the shaded probabilty that is implied by the code pt(1.5, df = 30, lower.tail = TRUE) and Figure 11.5 shows the shaded probabilty that is implied by the code pt(1.5, df = 30, lower.tail = FALSE). FIGURE 11.4: pt(1.5, df = 30, lower.tail = TRUE) FIGURE 11.5: pt(1.5, df = 30, lower.tail = TRUE) Caveat: It is important to note that the t-distribution is often referred to as a “small sample” distribution. That is because once the degrees of freedom are large enough (when the sample size is large), the t-distribution is actually quite similar to the normal distribution (See Figure ?? ). For analysis purposes, however, you don’t need to determine when to use one or the other as your sampling distribution: just always use the t-distribution. In our IMDB movies example, we observe a t_stat = 2.91, and we want to know what the probability of observing a t-statistic as large as this would be under the null distribution. Therefore we want to include the argument lower.tail = FALSE when computing our p-value. Note our \\(df = n_1 + n_2 - 2 = 36 + 32 - 2 = 66\\), so our p-value is given by pt(2.91, 66, lower.tail = FALSE) = 0.002. This tells us that if the null distribution is true (i.e. there is no true difference between average ratings of romance and action movies on IMDB), we would only observe a difference as large as we did 0.25% of the time. This provides evidence - via proof by contradiction - that the null distribution is likely false; that is, there is likely a true difference in average ratings of romance and action moview on IMDB. 11.3.3 Bootstrap p-values In Section 10.2.2, we introduced you to the infer package to use the bootstrap procedure to approximate a sampling distribution, compute approximate standard errors, and construct confidence intervals. Recall that theinfer package workflow emphasizes each of the steps in the overall process in Figure 11.6 using function names that are intuitively named: specify() the variables of interest in your data frame generate() replicates of bootstrap resamples with replacement calculate() the summary statistic of interest visualize() the resulting bootstrap distribution and the confidence interval. FIGURE 11.6: Confidence intervals via the infer package. In this section, we now show you how to extend and modify the previously seen infer pipeline to compute p-values. You’ll notice that the basic outline of the workflow is almost identical, except for an additional hypothesize() step between specify() and generate(), as can be seen in Figure 11.7. FIGURE 11.7: Hypothesis testing via the infer package. Furthermore, we’ll use a pre-specified significance level \\(\\alpha\\) = 0.05 for this hypothesis test. Let’s leave the justification of this choice of \\(\\alpha\\) until later on in Section ??. infer package workflow 1. specify variables Recall that we use the specify() verb to denote the response and, if needed, explanatory variables for our study. In this case, since we are interested in any potential effects of genre on IMDB rating, we set rating as the response variable and genre as the explanatory variable using the formula argument using the notation &lt;response&gt; ~ &lt;explanatory&gt; where &lt;response&gt; is the name of the response variable in the data frame and &lt;explanatory&gt; is the name of the explanatory variable. So in our case it is rating ~ genre. movies_sample %&gt;% specify(formula = rating ~ genre) Response: rating (numeric) Explanatory: genre (factor) # A tibble: 68 x 2 rating genre &lt;dbl&gt; &lt;fct&gt; 1 3.1 Action 2 6.3 Romance 3 6.8 Romance 4 5 Romance 5 4 Action 6 4.9 Romance 7 7.4 Romance 8 3.5 Action 9 7.7 Romance 10 5.8 Romance # … with 58 more rows Again, notice how the movies_sample data itself doesn’t change, but the Response: rating (factor) and Explanatory: genre (factor) meta-data do. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data as we saw in Section 4.4. 2. hypothesize the null In order to compute p-values using the infer workflow, we need a new step: hypothesize(). Recall that we are starting out with the hypothesis that there is no difference in average ratings between genres. We specify this in our infer workflow by setting the null argument of the hypothesize() function to either: &quot;point&quot; for hypotheses involving a single group/sample or &quot;independence&quot; for hypotheses involving two groups/samples In our case, since we have two groups (the “Romance” and “Action” genres), we set null = &quot;independence&quot;. movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) # A tibble: 68 x 2 rating genre &lt;dbl&gt; &lt;fct&gt; 1 3.1 Action 2 6.3 Romance 3 6.8 Romance 4 5 Romance 5 4 Action 6 4.9 Romance 7 7.4 Romance 8 3.5 Action 9 7.7 Romance 10 5.8 Romance # … with 58 more rows Again, the data has not changed yet. This will occur at the upcoming generate() step; we’re merely setting meta-data for now. 3. generate replicates After we have set the null hypothesis, we simulate observations assuming the null hypothesis is true by randomly shuffling (i.e. permuting) the genre variable. First, imagine a hypothetical universe with no difference in average ratings across genres. In such a hypothetical universe, the genre of movie would have no bearing on its IMDB rating. In our movies_sample dataframe then, the genre variable would thus be an irrelevant label. If the genre label is irrelevant, then we can randomly “shuffle” this label to no consequence! Randomly shuffling the 68 genre labels in our dataframe is equivalent to generating one bootstrap sample. We can create 1000 such bootstrap samples in which there is no relationship between genre and rating by setting reps = 1000 in the generate() function. However, unlike with confidence intervals where we generated replicates using type = &quot;bootstrap&quot; resampling with replacement, we’ll now perform shuffles/permutations by setting type = &quot;permute&quot;. Note that shuffles/permutations are a form of resampling without replacement. movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) Note that the resulting data frame has 68,000 rows. This is because we performed shuffles/permutations of the 68 values of genre 1000 times and thus 68,000 = 1000 \\(\\times\\) 68. Accordingly the variable replicate, indicating which resample each row belongs to, has the value 1 68 times, the value 2 68 times, all the way through to the value 1000 68 times. 4. calculate summary statistics Now that we have 1000 replicated “shuffles” assuming the null hypothesis that both “Romance” and “Action” movies have the same ratings on average, let’s calculate() the appropriate summary statistic for each of our 1000 shuffles. For each of our 1000 shuffles, we can calculate the difference in means by setting stat = &quot;diff in means&quot;. Let’s save the result in a data frame called null_distribution: set.seed(2018) null_distribution &lt;- movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Romance&quot;, &quot;Action&quot;)) null_distribution # A tibble: 1,000 x 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 -0.783 2 2 -0.0389 3 3 -0.110 4 4 -0.0979 5 5 0.575 6 6 0.203 7 7 0.280 8 8 -0.7 9 9 0.298 10 10 0.233 # … with 990 more rows Observe that we have 1000 values of stat, each representing one “shuffled” instance of \\(\\bar{x}_{Romance} - \\bar{x}_{Action}\\) in a hypothesized world of no rating difference across genres. But wait! What happened in real-life? What was the observed difference in average ratings? In other words, what was the observed estimate \\(\\bar{x}_{Romance} - \\bar{x}_{Action}\\)? Recall from Section 11.3.1 that we computed this observed difference 6.32 - 5.28 = 1.05. We can also achieve this using the code above but with the hypothesize() and generate() steps removed. Let’s save this in obs_diff_means obs_diff_means &lt;- movies_sample %&gt;% specify(formula = rating ~ genre) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Romance&quot;, &quot;Action&quot;)) obs_diff_means # A tibble: 1 x 1 stat &lt;dbl&gt; 1 1.05 5. visualize the p-value The final step is to measure how surprised we would be by a difference in average ratings of 1.05 in a hypothesized universe of no difference across genres. If very surprised, then we would be inclined to reject the validity of our hypothesized universe. We start by visualizing the null distribution of our 1000 values of \\(\\bar{x}_{Romance} - \\bar{x}_{Action}\\) using visualize() in Figure 11.8. Recall that these are values of the difference in average ratings assuming the null hypothesis is true. visualize(null_distribution, binwidth = 0.1) FIGURE 11.8: Bootstrap distribution Let’s now add what happened in real-life to Figure 11.8, the observed difference in promotions rates of 6.32 - 5.28 = 1.05. However, instead of merely adding a vertical line using geom_vline(), let’s use the shade_p_value() function with obs_stat set to the observed test statistic value we saved in obs_diff_means and direction = &quot;right&quot;: visualize(null_distribution, binwidth = 0.1) + shade_p_value(obs_stat = obs_diff_means, direction = &quot;right&quot;) FIGURE 11.9: Shaded histogram to show p-value. In the resulting Figure 11.9, the solid red line marks our observed difference in means. However, what does the shaded-region correspond to? This is the p-value. Judging by the shaded region in Figure 11.9, it seems we would somewhat rarely observe differences in ratings of 1.05 or more in a hypothesized universe of no differences across genres. In other words, the p-value is somewhat small. Hence, we would be inclined to reject this hypothesized universe. What fraction of the null distribution is shaded? In other words, what is the exact p-value? We can compute its numerical value using the get_p_value() function using the exact same arguments as with the visualize() code above: null_distribution %&gt;% get_p_value(obs_stat = obs_diff_means, direction = &quot;right&quot;) # A tibble: 1 x 1 p_value &lt;dbl&gt; 1 0.004 In other words, the probability of observing a difference in ratings as large as 1.05 due to sampling variation alone is only 0.004 = 0.4%. 11.4 Interpretation of P-values Like many statistical concepts, p-values are often misunderstood and misinterpreted. Remember, a p-value is the probability that you would observe data as extreme as the data you do if, in fact, the null hypothesis is true. As Wikipedia notes: The p-value is not the probability that the null hypothesis is true, or the probability that the alternative hypothesis is false. The p-value is not the probability that the observed effects were produced by random chance alone. The p-value does not indicate the size or importance of the observed effect. Finally, remember that the p-value is a probabilistic attempt at making a proof by contradiction. Unlike in math, this is not a definitive proof. For example, if the p-value is 0.10, this means that if the null hypothesis is true, there is a 10% chance that you would observe an effect as large as the one in your sample. Depending upon if you are a glass-half-empty or glass-half-full kind of person, this could be seen as large or small: “Only 10% chance is small, which is unlikely. This must mean that the null hypothesis is not true,” or “But we don’t know that for sure: in 10% of possible samples, this does occur just by chance. The null hypothesis could be true.” This will be important to keep in mind as we move towards using p-values for decision making in Chapter 12. "],
["12-hypothesis-tests.html", "Chapter 12 Hypothesis tests 12.1 Conducting Hypothesis Tests 12.2 Interpreting regression tables 12.3 More advanced points to consider", " Chapter 12 Hypothesis tests In Chapter 11, we introduced the p-value, which provides analysts with a probability (between 0 and 1) that the observed data would be found if the null hypothesis were true. Readers familiar with the use of statistics may have noticed, however, that Chapter 11 did not refer to any criteria (e.g., p &lt; .05) or use the phrase “statistically significant”. This is because the concept of a p-value is distinct from the use of a p-value to make a decision. In this chapter, we introduce hypothesis testing, which can be used for just this purpose. ###Needed Packages {-} Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 2.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(infer) library(ggplot2movies) ##Decision making {#decision-making} Remember that a p-value is a probabilistic proof by contradiction. It might show that the chance that the observed data would occur under the null hypothesis is 2%, 20%, or 50%. But at what level is the evidence enough that we would decide that the null hypothesis must not be true? Conventional wisdom is to use \\(p &lt; 0.05\\) as this threshold, where \\(p\\) denotes the p-value. But as many have pointed out – particularly in the current ‘replication crisis’ era – this threshold is arbitrary. Why is 5% considered small enough? Why not 0.5%? Why not 0.05%? Decisions regarding these thresholds require substantive knowledge of a field, the role of statistics in science, and some important trade-offs, which we will introduce next. ##Decision making trade-offs {#trade-offs} Imagine that you’ve been invited to a party, and you are trying to decide if you should go. On the one hand, the party might be a good time, and you’d be happy that you went. On the other hand, it might not be that much fun and you’d be unhappy that you went. In advance, you don’t know which kind of party it will be. How do you decide? We can formalize this decision making in terms of a 2x2 table crossing your decision (left) with information about the party (top): TABLE 12.1: Party decision making Party is fun Party is not fun Go to party Great decision! Type I error (wasted time) Stay home Type II error (missed out) Great decision! As you can see from this table, there are 4 possible combinations. If you decide to go to the party and it is in fact fun, you’re happy. If you decide to stay home and you hear from your friends that it was terrible, you’re happy. But in the other two cases you are not happy: Type I error: You decide to go to the party and the party is lame. You’ve now wasted your time and are unhappy. Type II error: You decide to forgo the party and stay home, but you later hear that the party was awesome. You’ve now missed out and are unhappy. In life, we often have to make decisions like this. In making these decisions, there are trade-offs. Perhaps you are the type of person that has FOMO – in that case, you may really want to minimize your Type II error, but at the expense of attending some boring parties and wasting your time (a higher Type I error). Or perhaps you are risk averse and hate wasting time – in which case you want to minimize your Type I error, at the expense of missing out on some really great parties (a higher Type II error). There are a few important points here: * When making a decision, you cannot know in advance what the actual outcome will be. * Sometimes your decision will be the right one. Ideally, you’d like this to be most of the time. * But, sometimes your decision will be the wrong one. Importantly, you cannot minimize both Type I and II errors at the same time. One will be minimized at the expense of the other. * Depending upon the context, you may decide that minimizing Type I or II errors is more important to you. These features of decision-making play out again and again in life. In the next sections, we provide two common examples, one in medicine, the other in law. ###Medicine {#medicine} Imagine that you might be pregnant and take a pregnancy test. This test is based upon levels of HcG in your urine, and when these levels are “high enough” (determined by the pregnancy test maker), the test will tell you that you are pregnant (+). If the levels are not “high enough”, the test will tell you that you are not pregnant (-). Depending upon how the test determines “high enough” levels of HcG, however, the test might be wrong. To see how, examine the following table. TABLE 12.2: Pregnancy test decision making Pregnant Not pregnant Test + Correct Type I error: False Positive Test - Type II error: False Negative Correct As the table notes, in two of the cases, the test correctly determines that you are either pregnant or not pregnant. But there are also two cases in which the test (a decision) is incorrect: * Type I error: False Positive. In this case, the test tells you that you are pregnant when in fact you are not. This would occur if the level of HcG required to indicate positive was too low. * Type II error: False Negative. In this case, the test tells you that you are not pregnant but you actually are. This would occur if the level of HcG required to indicate positive is too high. When a pregnancy test manufacturer develops the test, they have to pay attention to these two possible error types and think through the trade-offs of each. For example, if they wanted to minimize the Type II error (False Negative), they could just create a test that always tells people they are pregnant (i.e., HcG &gt;= 0). Conversely, if they wanted to minimize the Type I error (False Positive), they could set the HcG level to be very high, so that it only detects pregnancy for those that are 6 months pregnant. Of course, the trade-off here is that certainly many who took the test would actually be pregnant, and yet the test would tell them otherwise. In developing these tests, which do you think test manufacturers focus on minimizing: Type I or II errors? ###Law {#law} Imagine that you are on the jury of a criminal trial. You are presented with evidence that a crime has been committed and must make a decision regarding the guilt of the defendant. But you were not there when the crime was committed, so it is impossible to know with 100% accuracy that your decision is correct. Instead, you again encounter this 2x2 table: TABLE 12.3: Criminal trial decision making Guilty Innocent “Guilty” verdict Correct Type I error: Wrongly Convicted “Not Guilty” verdict Type II error: Insufficient Evidence Correct As the table notes, in two of the cases, the jury correctly determines that the defendant is either guilty or not. But there are also two cases in which the jury’s decision is incorrect: * Type I error: Wrongly Convicted. In this case, the jury decides that the defendant is guilty when in fact they are not. This might be because evidence that was presented was falsified or because prejudices and discrimination affect how the jury perceives the defendant. * Type II error: Insufficient Evidence. In this case, the jury decides that the defendant is “not guilty” when in fact they are. This is typically because there is insufficient evidence. In the US court system, the assumption is supposed to be that a defendant is innocent until proven guilty, meaning that a high burden of proof is required to find a defendant guilty. This means that the system is designed to have a low Type I error. The trade-off implicit in this is that the Type II error may be higher – that is, that because the burden of proof is high, some perpetrators will “get off”. (Note, of course, that we’re describing the ideal; as the Innocence Project’s work shows, Type I errors are more common than we’d like, particularly among racial minorities). ###Commonalities Before connecting these to statistical decision making, it’s interesting to note that in all three of the cases we’ve introduced here – party attendance, medicine, and law – the minimization of Type I error is often primary. That is, we’d prefer a decision rule that doesn’t send us to parties we don’t like, doesn’t tell us we are pregnant when we aren’t, and doesn’t wrongfully convict people of crimes. This is not to say Type II error doesn’t matter, but that it is often seen as secondary to Type I. ##Hypothesis test: Decision making in statistics {#ht} The same sort of decision making problems face statistics as well: based on some p-value criterion, we could either reject the null hypothesis or not. And either the null hypothesis is true, or it is not – in which case some alternative hypothesis must be true. This is the first time we have mentioned an alternative hypothesis. This hypothesis is what we are seeking evidence to prove when we are conducting what is called a hypothesis test: TABLE 12.4: Hypothesis test decision making Alternative Model Null Model Reject Correct \\(\\alpha =\\) Type I error Not Reject \\(\\beta =\\) Type II error Correct There is a lot of new information here to define: We reject the null hypothesis if the p-value is smaller than some threshold (i.e., p &lt; threshold). We do not reject the null hypothesis if the p-value is larger than this threshold (i.e., p &gt; threshold). The null model is as we’ve defined it in Chapter 11. In most cases it is that there is no effect, no difference, or no relationship. (Other null hypotheses are possible, these are just the most common.) The alternative model is a model we are seeking evidence to prove is correct. For example, the alternative model may be that there is an average non-zero difference between men and women’s SAT scores. Or that there is a correlation between income and education. Specifying an alternative can be tricky – usually this is based both on substantive knowledge and findings from prior studies. Just as in the other decision-making context, there are two cases in which these decisions are correct, and two cases in which they are not: \\(\\boldsymbol{\\alpha} =\\) Type I error: The test rejects the null hypothesis (because p &lt; threshold), yet the null hypothesis is actually true. For example, the test indicates that there is a relationship between education and income when in fact there is not. \\(\\boldsymbol{\\beta =}\\) Type II error: The test does not reject the null hypothesis (because p &gt; threshold), but the alternative hypothesis is actually true. For example, the test indicates that there is not enough evidence to indicate a relationship between education and income, yet in fact there is a relationship. Just as in your social life, medicine, and law, these two error types are in conflict with one another. A test that minimizes Type I error completely (by setting the threshold to 0) never rejects the null hypothesis, thus maximizing the Type II error. And vice versa, a test that minimizes Type II error completely (by setting the threshold to 1) always rejects the null hypothesis, thus maximizing the Type I error. In science, these values have been somewhat arbitrarily set as: \\(\\boldsymbol{\\alpha} =\\) Type I error: set the threshold to 0.05. Thus, finding that there is a 5% or smaller chance under the null hypothesis that a sample would produce a result this extreme is deemed sufficient evidence to decide the null hypothesis is not true. \\(\\boldsymbol{\\beta =}\\) Type II error: 0.20. That is, we are willing to accept that there is a 20% chance that we do not reject the null hypothesis when in fact the alternative hypothesis is true. This use of a threshold for rejecting a null hypothesis gives rise to some important language: When \\(p &lt; 0.05\\) (or whatever threshold is used), we say that we reject the null hypothesis. This is often referred to as a “statistically significant” effect. When \\(p &gt; 0.05\\) (or whatever threshold is used), we say that we do not have enough evidence to reject the null hypothesis. Note that it is not appropriate to say that we accept the null hypothesis. Finally, there is one more piece of vocabulary that is important: **Power = 1 – Type II error = \\(1-\\beta\\). Power is the probability that we will reject the null hypothesis when in fact it is false. For example, if our Type II error is 0.20, then we can say our test has 80% power for rejecting the null hypothesis when the alternative hypothesis is true. Conversely, if a study has not been designed well, it may be under-powered to detect an effect that is substantively important. The power of a hypothesis test depends on: The magnitude of the effect (e.g. how big the true parameter value is in the population) Sample size (n) The type I error rate (\\(\\alpha\\)) Sometimes other sample statistics 12.1 Conducting Hypothesis Tests In data analysis, hypothesis tests can be broken down into the following framework given by Allen Downey here: FIGURE 12.1: Hypothesis Testing Framework In practice, conducting a hypothesis test is straightforward: Determine the sampling distribution based upon the null model Find the test-statistic value observed in your data Calculate a p-value Reject the null hypothesis if the p-value is less than a pre-defined threshold. This last point is significant – for the hypothesis test to be valid, you must pre-specify your threshold, not after you have seen the p-value in your data. ###Promotions Example {#ht-activity} Let’s consider a study that investigated gender discrimination in the workplace that was published in the “Journal of Applied Psychology” in 1974. This data is also used in the OpenIntro series of statistics textbooks. Study participants included 48 male bank supervisors who attended a management institute at University of North Carolina in 1972. The supervisors were asked to assume the hypothetical role of a personnel director at the bank. Each supervisor was given a job candidate’s personnel file and asked to decide whether or not the candidate should be promoted to a manager position at the bank. Each of the personnel files given to the supervisors were identical except that half of them indicated that the candidate was female and half indicated the candidate was male. Personnel files were randomly distributed to the 48 supervisors. Because only the candidate’s gender varied from file to file, and the files were randomly assigned to study participants, the researchers were able to isolate the effect of gender on promotion rates. The moderndive package contains the data on the 48 candidates in the promotions data frame. Let’s explore this data first: promotions # A tibble: 48 x 3 id decision gender &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 1 promoted male 2 2 promoted male 3 3 promoted male 4 4 promoted male 5 5 promoted male 6 6 promoted male 7 7 promoted male 8 8 promoted male 9 9 promoted male 10 10 promoted male # … with 38 more rows The variable id acts as an identification variable for all 48 rows, the decision variable indicates whether the candidate was selected for promotion or not, while the gender variable indicates the gender of the candidate indicated on the personnel file. Recall that this data does not pertain to 24 actual men and 24 actual women, but rather 48 identical personnel files of which 24 were indicated to be male candidates and 24 were indicated to be female candidates. Let’s perform an exploratory data analysis of the relationship between the two categorical variables decision and gender. Recall that we saw in Section 3.8.3 that one way we can visualize such a relationship is using a stacked barplot. ggplot(promotions, aes(x = gender, fill = decision)) + geom_bar() + labs(x = &quot;Gender of name on resume&quot;) FIGURE 12.2: Barplot of relationship between gender and promotion decision. Observe in Figure 12.2 that it appears that female personnel files were much less likely to be accepted for promotion. Let’s quantify these promotions rates by computing the proportion of personnel files accepted for promotion for each group using the dplyr package for data wrangling: promotions %&gt;% group_by(gender, decision) %&gt;% summarize(n = n()) # A tibble: 4 x 3 # Groups: gender [2] gender decision n &lt;fct&gt; &lt;fct&gt; &lt;int&gt; 1 male not 3 2 male promoted 21 3 female not 10 4 female promoted 14 So of the 24 male files, 21 were selected for promotion, for a proportion of 21/24 = 0.875 = 87.5%. On the other hand, of the 24 female files, 14 were selected for promotion, for a proportion of 14/24 = 0.583 = 58.3%. Comparing these two rates of promotion, it appears that males were selected for promotion at a rate 0.875 - 0.583 = 0.292 = 29.2% higher than females. The question is however, does this provide conclusive evidence that there is gender discrimination in this context? Could a difference in promotion rates of 29.2% still occur by chance, even in a hypothetical world where no gender-based discrimination existed? To answer this question, we can conduct the following hypothesis test: \\[H_0: \\pi_m = \\pi_f\\] \\[H_1: \\pi_m \\neq \\pi_f,\\] where \\(\\pi_f\\) is the proportion of female files selected for promotion and \\(pi_m\\) is the propotion of male files selected for promotion. Here the null hypothesis corresponds to the scenario in which there is no gender discrimination; that is, males and females are promoted at identical rates. We will specify this test ahead of time to have \\(\\alpha = 0.05\\). That is, we are comfortable with a 5% Type I error rate, and we will reject the null hypothesis if \\(p &lt; 0.05\\). Note the null hypothesis can be rewritten as \\[H_0: \\pi_m - \\pi_f = 0\\] by subtracting \\(\\pi_m\\) from both sides. Therefore, the population proportion we are interested in is \\(\\pi_m - \\pi_f\\). Referring back to Table 10.1, we can see that the appropriate estimator is \\(\\hat{\\pi}_m - \\hat{\\pi}_f\\). Under the null hypothesis, \\(\\hat{\\pi}_m - \\hat{\\pi}_f\\) is normally distributed with mean \\(\\pi_m - \\pi_f\\) and standard error \\(\\sqrt{\\frac{\\hat{\\pi}_m(1-\\hat{\\pi}_m)}{n_m} + \\frac{\\hat{\\pi}_f(1-\\hat{\\pi}_f)}{n_f}}\\). Recall from Chapter 11 that we calculate a test statistic using the following general formula: \\[t\\_stat = \\frac{Estimate - Null \\ \\ value}{SE(Estimate)}\\] In this example, we have \\[test\\_stat = \\frac{(\\hat{\\pi}_m - \\hat{\\pi}_f) - 0}{\\sqrt{\\frac{\\pi_m(1-\\pi_m)}{n_m} + \\frac{\\pi_f(1-\\pi_f)}{n_f}}}\\] Note that the null value here is \\(0\\) because our null hypothesis states \\(\\pi_m - \\pi_f = 0\\). We can use the promotions data to compute this observed test statistic. However, on the denominator we need separate values for \\(\\pi_m\\) and \\(\\pi_f\\), which aren’t specified in our null hypothesis. When conducting a hypothesis test on the difference in proportions, we compute a pooled proportion to plug in for these unknown values. This pooled estimate, which we will denote \\(\\pi_0\\) can be calculated by \\[\\pi_{0} = \\frac{\\# \\ of \\ successes_1 + \\# of successes_2}{n_1 + n_2}.\\] Therefore, the test statistic is computed by \\[t\\_stat = \\frac{(\\hat{\\pi}_m - \\hat{\\pi}_f) - 0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n_m} + \\frac{\\pi_0(1-\\pi_0)}{n_f}}}\\] results &lt;- promotions %&gt;% group_by(gender, decision) %&gt;% summarize(n = n()) %&gt;% mutate(prop = n / sum(n)) male_promotions &lt;- as.numeric(results[2,3]) female_promotions &lt;- as.numeric(results[4,3]) n_m &lt;- 24 n_f &lt;- 24 pi_0 &lt;- (male_promotions + female_promotions) / (n_m + n_f) pi_0 [1] 0.729 SE_pi_0 &lt;- sqrt(pi_0 * (1 - pi_0) / n_m + pi_0 * (1 - pi_0) / n_f) pi_hat_m &lt;- as.numeric(results[2,4]) pi_hat_f &lt;- as.numeric(results[4,4]) test_stat &lt;- (pi_hat_m - pi_hat_f) / SE_pi_0 test_stat [1] 2.27 In the promotions example, \\[\\pi_0 = \\frac{21 + 14}{24 + 24} = 0.729,\\] and the test statistic is equal to 2.27. We can then use this test statistic to compute our p-value. p_value &lt;- pnorm(test_stat, lower.tail = FALSE)*2 p_value [1] 0.023 FIGURE 12.3: P-value for Promotions Hypothesis Test Note that we use the function pnorm() here because our test statistic follows the normal distribution. We set lower.tail = FALSE so that it gives us the probability in the left tail of the distribution, and we multiply by 2 because we are conducting a two-sided hypothesis test. The p-value = 0.02 is represented by the shaded region in Figure 12.3. Because our p-value is less than our pre-specified level of \\(\\alpha = 0.05\\), we reject the null hypothesis and conclude that there is sufficient evidence of gender discrimination in this context. 12.1.1 Regression Example Let’s return to our example from Chapter 6 on teaching evaluations and demonstrate how to conduct a hypothesis test on regression coefficients. Recall using simple linear regression we modeled the relationship between A numerical outcome variable \\(y\\), the instructor’s teaching score and A single numerical explanatory variable \\(x\\), the instructor’s “beauty” score. We first created an evals_ch6 data frame that selected a subset of variables from the evals data frame included in the moderndive package. This evals_ch6 data frame contains only the variables of interest for our analysis, in particular the instructor’s teaching score and the “beauty” rating bty_avg: evals_ch6 &lt;- evals %&gt;% select(ID, score, bty_avg, age) glimpse(evals_ch6) Observations: 463 Variables: 4 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18… $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4… $ bty_avg &lt;dbl&gt; 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, 3.17, 3… $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 4… In Section 6.1.1, we performed an exploratory data analysis of the relationship between these two variables. We saw there that there was a weakly positive correlation of 0.187 between the two variables. This was evidenced in Figure 12.4 of the scatterplot along with the “best-fitting” regression line that summarizes the linear relationship between the two variables. Recall in Subsection 6.3.3 that we defined a “best-fitting” line as the line that minimizes the sum of squared residuals. ggplot(evals_ch6, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship between teaching and beauty scores&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 12.4: Relationship with regression line. Looking at this plot again, you might be asking “Does that line really have all that positive of a slope?” It does increase from left to right as the bty_avg variable increases, but by how much? To get to this information, recall that we followed a two-step procedure: We first “fit” the linear regression model using the lm() function with the formula score ~ bty_avg. We saved this model in score_model. We get the regression table by applying the get_regression_table() from the moderndive package to score_model. # Fit regression model: score_model &lt;- lm(score ~ bty_avg, data = evals_ch6) # Get regression table: get_regression_table(score_model) TABLE 12.5: Previously seen linear regression table. term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Using the values in the estimate column of the resulting regression table in Table 12.5, we could then obtain the equation of the “best-fitting” regression line in Figure 12.4: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x\\\\ \\widehat{\\text{score}} &amp;= b_0 + b_{\\text{bty}\\_\\text{avg}} \\cdot\\text{bty}\\_\\text{avg}\\\\ &amp;= 3.880 + 0.067\\cdot\\text{bty}\\_\\text{avg} \\end{aligned} \\] where \\(b_0\\) is the fitted intercept and \\(b_1\\) is the fitted slope for bty_avg. Recall the interpretation of the \\(b_1\\) = 0.067 value of the fitted slope: For every increase of one unit in “beauty” rating, there is an associated increase, on average, of 0.067 units of evaluation score. Thus, the slope value quantifies the relationship between the y variable of score and the x variable bty_avg. We also discussed the intercept value of \\(b_0\\) = 3.88 and its lack of practical interpretation, since the range of possible “beauty” scores does not include 0. Let’s now formally test whether there is a relationship between teaching score and beauty score, beyond what we would expect simply due to chance. 12.1.2 Sampling scenario Let’s now revisit this study in terms of terminology and notation related to sampling we studied in Section ??. First, let’s view the instructors for these 463 courses as a representative sample from a greater study population. In our case, let’s assume that the study population is all instructors at UT Austin and that the sample of instructors who taught these 463 is a representative sample. Unfortunately, we can only assume these two facts without more knowledge of the sampling methodology used by the researchers. Since we are viewing these \\(n\\) = 463 courses as a sample, we can view our fitted slope \\(b_1\\) = 0.067 as a point estimate of the population slope \\(\\beta_1\\). In other words, \\(\\beta_1\\) quantifies the relationship between teaching score and “beauty” average bty_avg for all instructors at UT Austin. Similarly, we can view our fitted intercept \\(b_0\\) = 3.88 as a point estimate of the population intercept \\(\\beta_0\\) for all instructors at UT Austin. Putting these two ideas together, we can view the equation of the fitted line \\(\\widehat{y}\\) = \\(b_0 + b_1 \\cdot x\\) = \\(3.880 + 0.067 \\cdot \\text{bty}\\_\\text{avg}\\) as an estimate of some true and unknown population line \\(y = \\beta_0 + \\beta_1 \\cdot x\\). Thus we can draw parallels between our teaching evals analysis and all the sampling scenarios we’ve seen previously in Table ??. In this chapter, we’ll focus on the final two scenarios: regression slopes and regression intercepts. TABLE 12.6: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Notation. 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) 6 Population regression intercept \\(\\beta_0\\) Fitted regression intercept \\(b_0\\) or \\(\\widehat{\\beta}_0\\) Since we are now viewing our fitted slope \\(b_1\\) and fitted intercept \\(b_0\\) as point estimates based on a sample, these estimates will be subject to sampling variability, as we’ve seen numerous times throughout this book. In other words, if we collected new sample of data on a different set of \\(n\\) = 463 courses and their instructors, the new fitted slope \\(b_1\\) will likely differ from 0.067. The same goes for the new fitted intercept \\(b_0\\). But by how much will they differ? In other words, by how much will these estimates vary? This information is contained in the remaining columns of the regression table in Table 12.5. Our knowledge of sampling from Chapter 9, confidence intervals from Chapter ??, and hypothesis tests from Chapter ?? will help us interpret these remaining columns. 12.2 Interpreting regression tables In Chapters 6 and 7 and in our regression refresher earlier, we focused only on the two leftmost columns the regression table in Table 12.5: term and estimate. Let’s now shift our attention to the remaining columns: std_error, statistic, p_value, lower_ci and upper_ci. TABLE 12.7: Previously seen regression table. term estimate std_error statistic p_value lower_ci upper_ci intercept 3.880 0.076 50.96 0 3.731 4.030 bty_avg 0.067 0.016 4.09 0 0.035 0.099 Given the lack of practical interpretation for the fitted intercept \\(b_0\\), in this section we’ll focus only on the second row of the table corresponding to the fitted slope \\(b_1\\). We’ll first interpret the std_error, statistic, p_value, lower_ci and upper_ci columns. Afterwards in the upcoming Subsection ??, we’ll discuss how R computes these values. 12.2.1 Standard error The third column of the regression table in Table 12.5 std_error corresponds to the standard error of our estimates. Recall the definition of standard error we saw in Subsection ??: The standard error is the standard deviation of any point estimate computed from a sample. So what does this mean in terms of the fitted slope \\(b_1\\) = 0.067? This value is just one possible value of the fitted slope resulting from this particular sample of \\(n\\) = 463 pairs of teaching and beauty scores. However, if we collected a different sample of \\(n\\) = 463 pairs of teaching and beauty scores, we will almost certainly obtain a different fitted slope \\(b_1\\). This is due to sampling variability. Say we hypothetically collected 1000 such samples of pairs of teaching and beauty scores, computed the 1000 resulting values of the fitted slope \\(b_1\\), and visualized them in a histogram. This would be a visualization of the sampling distribution of \\(b_1\\), which we defined in Subsection ??. Further recall that the standard deviation of the sampling distribution of \\(b_1\\) has a special name: the standard error. Recall that we constructed three sampling distributions for the sample proportion \\(\\widehat{p}\\) using shovels of size 25, 50, and 100 in Figure 9.13. We observed that as the sample size increased, the standard error decreased as evidenced by the narrowing sampling distribution. The standard error of \\(b_1\\) similarly quantifies how much variation in the fitted slope \\(b_1\\) one would expect between different samples. So in our case, we can expect about 0.016 units of variation in the bty_avg slope variable. Recall that the estimate and std_error values play a key role in inferring the value of the unknown population slope \\(\\beta_1\\) relating to all instructors. In Section ??, we’ll perform a simulation using the infer package to construct the bootstrap distribution for \\(b_1\\) in this case. Recall from Subsection ?? that the bootstrap distribution is an approximation to the sampling distribution in that they have a similar shape. Since they have a similar shape, they have similar standard errors. However, unlike the sampling distribution, the bootstrap distribution is constructed from a single sample, which is a practice more aligned with what’s done in real-life. 12.2.2 Test statistic The fourth column of the regression table in Table 12.5 statistic corresponds to a test statistic relating to the following hypothesis test: \\[ \\begin{aligned} H_0 &amp;: \\beta_1 = 0\\\\ \\text{vs } H_A&amp;: \\beta_1 \\neq 0 \\end{aligned} \\] Recall our terminology, notation, and definitions related to hypothesis tests we introduced in Section ??. A hypothesis test consists of a test between two competing hypotheses: 1) a null hypothesis \\(H_0\\) versus 2) an alternative hypothesis \\(H_A\\). A test statistic is a point estimate/sample statistic formula used for hypothesis testing. Here, our null hypothesis \\(H_0\\) assumes that the population slope \\(\\beta_1\\) is 0. If the population slope \\(\\beta_1\\) is truly 0, then this is saying that there is no true relationship between teaching and “beauty” scores for all the instructors in our population. In other words, \\(x\\) = “beauty” score would have no associated effect on \\(y\\) = teaching score. The alternative hypothesis \\(H_A\\), on the other hand, assumes that population slope \\(\\beta_1\\) is not 0, meaning it could be either positive or negative, suggesting either a positive or negative relationship between teaching and “beauty” scores. Recall we called such alternative hypotheses two-sided. By convention, all hypothesis testing for regression assumes two-sided alternatives. Recall our “hypothesized universe” of no gender discrimination we assumed in our promotions activity in Section ??. Similarly here when conducting this hypothesis test, we’ll assume a “hypothesized universe” where there is no relationship between teaching and “beauty” scores. In other words, we’ll assume the null hypothesis \\(H_0: \\beta_1 = 0\\) is true. The statistic column in the regression table is a tricky one however. It corresponds to a standardized t-test statistic, much like the two-sample \\(t\\) statistic we saw in Subsection ?? where we used a theory-based method for conducting hypothesis tests. In both these cases, the null distribution can be mathematically proven to be a \\(t\\)-distribution. Since such test statistics are tricky for individuals new to statistical inference to study, we’ll skip this and jump into interpreting the p-value. If you’re curious however, we’ve included a discussion of this standardized t-test statistic in Subsection ??. 12.2.3 p-value The fifth column of the regression table in Table 12.5 p-value corresponds to the p-value of the hypothesis test \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\). Again recalling our terminology, notation, and definitions related to hypothesis tests we introduced in Section ??, let’s focus on the definition of the p-value: A p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true Recall that you can intuitively think of the p-value as quantifying how “extreme” the observed fitted slope of \\(b_1\\) = 0.067 is in a “hypothesized universe” where is there is no relationship between teaching and “beauty” scores. Following the hypothesis testing procedure we outlined in Section ??, since the p-value in this case is 0, for any choice of significance level \\(\\alpha\\) we would reject \\(H_0\\) in favor of \\(H_A\\). Using non-statistical language, this is saying: we reject the hypothesis that there is no relationship between teaching and “beauty” scores in favor of the hypothesis that that is. In other words, the evidence suggests there is a significant relationship, one that is positive. More precisely however, the p-value corresponds to how extreme the observed test statistic of 4.09 is when compared to the appropriate null distribution. In Section ??, we’ll perform a simulation using the infer package to construct the null distribution in this case. An extra caveat here is that the results of this hypothesis test are only valid if certain “conditions for inference for regression” are met, which we’ll introduce shortly in Section ??. 12.3 More advanced points to consider As this is an introductory book, we have introduced some concepts but not developed them in full detail. This does not mean that there are not things to say about these – more that there is simply too much to say at this time. These topics include: Planning studies: If you are planning a study, you need to determine the sample size \\(n\\) that is sufficient for minimizing Type II error (i.e, maximizing power). To do so, you will need to know the sampling distribution of the estimator not just under the null hypothesis, but also under the alternative hypothesis. You will need to specify the size of the outcome, difference, or relationship you are seeking to understand. From this, for a given Type II error level, you can determine the minimum sample size you will need. Multiple testing: We introduced the use of hypothesis testing here with a single test. When you are analyzing data, however, you often conduct multiple hypothesis tests on the same data. By conducting multiple hypothesis tests, in combination Type I error (what is called “familywise error rate”) is typically higher, and sometimes much higher, than the Type I error of each test in isolation. There are procedures you can use to adjust for this. P-hacking: Again, we have focused on hypothesis testing as if there is a single test and a single null model. But in practice, analysts might try many different specifications or models, with the goal of finding a test that rejects a null hypothesis (p &lt; .05). Why might an analyst do this? Because the system of rewards in science often tips towards rewarding those that find effects (p &lt; .05), not those that do not. "],
["A-appendixA.html", "A Statistical Background A.1 Basic statistical terms A.2 Normal distribution discussion", " A Statistical Background A.1 Basic statistical terms A.1.1 Mean The mean AKA average is the most commonly reported measure of center. It is commonly called the “average” though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \\(n\\) data points, the mean is given by: \\[Mean = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] A.1.2 Median The median is calculated by first sorting a variable’s data from smallest to largest. After sorting the data, the middle element in the list is the median. If the middle falls between two values, then the median is the mean of those two values. A.1.3 Standard deviation We will next discuss the standard deviation of a sample dataset pertaining to one variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far to expect a given data value is from its mean: \\[Standard \\, deviation = \\sqrt{\\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \\cdots + (x_n - Mean)^2}{n - 1}}\\] A.1.4 Five-number summary The five-number summary consists of five values: minimum, first quantile AKA 25th percentile, second quantile AKA median AKA 50th percentile, third quantile AKA 75th, and maximum. The quantiles are calculated as first quantile (\\(Q_1\\)): the median of the first half of the sorted data third quantile (\\(Q_3\\)): the median of the second half of the sorted data The interquartile range is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values is. The five-number summary is not influenced by the presence of outliers in the ways that the mean and standard deviation are. It is, thus, recommended for skewed datasets. A.1.5 Distribution The distribution of a variable/dataset corresponds to generalizing patterns in the dataset. It often shows how frequently elements in the dataset appear. It shows how the data varies and gives some information about where a typical element in the data might fall. Distributions are most easily seen through data visualization. A.1.6 Outliers Outliers correspond to values in the dataset that fall far outside the range of “ordinary” values. In regards to a boxplot (by default), they correspond to values below \\(Q_1 - (1.5 * IQR)\\) or above \\(Q_3 + (1.5 * IQR)\\). Note that these terms (aside from Distribution) only apply to quantitative variables. A.2 Normal distribution discussion "],
["B-appendixB.html", "B Inference Examples Needed packages B.1 Inference mind map B.2 One mean B.3 One proportion B.4 Two proportions B.5 Two means (independent samples) B.6 Two means (paired samples)", " B Inference Examples This appendix is designed to provide you with examples of the five basic hypothesis tests and their corresponding confidence intervals. Traditional theory-based methods as well as computational-based methods are presented. Note: This appendix is still under construction. If you would like to contribute, please check us out on GitHub at https://github.com/moderndive/moderndive_book. Please check out our sneak peak of infer below in the meanwhile. For more details on infer visit https://infer.netlify.com/. include_image(path = &quot;images/sign-2408065_1920.png&quot;, html_opts=&quot;height=100px&quot;, latex_opts = &quot;width=20%&quot;) ![](images/sign-2408065_1920.png){ height=100px } Needed packages library(dplyr) library(ggplot2) library(infer) library(knitr) library(kableExtra) library(readr) library(janitor) B.1 Inference mind map To help you better navigate and choose the appropriate analysis, we’ve created a mind map on http://coggle.it available here and below. FIGURE B.1: Mind map for Inference B.2 One mean B.2.1 Problem statement The National Survey of Family Growth conducted by the Centers for Disease Control gathers information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health. One of the variables collected on this survey is the age at first marriage. 5,534 randomly sampled US women between 2006 and 2010 completed the survey. The women sampled here had been married at least once. Do we have evidence that the mean age of first marriage for all US women from 2006 to 2010 is greater than 23 years? (Tweaked a bit from Diez, Barr, and Çetinkaya-Rundel 2014 [Chapter 4]) B.2.2 Competing hypotheses In words Null hypothesis: The mean age of first marriage for all US women from 2006 to 2010 is equal to 23 years. Alternative hypothesis: The mean age of first marriage for all US women from 2006 to 2010 is greater than 23 years. In symbols (with annotations) \\(H_0: \\mu = \\mu_{0}\\), where \\(\\mu\\) represents the mean age of first marriage for all US women from 2006 to 2010 and \\(\\mu_0\\) is 23. \\(H_A: \\mu &gt; 23\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.2.3 Exploring the sample data age_at_marriage &lt;- read_csv(&quot;https://moderndive.com/data/ageAtMar.csv&quot;) age_summ &lt;- age_at_marriage %&gt;% summarize(sample_size = n(), mean = mean(age), sd = sd(age), minimum = min(age), lower_quartile = quantile(age, 0.25), median = median(age), upper_quartile = quantile(age, 0.75), max = max(age)) kable(age_summ) %&gt;% kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16), latex_options = c(&quot;HOLD_position&quot;)) sample_size mean sd minimum lower_quartile median upper_quartile max 5534 23.4 4.72 10 20 23 26 43 The histogram below also shows the distribution of age. ggplot(data = age_at_marriage, mapping = aes(x = age)) + geom_histogram(binwidth = 3, color = &quot;white&quot;) The observed statistic of interest here is the sample mean: x_bar &lt;- age_at_marriage %&gt;% specify(response = age) %&gt;% calculate(stat = &quot;mean&quot;) x_bar # A tibble: 1 x 1 stat &lt;dbl&gt; 1 23.4 Guess about statistical significance We are looking to see if the observed sample mean of 23.44 is statistically greater than \\(\\mu_0 = 23\\). They seem to be quite close, but we have a large sample size here. Let’s guess that the large sample size will lead us to reject this practically small difference. B.2.4 Non-traditional methods Bootstrapping for hypothesis test In order to look to see if the observed sample mean of 23.44 is statistically greater than \\(\\mu_0 = 23\\), we need to account for the sample size. We also need to determine a process that replicates how the original sample of size 5534 was selected. We can use the idea of bootstrapping to simulate the population from which the sample came and then generate samples from that simulated population to account for sampling variability. Recall how bootstrapping would apply in this context: Sample with replacement from our original sample of 5534 women and repeat this process 10,000 times, calculate the mean for each of the 10,000 bootstrap samples created in Step 1., combine all of these bootstrap statistics calculated in Step 2 into a boot_distn object, and shift the center of this distribution over to the null value of 23. (This is needed since it will be centered at 23.44 via the process of bootstrapping.) set.seed(2018) null_distn_one_mean &lt;- age_at_marriage %&gt;% specify(response = age) %&gt;% hypothesize(null = &quot;point&quot;, mu = 23) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) null_distn_one_mean %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a right-tailed test so we will be looking for values that are greater than or equal to 23.44 for our \\(p\\)-value. null_distn_one_mean %&gt;% visualize(obs_stat = x_bar, direction = &quot;greater&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_one_mean %&gt;% get_pvalue(obs_stat = x_bar, direction = &quot;greater&quot;) pvalue # A tibble: 1 x 1 p_value &lt;dbl&gt; 1 0 So our \\(p\\)-value is 0 and we reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are far into the tail of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\mu\\) using our sample data using bootstrapping. Note that we don’t need to shift this distribution since we want the center of our confidence interval to be our point estimate \\(\\bar{x}_{obs} = 23.44\\). boot_distn_one_mean &lt;- age_at_marriage %&gt;% specify(response = age) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) ci &lt;- boot_distn_one_mean %&gt;% get_ci() ci # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 23.3 23.6 boot_distn_one_mean %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 23 is not contained in this confidence interval as a plausible value of \\(\\mu\\) (the unknown population mean) and the entire interval is larger than 23. This matches with our hypothesis test results of rejecting the null hypothesis in favor of the alternative (\\(\\mu &gt; 23\\)). Interpretation: We are 95% confident the true mean age of first marriage for all US women from 2006 to 2010 is between 23.316 and 23.565. B.2.5 Traditional methods Check conditions Remember that in order to use the shortcut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations are collected independently. The cases are selected independently through random sampling so this condition is met. Approximately normal: The distribution of the response variable should be normal or the sample size should be at least 30. The histogram for the sample above does show some skew. The Q-Q plot below also shows some skew. ggplot(data = age_at_marriage, mapping = aes(sample = age)) + stat_qq() The sample size here is quite large though (\\(n = 5534\\)) so both conditions are met. Test statistic The test statistic is a random variable based on the sample data. Here, we want to look at a way to estimate the population mean \\(\\mu\\). A good guess is the sample mean \\(\\bar{X}\\). Recall that this sample mean is actually a random variable that will vary as different samples are (theoretically, would be) collected. We are looking to see how likely is it for us to have observed a sample mean of \\(\\bar{x}_{obs} = 23.44\\) or larger assuming that the population mean is 23 (assuming the null hypothesis is true). If the conditions are met and assuming \\(H_0\\) is true, we can “standardize” this original test statistic of \\(\\bar{X}\\) into a \\(T\\) statistic that follows a \\(t\\) distribution with degrees of freedom equal to \\(df = n - 1\\): \\[ T =\\dfrac{ \\bar{X} - \\mu_0}{ S / \\sqrt{n} } \\sim t (df = n - 1) \\] where \\(S\\) represents the standard deviation of the sample and \\(n\\) is the sample size. Observed test statistic While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. We can use the t_test() function to perform this analysis for us. t_test_results &lt;- age_at_marriage %&gt;% infer::t_test(formula = age ~ NULL, alternative = &quot;greater&quot;, mu = 23) t_test_results # A tibble: 1 x 6 statistic t_df p_value alternative lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 6.94 5533 2.25e-12 greater 23.3 Inf We see here that the \\(t_{obs}\\) value is 6.936. Compute \\(p\\)-value The \\(p\\)-value—the probability of observing an \\(t_{obs}\\) value of 6.936 or more in our null distribution of a \\(t\\) with 5533 degrees of freedom—is essentially 0. State conclusion We, therefore, have sufficient evidence to reject the null hypothesis. Our initial guess that our observed sample mean was statistically greater than the hypothesized mean has supporting evidence here. Based on this sample, we have evidence that the mean age of first marriage for all US women from 2006 to 2010 is greater than 23 years. Confidence interval t.test(x = age_at_marriage$age, alternative = &quot;two.sided&quot;, mu = 23)$conf [1] 23.3 23.6 attr(,&quot;conf.level&quot;) [1] 0.95 B.2.6 Comparing results Observing the bootstrap distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions also being met (the large sample size was the driver here) leads us to better guess that using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) will lead to similar results. B.3 One proportion B.3.1 Problem statement The CEO of a large electric utility claims that 80 percent of his 1,000,000 customers are satisfied with the service they receive. To test this claim, the local newspaper surveyed 100 customers, using simple random sampling. 73 were satisfied and the remaining were unsatisfied. Based on these findings from the sample, can we reject the CEO’s hypothesis that 80% of the customers are satisfied? [Tweaked a bit from http://stattrek.com/hypothesis-test/proportion.aspx?Tutorial=AP] B.3.2 Competing hypotheses In words Null hypothesis: The proportion of all customers of the large electric utility satisfied with service they receive is equal 0.80. Alternative hypothesis: The proportion of all customers of the large electric utility satisfied with service they receive is different from 0.80. In symbols (with annotations) \\(H_0: \\pi = p_{0}\\), where \\(\\pi\\) represents the proportion of all customers of the large electric utility satisfied with service they receive and \\(p_0\\) is 0.8. \\(H_A: \\pi \\ne 0.8\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.3.3 Exploring the sample data elec &lt;- c(rep(&quot;satisfied&quot;, 73), rep(&quot;unsatisfied&quot;, 27)) %&gt;% as_data_frame() %&gt;% rename(satisfy = value) The bar graph below also shows the distribution of satisfy. ggplot(data = elec, aes(x = satisfy)) + geom_bar() The observed statistic is computed as p_hat &lt;- elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% calculate(stat = &quot;prop&quot;) p_hat # A tibble: 1 x 1 stat &lt;dbl&gt; 1 0.73 Guess about statistical significance We are looking to see if the sample proportion of 0.73 is statistically different from \\(p_0 = 0.8\\) based on this sample. They seem to be quite close, and our sample size is not huge here (\\(n = 100\\)). Let’s guess that we do not have evidence to reject the null hypothesis. B.3.4 Non-traditional methods Simulation for hypothesis test In order to look to see if 0.73 is statistically different from 0.8, we need to account for the sample size. We also need to determine a process that replicates how the original sample of size 100 was selected. We can use the idea of an unfair coin to simulate this process. We will simulate flipping an unfair coin (with probability of success 0.8 matching the null hypothesis) 100 times. Then we will keep track of how many heads come up in those 100 flips. Our simulated statistic matches with how we calculated the original statistic \\(\\hat{p}\\): the number of heads (satisfied) out of our total sample of 100. We then repeat this process many times (say 10,000) to create the null distribution looking at the simulated proportions of successes: set.seed(2018) null_distn_one_prop &lt;- elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = 0.8) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;prop&quot;) null_distn_one_prop %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a two-tailed test so we will be looking for values that are 0.8 - 0.73 = 0.07 away from 0.8 in BOTH directions for our \\(p\\)-value: null_distn_one_prop %&gt;% visualize(obs_stat = p_hat, direction = &quot;both&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_one_prop %&gt;% get_pvalue(obs_stat = p_hat, direction = &quot;both&quot;) pvalue # A tibble: 1 x 1 p_value &lt;dbl&gt; 1 0.114 So our \\(p\\)-value is 0.114 and we fail to reject the null hypothesis at the 5% level. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\pi\\) using our sample data. To do so, we use bootstrapping, which involves sampling with replacement from our original sample of 100 survey respondents and repeating this process 10,000 times, calculating the proportion of successes for each of the 10,000 bootstrap samples created in Step 1., combining all of these bootstrap statistics calculated in Step 2 into a boot_distn object, identifying the 2.5th and 97.5th percentiles of this distribution (corresponding to the 5% significance level chosen) to find a 95% confidence interval for \\(\\pi\\), and interpret this confidence interval in the context of the problem. boot_distn_one_prop &lt;- elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;prop&quot;) Just as we use the mean function for calculating the mean over a numerical variable, we can also use it to compute the proportion of successes for a categorical variable where we specify what we are calling a “success” after the ==. (Think about the formula for calculating a mean and how R handles logical statements such as satisfy == &quot;satisfied&quot; for why this must be true.) ci &lt;- boot_distn_one_prop %&gt;% get_ci() ci # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 0.64 0.81 boot_distn_one_prop %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0.80 is contained in this confidence interval as a plausible value of \\(\\pi\\) (the unknown population proportion). This matches with our hypothesis test results of failing to reject the null hypothesis. Interpretation: We are 95% confident the true proportion of customers who are satisfied with the service they receive is between 0.64 and 0.81. B.3.5 Traditional methods Check conditions Remember that in order to use the shortcut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations are collected independently. The cases are selected independently through random sampling so this condition is met. Approximately normal: The number of expected successes and expected failures is at least 10. This condition is met since 73 and 27 are both greater than 10. Test statistic The test statistic is a random variable based on the sample data. Here, we want to look at a way to estimate the population proportion \\(\\pi\\). A good guess is the sample proportion \\(\\hat{P}\\). Recall that this sample proportion is actually a random variable that will vary as different samples are (theoretically, would be) collected. We are looking to see how likely is it for us to have observed a sample proportion of \\(\\hat{p}_{obs} = 0.73\\) or larger assuming that the population proportion is 0.80 (assuming the null hypothesis is true). If the conditions are met and assuming \\(H_0\\) is true, we can standardize this original test statistic of \\(\\hat{P}\\) into a \\(Z\\) statistic that follows a \\(N(0, 1)\\) distribution. \\[ Z =\\dfrac{ \\hat{P} - p_0}{\\sqrt{\\dfrac{p_0(1 - p_0)}{n} }} \\sim N(0, 1) \\] Observed test statistic While one could compute this observed test statistic by “hand” by plugging the observed values into the formula, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. The calculation has been done in R below for completeness though: p_hat &lt;- 0.73 p0 &lt;- 0.8 n &lt;- 100 (z_obs &lt;- (p_hat - p0) / sqrt( (p0 * (1 - p0)) / n)) [1] -1.75 We see here that the \\(z_{obs}\\) value is around -1.75. Our observed sample proportion of 0.73 is 1.75 standard errors below the hypothesized parameter value of 0.8. Visualize and compute \\(p\\)-value elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = 0.8) %&gt;% calculate(stat = &quot;z&quot;) %&gt;% visualize(method = &quot;theoretical&quot;, obs_stat = z_obs, direction = &quot;both&quot;) 2 * pnorm(z_obs) [1] 0.0801 The \\(p\\)-value—the probability of observing an \\(z_{obs}\\) value of -1.75 or more extreme (in both directions) in our null distribution—is around 8%. Note that we could also do this test directly using the prop.test function. stats::prop.test(x = table(elec$satisfy), n = length(elec$satisfy), alternative = &quot;two.sided&quot;, p = 0.8, correct = FALSE) 1-sample proportions test without continuity correction data: table(elec$satisfy), null probability 0.8 X-squared = 3, df = 1, p-value = 0.08 alternative hypothesis: true p is not equal to 0.8 95 percent confidence interval: 0.636 0.807 sample estimates: p 0.73 prop.test does a \\(\\chi^2\\) test here but this matches up exactly with what we would expect: \\(x^2_{obs} = 3.06 = (-1.75)^2 = (z_{obs})^2\\) and the \\(p\\)-values are the same because we are focusing on a two-tailed test. Note that the 95 percent confidence interval given above matches well with the one calculated using bootstrapping. State conclusion We, therefore, do not have sufficient evidence to reject the null hypothesis. Our initial guess that our observed sample proportion was not statistically greater than the hypothesized proportion has not been invalidated. Based on this sample, we have do not evidence that the proportion of all customers of the large electric utility satisfied with service they receive is different from 0.80, at the 5% level. B.3.6 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions also being met leads us to better guess that using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) will lead to similar results. B.4 Two proportions B.4.1 Problem statement A 2010 survey asked 827 randomly sampled registered voters in California “Do you support? Or do you oppose? Drilling for oil and natural gas off the Coast of California? Or do you not know enough to say?” Conduct a hypothesis test to determine if the data provide strong evidence that the proportion of college graduates who do not have an opinion on this issue is different than that of non-college graduates. (Tweaked a bit from Diez, Barr, and Çetinkaya-Rundel 2014 [Chapter 6]) B.4.2 Competing hypotheses In words Null hypothesis: There is no association between having an opinion on drilling and having a college degree for all registered California voters in 2010. Alternative hypothesis: There is an association between having an opinion on drilling and having a college degree for all registered California voters in 2010. Another way in words Null hypothesis: The probability that a Californian voter in 2010 having no opinion on drilling and is a college graduate is the same as that of a non-college graduate. Alternative hypothesis: These parameter probabilities are different. In symbols (with annotations) \\(H_0: \\pi_{college} = \\pi_{no\\_college}\\) or \\(H_0: \\pi_{college} - \\pi_{no\\_college} = 0\\), where \\(\\pi\\) represents the probability of not having an opinion on drilling. \\(H_A: \\pi_{college} - \\pi_{no\\_college} \\ne 0\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.4.3 Exploring the sample data offshore &lt;- read_csv(&quot;https://moderndive.com/data/offshore.csv&quot;) offshore %&gt;% tabyl(college_grad, response) college_grad no opinion opinion no 131 258 yes 104 334 off_summ &lt;- offshore %&gt;% group_by(college_grad) %&gt;% summarize(prop_no_opinion = mean(response == &quot;no opinion&quot;), sample_size = n()) ggplot(offshore, aes(x = college_grad, fill = response)) + geom_bar(position = &quot;fill&quot;) + coord_flip() Guess about statistical significance We are looking to see if a difference exists in the size of the bars corresponding to no opinion for the plot. Based solely on the plot, we have little reason to believe that a difference exists since the bars seem to be about the same size, BUT…it’s important to use statistics to see if that difference is actually statistically significant! B.4.4 Non-traditional methods Collecting summary info The observed statistic is d_hat &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) d_hat # A tibble: 1 x 1 stat &lt;dbl&gt; 1 -0.0993 Randomization for hypothesis test In order to look to see if the observed sample proportion of no opinion for college graduates of 0.337 is statistically different than that for graduates of 0.237, we need to account for the sample sizes. Note that this is the same as looking to see if \\(\\hat{p}_{grad} - \\hat{p}_{nograd}\\) is statistically different than 0. We also need to determine a process that replicates how the original group sizes of 389 and 438 were selected. We can use the idea of randomization testing (also known as permutation testing) to simulate the population from which the sample came (with two groups of different sizes) and then generate samples using shuffling from that simulated population to account for sampling variability. set.seed(2018) null_distn_two_props &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) null_distn_two_props %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a two-tailed test so we will be looking for values that are greater than or equal to -0.099 or less than or equal to 0.099 for our \\(p\\)-value. null_distn_two_props %&gt;% visualize(obs_stat = d_hat, direction = &quot;two_sided&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_two_props %&gt;% get_pvalue(obs_stat = d_hat, direction = &quot;two_sided&quot;) pvalue # A tibble: 1 x 1 p_value &lt;dbl&gt; 1 0.003 So our \\(p\\)-value is 0.003 and we reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are far into the tails of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\pi_{college} - \\pi_{no\\_college}\\) using our sample data with bootstrapping. boot_distn_two_props &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) ci &lt;- boot_distn_two_props %&gt;% get_ci() ci # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 -0.161 -0.0378 boot_distn_two_props %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0 is not contained in this confidence interval as a plausible value of \\(\\pi_{college} - \\pi_{no\\_college}\\) (the unknown population parameter). This matches with our hypothesis test results of rejecting the null hypothesis. Since zero is not a plausible value of the population parameter, we have evidence that the proportion of college graduates in California with no opinion on drilling is different than that of non-college graduates. Interpretation: We are 95% confident the true proportion of non-college graduates with no opinion on offshore drilling in California is between 0.16 dollars smaller to 0.04 dollars smaller than for college graduates. B.4.5 Traditional methods B.4.6 Check conditions Remember that in order to use the short-cut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: Each case that was selected must be independent of all the other cases selected. This condition is met since cases were selected at random to observe. Sample size: The number of pooled successes and pooled failures must be at least 10 for each group. We need to first figure out the pooled success rate: \\[\\hat{p}_{obs} = \\dfrac{131 + 104}{827} = 0.28.\\] We now determine expected (pooled) success and failure counts: \\(0.28 \\cdot (131 + 258) = 108.92\\), \\(0.72 \\cdot (131 + 258) = 280.08\\) \\(0.28 \\cdot (104 + 334) = 122.64\\), \\(0.72 \\cdot (104 + 334) = 315.36\\) Independent selection of samples: The cases are not paired in any meaningful way. We have no reason to suspect that a college graduate selected would have any relationship to a non-college graduate selected. B.4.7 Test statistic The test statistic is a random variable based on the sample data. Here, we are interested in seeing if our observed difference in sample proportions corresponding to no opinion on drilling (\\(\\hat{p}_{college, obs} - \\hat{p}_{no\\_college, obs}\\) = 0.033) is statistically different than 0. Assuming that conditions are met and the null hypothesis is true, we can use the standard normal distribution to standardize the difference in sample proportions (\\(\\hat{P}_{college} - \\hat{P}_{no\\_college}\\)) using the standard error of \\(\\hat{P}_{college} - \\hat{P}_{no\\_college}\\) and the pooled estimate: \\[ Z =\\dfrac{ (\\hat{P}_1 - \\hat{P}_2) - 0}{\\sqrt{\\dfrac{\\hat{P}(1 - \\hat{P})}{n_1} + \\dfrac{\\hat{P}(1 - \\hat{P})}{n_2} }} \\sim N(0, 1) \\] where \\(\\hat{P} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}}.\\) Observed test statistic While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. We can use the prop.test function to perform this analysis for us. z_hat &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% calculate(stat = &quot;z&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) z_hat # A tibble: 1 x 1 stat &lt;dbl&gt; 1 -3.16 The observed difference in sample proportions is 3.16 standard deviations smaller than 0. The \\(p\\)-value—the probability of observing a \\(Z\\) value of -3.16 or more extreme in our null distribution—is 0.0016. This can also be calculated in R directly: 2 * pnorm(-3.16, lower.tail = TRUE) [1] 0.00158 B.4.8 State conclusion We, therefore, have sufficient evidence to reject the null hypothesis. Our initial guess that a statistically significant difference did not exist in the proportions of no opinion on offshore drilling between college educated and non-college educated Californians was not validated. We do have evidence to suggest that there is a dependency between college graduation and position on offshore drilling for Californians. B.4.9 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions were not met since the number of pairs was small, but the sample data was not highly skewed. Using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) lead to similar results. B.5 Two means (independent samples) B.5.1 Problem statement Average income varies from one region of the country to another, and it often reflects both lifestyles and regional living expenses. Suppose a new graduate is considering a job in two locations, Cleveland, OH and Sacramento, CA, and he wants to see whether the average income in one of these cities is higher than the other. He would like to conduct a hypothesis test based on two randomly selected samples from the 2000 Census. (Tweaked a bit from Diez, Barr, and Çetinkaya-Rundel 2014 [Chapter 5]) B.5.2 Competing hypotheses In words Null hypothesis: There is no association between income and location (Cleveland, OH and Sacramento, CA). Alternative hypothesis: There is an association between income and location (Cleveland, OH and Sacramento, CA). Another way in words Null hypothesis: The mean income is the same for both cities. Alternative hypothesis: The mean income is different for the two cities. In symbols (with annotations) \\(H_0: \\mu_{sac} = \\mu_{cle}\\) or \\(H_0: \\mu_{sac} - \\mu_{cle} = 0\\), where \\(\\mu\\) represents the average income. \\(H_A: \\mu_{sac} - \\mu_{cle} \\ne 0\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.5.3 Exploring the sample data cle_sac &lt;- read.delim(&quot;https://moderndive.com/data/cleSac.txt&quot;) %&gt;% rename(metro_area = Metropolitan_area_Detailed, income = Total_personal_income) %&gt;% na.omit() inc_summ &lt;- cle_sac %&gt;% group_by(metro_area) %&gt;% summarize(sample_size = n(), mean = mean(income), sd = sd(income), minimum = min(income), lower_quartile = quantile(income, 0.25), median = median(income), upper_quartile = quantile(income, 0.75), max = max(income)) kable(inc_summ) %&gt;% kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16), latex_options = c(&quot;HOLD_position&quot;)) metro_area sample_size mean sd minimum lower_quartile median upper_quartile max Cleveland_ OH 212 27467 27681 0 8475 21000 35275 152400 Sacramento_ CA 175 32428 35774 0 8050 20000 49350 206900 The boxplot below also shows the mean for each group highlighted by the red dots. ggplot(cle_sac, aes(x = metro_area, y = income)) + geom_boxplot() + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, color = &quot;red&quot;) Guess about statistical significance We are looking to see if a difference exists in the mean income of the two levels of the explanatory variable. Based solely on the boxplot, we have reason to believe that no difference exists. The distributions of income seem similar and the means fall in roughly the same place. B.5.4 Non-traditional methods Collecting summary info We now compute the observed statistic: d_hat &lt;- cle_sac %&gt;% specify(income ~ metro_area) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Sacramento_ CA&quot;, &quot;Cleveland_ OH&quot;)) d_hat # A tibble: 1 x 1 stat &lt;dbl&gt; 1 4960. Randomization for hypothesis test In order to look to see if the observed sample mean for Sacramento of 27467.066 is statistically different than that for Cleveland of 32427.543, we need to account for the sample sizes. Note that this is the same as looking to see if \\(\\bar{x}_{sac} - \\bar{x}_{cle}\\) is statistically different than 0. We also need to determine a process that replicates how the original group sizes of 212 and 175 were selected. We can use the idea of randomization testing (also known as permutation testing) to simulate the population from which the sample came (with two groups of different sizes) and then generate samples using shuffling from that simulated population to account for sampling variability. set.seed(2018) null_distn_two_means &lt;- cle_sac %&gt;% specify(income ~ metro_area) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Sacramento_ CA&quot;, &quot;Cleveland_ OH&quot;)) null_distn_two_means %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a two-tailed test so we will be looking for values that are greater than or equal to 4960.477 or less than or equal to -4960.477 for our \\(p\\)-value. null_distn_two_means %&gt;% visualize(obs_stat = d_hat, direction = &quot;both&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_two_means %&gt;% get_pvalue(obs_stat = d_hat, direction = &quot;both&quot;) pvalue # A tibble: 1 x 1 p_value &lt;dbl&gt; 1 0.130 So our \\(p\\)-value is 0.13 and we fail to reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are not very far into the tail of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\mu_{sac} - \\mu_{cle}\\) using our sample data with bootstrapping. Here we will bootstrap each of the groups with replacement instead of shuffling. This is done using the groups argument in the resample function to fix the size of each group to be the same as the original group sizes of 175 for Sacramento and 212 for Cleveland. boot_distn_two_means &lt;- cle_sac %&gt;% specify(income ~ metro_area) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Sacramento_ CA&quot;, &quot;Cleveland_ OH&quot;)) ci &lt;- boot_distn_two_means %&gt;% get_ci() ci # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 -1446. 11308. boot_distn_two_means %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0 is contained in this confidence interval as a plausible value of \\(\\mu_{sac} - \\mu_{cle}\\) (the unknown population parameter). This matches with our hypothesis test results of failing to reject the null hypothesis. Since zero is a plausible value of the population parameter, we do not have evidence that Sacramento incomes are different than Cleveland incomes. Interpretation: We are 95% confident the true mean yearly income for those living in Sacramento is between 1445.53 dollars smaller to 11307.82 dollars higher than for Cleveland. Note: You could also use the null distribution based on randomization with a shift to have its center at \\(\\bar{x}_{sac} - \\bar{x}_{cle} = \\$4960.48\\) instead of at 0 and calculate its percentiles. The confidence interval produced via this method should be comparable to the one done using bootstrapping above. B.5.5 Traditional methods Check conditions Remember that in order to use the short-cut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations are independent in both groups. This metro_area variable is met since the cases are randomly selected from each city. Approximately normal: The distribution of the response for each group should be normal or the sample sizes should be at least 30. ggplot(cle_sac, aes(x = income)) + geom_histogram(color = &quot;white&quot;, binwidth = 20000) + facet_wrap(~ metro_area) We have some reason to doubt the normality assumption here since both the histograms show deviation from a normal model fitting the data well for each group. The sample sizes for each group are greater than 100 though so the assumptions should still apply. Independent samples: The samples should be collected without any natural pairing. There is no mention of there being a relationship between those selected in Cleveland and in Sacramento. B.5.6 Test statistic The test statistic is a random variable based on the sample data. Here, we are interested in seeing if our observed difference in sample means (\\(\\bar{x}_{sac, obs} - \\bar{x}_{cle, obs}\\) = 4960.477) is statistically different than 0. Assuming that conditions are met and the null hypothesis is true, we can use the \\(t\\) distribution to standardize the difference in sample means (\\(\\bar{X}_{sac} - \\bar{X}_{cle}\\)) using the approximate standard error of \\(\\bar{X}_{sac} - \\bar{X}_{cle}\\) (invoking \\(S_{sac}\\) and \\(S_{cle}\\) as estimates of unknown \\(\\sigma_{sac}\\) and \\(\\sigma_{cle}\\)). \\[ T =\\dfrac{ (\\bar{X}_1 - \\bar{X}_2) - 0}{ \\sqrt{\\dfrac{S_1^2}{n_1} + \\dfrac{S_2^2}{n_2}} } \\sim t (df = min(n_1 - 1, n_2 - 1)) \\] where 1 = Sacramento and 2 = Cleveland with \\(S_1^2\\) and \\(S_2^2\\) the sample variance of the incomes of both cities, respectively, and \\(n_1 = 175\\) for Sacramento and \\(n_2 = 212\\) for Cleveland. Observed test statistic Note that we could also do (ALMOST) this test directly using the t.test function. The x and y arguments are expected to both be numeric vectors here so we’ll need to appropriately filter our datasets. cle_sac %&gt;% specify(income ~ metro_area) %&gt;% calculate(stat = &quot;t&quot;, order = c(&quot;Cleveland_ OH&quot;, &quot;Sacramento_ CA&quot;)) # A tibble: 1 x 1 stat &lt;dbl&gt; 1 -1.50 We see here that the observed test statistic value is around -1.5. While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. B.5.7 Compute \\(p\\)-value The \\(p\\)-value—the probability of observing an \\(t_{174}\\) value of -1.501 or more extreme (in both directions) in our null distribution—is 0.13. This can also be calculated in R directly: 2 * pt(-1.501, df = min(212 - 1, 175 - 1), lower.tail = TRUE) [1] 0.135 We can also approximate by using the standard normal curve: 2 * pnorm(-1.501) [1] 0.133 Note that the 95 percent confidence interval given above matches well with the one calculated using bootstrapping. B.5.8 State conclusion We, therefore, do not have sufficient evidence to reject the null hypothesis. Our initial guess that a statistically significant difference not existing in the means was backed by this statistical analysis. We do not have evidence to suggest that the true mean income differs between Cleveland, OH and Sacramento, CA based on this data. B.5.9 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions also being met leads us to better guess that using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) will lead to similar results. B.6 Two means (paired samples) Problem statement Trace metals in drinking water affect the flavor and an unusually high concentration can pose a health hazard. Ten pairs of data were taken measuring zinc concentration in bottom water and surface water at 10 randomly selected locations on a stretch of river. Do the data suggest that the true average concentration in the surface water is smaller than that of bottom water? (Note that units are not given.) [Tweaked a bit from https://onlinecourses.science.psu.edu/stat500/node/51] B.6.1 Competing hypotheses In words Null hypothesis: The mean concentration in the bottom water is the same as that of the surface water at different paired locations. Alternative hypothesis: The mean concentration in the surface water is smaller than that of the bottom water at different paired locations. In symbols (with annotations) \\(H_0: \\mu_{diff} = 0\\), where \\(\\mu_{diff}\\) represents the mean difference in concentration for surface water minus bottom water. \\(H_A: \\mu_{diff} &lt; 0\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.6.2 Exploring the sample data zinc_tidy &lt;- read_csv(&quot;https://moderndive.com/data/zinc_tidy.csv&quot;) We want to look at the differences in surface - bottom for each location: zinc_diff &lt;- zinc_tidy %&gt;% group_by(loc_id) %&gt;% summarize(pair_diff = diff(concentration)) %&gt;% ungroup() Next we calculate the mean difference as our observed statistic: d_hat &lt;- zinc_diff %&gt;% specify(response = pair_diff) %&gt;% calculate(stat = &quot;mean&quot;) d_hat # A tibble: 1 x 1 stat &lt;dbl&gt; 1 -0.0804 The histogram below also shows the distribution of pair_diff. ggplot(zinc_diff, aes(x = pair_diff)) + geom_histogram(binwidth = 0.04, color = &quot;white&quot;) Guess about statistical significance We are looking to see if the sample paired mean difference of -0.08 is statistically less than 0. They seem to be quite close, but we have a small number of pairs here. Let’s guess that we will fail to reject the null hypothesis. B.6.3 Non-traditional methods Bootstrapping for hypothesis test In order to look to see if the observed sample mean difference \\(\\bar{x}_{diff} = 4960.477\\) is statistically less than 0, we need to account for the number of pairs. We also need to determine a process that replicates how the paired data was selected in a way similar to how we calculated our original difference in sample means. Treating the differences as our data of interest, we next use the process of bootstrapping to build other simulated samples and then calculate the mean of the bootstrap samples. We hypothesize that the mean difference is zero. This process is similar to comparing the One Mean example seen above, but using the differences between the two groups as a single sample with a hypothesized mean difference of 0. set.seed(2018) null_distn_paired_means &lt;- zinc_diff %&gt;% specify(response = pair_diff) %&gt;% hypothesize(null = &quot;point&quot;, mu = 0) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) null_distn_paired_means %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a left-tailed test so we will be looking for values that are less than or equal to 4960.477 for our \\(p\\)-value. null_distn_paired_means %&gt;% visualize(obs_stat = d_hat, direction = &quot;less&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_paired_means %&gt;% get_pvalue(obs_stat = d_hat, direction = &quot;less&quot;) pvalue # A tibble: 1 x 1 p_value &lt;dbl&gt; 1 0 So our \\(p\\)-value is essentially 0 and we reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are far into the left tail of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\mu_{diff}\\) using our sample data (the calculated differences) with bootstrapping. This is similar to the bootstrapping done in a one sample mean case, except now our data is differences instead of raw numerical data. Note that this code is identical to the pipeline shown in the hypothesis test above except the hypothesize() function is not called. boot_distn_paired_means &lt;- zinc_diff %&gt;% specify(response = pair_diff) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) ci &lt;- boot_distn_paired_means %&gt;% get_ci() ci # A tibble: 1 x 2 `2.5%` `97.5%` &lt;dbl&gt; &lt;dbl&gt; 1 -0.112 -0.0503 boot_distn_paired_means %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0 is not contained in this confidence interval as a plausible value of \\(\\mu_{diff}\\) (the unknown population parameter). This matches with our hypothesis test results of rejecting the null hypothesis. Since zero is not a plausible value of the population parameter and since the entire confidence interval falls below zero, we have evidence that surface zinc concentration levels are lower, on average, than bottom level zinc concentrations. Interpretation: We are 95% confident the true mean zinc concentration on the surface is between 0.11 units smaller to 0.05 units smaller than on the bottom. B.6.4 Traditional methods Check conditions Remember that in order to use the shortcut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations among pairs are independent. The locations are selected independently through random sampling so this condition is met. Approximately normal: The distribution of population of differences is normal or the number of pairs is at least 30. The histogram above does show some skew so we have reason to doubt the population being normal based on this sample. We also only have 10 pairs which is fewer than the 30 needed. A theory-based test may not be valid here. Test statistic The test statistic is a random variable based on the sample data. Here, we want to look at a way to estimate the population mean difference \\(\\mu_{diff}\\). A good guess is the sample mean difference \\(\\bar{X}_{diff}\\). Recall that this sample mean is actually a random variable that will vary as different samples are (theoretically, would be) collected. We are looking to see how likely is it for us to have observed a sample mean of \\(\\bar{x}_{diff, obs} = 0.0804\\) or larger assuming that the population mean difference is 0 (assuming the null hypothesis is true). If the conditions are met and assuming \\(H_0\\) is true, we can “standardize” this original test statistic of \\(\\bar{X}_{diff}\\) into a \\(T\\) statistic that follows a \\(t\\) distribution with degrees of freedom equal to \\(df = n - 1\\): \\[ T =\\dfrac{ \\bar{X}_{diff} - 0}{ S_{diff} / \\sqrt{n} } \\sim t (df = n - 1) \\] where \\(S\\) represents the standard deviation of the sample differences and \\(n\\) is the number of pairs. Observed test statistic While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. We can use the t_test function on the differences to perform this analysis for us. t_test_results &lt;- zinc_diff %&gt;% infer::t_test(formula = pair_diff ~ NULL, alternative = &quot;less&quot;, mu = 0) t_test_results # A tibble: 1 x 6 statistic t_df p_value alternative lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 -4.86 9 0.000446 less -Inf -0.0501 We see here that the \\(t_{obs}\\) value is -4.864. Compute \\(p\\)-value The \\(p\\)-value—the probability of observing a \\(t_{obs}\\) value of -4.864 or less in our null distribution of a \\(t\\) with 9 degrees of freedom—is 0. This can also be calculated in R directly: pt(-4.8638, df = nrow(zinc_diff) - 1, lower.tail = TRUE) [1] 0.000446 State conclusion We, therefore, have sufficient evidence to reject the null hypothesis. Our initial guess that our observed sample mean difference was not statistically less than the hypothesized mean of 0 has been invalidated here. Based on this sample, we have evidence that the mean concentration in the bottom water is greater than that of the surface water at different paired locations. B.6.5 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions were not met since the number of pairs was small, but the sample data was not highly skewed. Using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) lead to similar results here. References "],
["C-appendixC.html", "C Reach for the Stars Needed packages C.1 Sorted barplots C.2 Interactive graphics", " C Reach for the Stars Needed packages library(dplyr) library(ggplot2) library(knitr) library(dygraphs) library(nycflights13) C.1 Sorted barplots Building upon the example in Section 3.8: flights_table &lt;- table(flights$carrier) flights_table 9E AA AS B6 DL EV F9 FL HA MQ OO UA US 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 58665 20536 VX WN YV 5162 12275 601 We can sort this table from highest to lowest counts by using the sort function: sorted_flights &lt;- sort(flights_table, decreasing = TRUE) names(sorted_flights) [1] &quot;UA&quot; &quot;B6&quot; &quot;EV&quot; &quot;DL&quot; &quot;AA&quot; &quot;MQ&quot; &quot;US&quot; &quot;9E&quot; &quot;WN&quot; &quot;VX&quot; &quot;FL&quot; &quot;AS&quot; &quot;F9&quot; &quot;YV&quot; &quot;HA&quot; [16] &quot;OO&quot; It is often preferred for barplots to be ordered corresponding to the heights of the bars. This allows the reader to more easily compare the ordering of different airlines in terms of departed flights (Robbins 2013). We can also much more easily answer questions like “How many airlines have more departing flights than Southwest Airlines?”. We can use the sorted table giving the number of flights defined as sorted_flights to reorder the carrier. ggplot(data = flights, mapping = aes(x = carrier)) + geom_bar() + scale_x_discrete(limits = names(sorted_flights)) FIGURE C.1: Number of flights departing NYC in 2013 by airline - Descending numbers The last addition here specifies the values of the horizontal x axis on a discrete scale to correspond to those given by the entries of sorted_flights. C.2 Interactive graphics C.2.1 Interactive linegraphs Another useful tool for viewing linegraphs such as this is the dygraph function in the dygraphs package in combination with the dyRangeSelector function. This allows us to zoom in on a selected range and get an interactive plot for us to work with: library(dygraphs) flights_day &lt;- mutate(flights, date = as.Date(time_hour)) flights_summarized &lt;- flights_day %&gt;% group_by(date) %&gt;% summarize(median_arr_delay = median(arr_delay, na.rm = TRUE)) rownames(flights_summarized) &lt;- flights_summarized$date flights_summarized &lt;- select(flights_summarized, -date) dyRangeSelector(dygraph(flights_summarized)) The syntax here is a little different than what we have covered so far. The dygraph function is expecting for the dates to be given as the rownames of the object. We then remove the date variable from the flights_summarized data frame since it is accounted for in the rownames. Lastly, we run the dygraph function on the new data frame that only contains the median arrival delay as a column and then provide the ability to have a selector to zoom in on the interactive plot via dyRangeSelector. (Note that this plot will only be interactive in the HTML version of this book.) References "],
["D-appendixD.html", "D Learning Check Solutions D.1 Chapter 2 Solutions D.2 Chapter 3 Solutions D.3 Chapter 4 Solutions D.4 Chapter 5 Solutions D.5 Chapter 6 Solutions", " D Learning Check Solutions D.1 Chapter 2 Solutions library(dplyr) library(ggplot2) library(nycflights13) (LC2.1) Repeat the above installing steps, but for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for writing reports in R. (LC2.2) “Load” the dplyr, nycflights13, and knitr packages as well by repeating the above steps. Solution: If the following code runs with no errors, you’ve succeeded! library(dplyr) library(nycflights13) library(knitr) (LC2.3) What does any ONE row in this flights dataset refer to? A. Data on an airline B. Data on a flight C. Data on an airport D. Data on multiple flights Solution: This is data on a flight. Not a flight path! Example: a flight path would be United 1545 to Houston a flight would be United 1545 to Houston at a specific date/time. For example: 2013/1/1 at 5:15am. (LC2.4) What are some examples in this dataset of categorical variables? What makes them different than quantitative variables? Solution: Hint: Type ?flights in the console to see what all the variables mean! Categorical: carrier the company dest the destination flight the flight number. Even though this is a number, its simply a label. Example United 1545 is not less than United 1714 Quantitative: distance the distance in miles time_hour time (LC2.5) What does int, dbl, and chr mean in the output above? Solution: int: integer. Used to count things i.e. a discrete value. Ex: the # of cars parked in a lot dbl: double. Used to measure things. i.e. a continuous value. Ex: your height in inches chr: character. i.e. text (LC2.6) What properties of the observational unit do each of lat, lon, alt, tz, dst, and tzone describe for the airports data frame? Note that you may want to use ?airports to get more information. Solution: lat long represent the airport geographic coordinates, alt is the altitude above sea level of the airport (Run airports %&gt;% filter(faa == &quot;DEN&quot;) to see the altitude of Denver International Airport), tz is the time zone difference with respect to GMT in London UK, dst is the daylight savings time zone, and tzone is the time zone label. (LC2.7) Provide the names of variables in a data frame with at least three variables in which one of them is an identification variable and the other two are not. In other words, create your own tidy dataset that matches these conditions. Solution: In the weather example in LC3.8, the combination of origin, year, month, day, hour are identification variables as they identify the observation in question. Anything else pertains to observations: temp, humid, wind_speed, etc. D.2 Chapter 3 Solutions library(nycflights13) library(ggplot2) library(dplyr) (LC3.1) Take a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights) in the console. In what respect do these data frames differ? Solution: flights contains all flight data, while alaska_flights contains only data from Alaskan carrier “AS”. We can see that flights has 336776 rows while alaska_flights has only 714 (LC3.2) What are some practical reasons why dep_delay and arr_delay have a positive relationship? Solution: The later a plane departs, typically the later it will arrive. (LC3.3) What variables (not necessarily in the flights data frame) would you expect to have a negative correlation (i.e. a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here. Solution: An example in the weather dataset is visibility, which measure visibility in miles. As visibility increases, we would expect departure delays to decrease. (LC3.4) Why do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaskan flights? Solution: The point (0,0) means no delay in departure nor arrival. From the point of view of Alaska airlines, this means the flight was on time. It seems most flights are at least close to being on time. (LC3.5) What are some other features of the plot that stand out to you? Solution: Different people will answer this one differently. One answer is most flights depart and arrive less than an hour late. (LC3.6) Create a new scatterplot using different variables in the alaska_flights data frame by modifying the example above. Solution: Many possibilities for this one, see the plot below. Is there a pattern in departure delay depending on when the flight is scheduled to depart? Interestingly, there seems to be only two blocks of time where flights depart. ggplot(data = alaska_flights, mapping = aes(x = dep_time, y = dep_delay)) + geom_point() (LC3.7) Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? Solution: Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? It thins out the points so we address overplotting. But more importantly it hints at the (statistical) density and distribution of the points: where are the points concentrated, where do they occur. We will see more about densities and distributions in Chapter 6 when we switch gears to statistical topics. (LC3.8) After viewing the Figure 3.4 above, give an approximate range of arrival delays and departure delays that occur the most frequently. How has that region changed compared to when you observed the same plot without the alpha = 0.2 set in Figure 3.2? Solution: After viewing the Figure 3.4 above, give a range of arrival delays and departure delays that occur most frequently? How has that region changed compared to when you observed the same plot without the alpha = 0.2 set in Figure 3.2? The lower plot suggests that most Alaska flights from NYC depart between 12 minutes early and on time and arrive between 50 minutes early and on time. (LC3.9) Take a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather) in the console. In what respect do these data frames differ? Solution: Take a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather) in the console. In what respect do these data frames differ? The rows of early_january_weather are a subset of weather. (LC3.10) View() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement whereas the hour variable does not? Solution: View() the flights data frame again. Why does the time_hour variable correctly identify the hour of the measurement whereas the hour variable does not? Because to uniquely identify an hour, we need the year/month/day/hour sequence, whereas there are only 24 possible hour’s. (LC3.11) Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis? Solution: Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis? Because lines suggest connectedness and ordering. (LC3.12) Why are linegraphs frequently used when time is the explanatory variable? Solution: Why are linegraphs frequently used when time is the explanatory variable? Because time is sequential: subsequent observations are closely related to each other. (LC3.13) Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013. Solution: Plot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013. Humidity is a good one to look at, since this very closely related to the cycles of a day. ggplot(data = early_january_weather, mapping = aes(x = time_hour, y = humid)) + geom_line() (LC3.14) What does changing the number of bins from 30 to 40 tell us about the distribution of temperatures? Solution: The distribution doesn’t change much. But by refining the bin width, we see that the temperature data has a high degree of accuracy. What do I mean by accuracy? Looking at the temp variabile by View(weather), we see that the precision of each temperature recording is 2 decimal places. (LC3.15) Would you classify the distribution of temperatures as symmetric or skewed? Solution: It is rather symmetric, i.e. there are no long tails on only one side of the distribution (LC3.16) What would you guess is the “center” value in this distribution? Why did you make that choice? Solution: The center is around 55.26°F. By running the summary() command, we see that the mean and median are very similar. In fact, when the distribution is symmetric the mean equals the median. (LC3.17) Is this data spread out greatly from the center or is it close? Why? Solution: This can only be answered relatively speaking! Let’s pick things to be relative to Seattle, WA temperatures: While, it appears that Seattle weather has a similar center of 55°F, its temperatures are almost entirely between 35°F and 75°F for a range of about 40°F. Seattle temperatures are much less spread out than New York i.e. much more consistent over the year. New York on the other hand has much colder days in the winter and much hotter days in the summer. Expressed differently, the middle 50% of values, as delineated by the interquartile range is 30°F: (LC3.18) What other things do you notice about the faceted plot above? How does a faceted plot help us see relationships between two variables? Solution: Certain months have much more consistent weather (August in particular), while others have crazy variability like January and October, representing changes in the seasons. Because we see temp recordings split by month, we are considering the relationship between these two variables. For example, for example for summer months, temperatures tend to be higher. (LC3.19) What do the numbers 1-12 correspond to in the plot above? What about 25, 50, 75, 100? Solution: While month is technically a number between 1-12, we’re viewing it as a categorical variable here. Specifically an ordinal categorical variable since there is a ordering to the categories 25, 50, 75, 100 are temperatures (LC3.20) For which types of data-sets would these types of faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics. Solution: We’d have 365 facets to look at. Way too many. We don’t really care about day-to-day fluctuation in weather so much, but maybe more week-to-week variation. We’d like to focus on seasonal trends. (LC3.21) Does the temp variable in the weather data-set have a lot of variability? Why do you say that? Solution: Again, like in LC (LC3.17), this is a relative question. I would say yes, because in New York City, you have 4 clear seasons with different weather. Whereas in Seattle WA and Portland OR, you have two seasons: summer and rain! (LC3.22) What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point. Solution: It appears to be an outlier. Let’s revisit the use of the filter command to hone in on it. We want all data points where the month is 5 and temp&lt;25 weather %&gt;% filter(month==5 &amp; temp &lt; 25) # A tibble: 1 x 16 origin year month day hour temp dewp humid wind_dir wind_speed wind_gust &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 JFK 2013 5 8 22 13.1 12.0 95.3 80 8.06 NA # … with 5 more variables: precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, # time_hour &lt;dttm&gt;, temp_in_C &lt;dbl&gt; There appears to be only one hour and only at JFK that recorded 13.1 F (-10.5 C) in the month of May. This is probably a data entry mistake! Why wasn’t the weather at least similar at EWR (Newark) and LGA (La Guardia)? (LC3.23) Which months have the highest variability in temperature? What reasons do you think this is? Solution: We are now interested in the spread of the data. One measure some of you may have seen previously is the standard deviation. But in this plot we can read off the Interquartile Range (IQR): The distance from the 1st to the 3rd quartiles i.e. the length of the boxes You can also think of this as the spread of the middle 50% of the data Just from eyeballing it, it seems November has the biggest IQR, i.e. the widest box, so has the most variation in temperature August has the smallest IQR, i.e. the narrowest box, so is the most consistent temperature-wise Here’s how we compute the exact IQR values for each month (we’ll see this more in depth Chapter 5 of the text): group the observations by month then for each group, i.e. month, summarize it by applying the summary statistic function IQR(), while making sure to skip over missing data via na.rm=TRUE then arrange the table in descending order of IQR weather %&gt;% group_by(month) %&gt;% summarize(IQR = IQR(temp, na.rm=TRUE)) %&gt;% arrange(desc(IQR)) month IQR 11 16.02 12 14.04 1 13.77 9 12.06 4 12.06 5 11.88 6 10.98 10 10.98 2 10.08 7 9.18 3 9.00 8 7.02 (LC3.24) We looked at the distribution of the numerical variable temp split by the numerical variable month that we converted to a categorical variable using the factor() function. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative? Solution: Because there are 12 unique values of month yielding only 12 boxes in our boxplot. There are many more unique values of pressure (469 unique values in fact), because values are to the first decimal place. This would lead to 469 boxes, which is too many for people to digest. (LC3.25) Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram? Solution: In a histogram, the bin corresponding to where an outlier lies may not by high enough for us to see. In a boxplot, they are explicitly labelled separately. (LC3.26) Why are histograms inappropriate for visualizing categorical variables? Solution: Histograms are for numerical variables i.e. the horizontal part of each histogram bar represents an interval, whereas for a categorical variable each bar represents only one level of the categorical variable. (LC3.27) What is the difference between histograms and barplots? Solution: See above. (LC3.28) How many Envoy Air flights departed NYC in 2013? Solution: Envoy Air is carrier code MQ and thus 26397 flights departed NYC in 2013. (LC3.29) What was the seventh highest airline in terms of departed flights from NYC in 2013? How could we better present the table to get this answer quickly? Solution: What a pain! We’ll see in Chapter 5 on Data Wrangling that applying arrange(desc(n)) will sort this table in descending order of n! (LC3.30) Why should pie charts be avoided and replaced by barplots? Solution: In our opinion, comparisons using horizontal lines are easier than comparing angles and areas of circles. (LC3.31) What is your opinion as to why pie charts continue to be used? Solution: Legacy? (LC3.32) What kinds of questions are not easily answered by looking at the above figure? Solution: Because the red, green, and blue bars don’t all start at 0 (only red does), it makes comparing counts hard. (LC3.33) What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights? Solution: The different airlines prefer different airports. For example, United is mostly a Newark carrier and JetBlue is a JFK carrier. If airlines didn’t prefer airports, each color would be roughly one third of each bar.} (LC3.34) Why might the side-by-side (AKA dodged) barplot be preferable to a stacked barplot in this case? Solution: We can easily compare the different aiports for a given carrier using a single comparison line i.e. things are lined up (LC3.35) What are the disadvantages of using a side-by-side (AKA dodged) barplot, in general? Solution: It is hard to get totals for each airline. (LC3.36) Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case? Solution: Not that different than using side-by-side; depends on how you want to organize your presentation. (LC3.37) What information about the different carriers at different airports is more easily seen in the faceted barplot? Solution: Now we can also compare the different carriers within a particular airport easily too. For example, we can read off who the top carrier for each airport is easily using a single horizontal line. D.3 Chapter 4 Solutions library(dplyr) library(ggplot2) library(nycflights13) (LC4.1) What’s another way using the “not” operator ! to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the flights data frame? Test this out using the code above. Solution: # Original in book not_BTV_SEA &lt;- flights %&gt;% filter(!(dest == &quot;BTV&quot; | dest == &quot;SEA&quot;)) # Alternative way not_BTV_SEA &lt;- flights %&gt;% filter(!dest == &quot;BTV&quot; &amp; !dest == &quot;SEA&quot;) # Yet another way not_BTV_SEA &lt;- flights %&gt;% filter(dest != &quot;BTV&quot; &amp; dest != &quot;SEA&quot;) (LC4.2) Say a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach? Solution: The missing patients may have died of lung cancer! So to ignore them might seriously bias your results! It is very important to think of what the consequences on your analysis are of ignoring missing data! Ask yourself: There is a systematic reasons why certain values are missing? If so, you might be biasing your results! If there isn’t, then it might be ok to “sweep missing values under the rug.” (LC4.3) Modify the above summarize function to create summary_temp to also use the n() summary function: summarize(count = n()). What does the returned value correspond to? Solution: It corresponds to a count of the number of observations/rows: weather %&gt;% summarize(count = n()) # A tibble: 1 x 1 count &lt;int&gt; 1 26115 (LC4.4) Why doesn’t the following code work? Run the code line by line instead of all at once, and then look at the data. In other words, run summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE)) first. summary_temp &lt;- weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE)) %&gt;% summarize(std_dev = sd(temp, na.rm = TRUE)) Solution: Consider the output of only running the first two lines: weather %&gt;% summarize(mean = mean(temp, na.rm = TRUE)) # A tibble: 1 x 1 mean &lt;dbl&gt; 1 55.3 Because after the first summarize(), the variable temp disappears as it has been collapsed to the value mean. So when we try to run the second summarize(), it can’t find the variable temp to compute the standard deviation of. (LC4.5) Recall from Chapter 3 when we looked at plots of temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in New York City throughout the year? Solution: The standard deviation is a quantification of spread and variability. We see that the period in November, December, and January has the most variation in weather, so you can expect very different temperatures on different days. (LC4.6) What code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC? Solution: Note: group_by(day) is not enough, because day is a value between 1-31. We need to group_by(year, month, day) library(dplyr) library(nycflights13) summary_temp_by_month &lt;- weather %&gt;% group_by(month) %&gt;% summarize( mean = mean(temp, na.rm = TRUE), std_dev = sd(temp, na.rm = TRUE) ) (LC4.7) Recreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin). What differs in the resulting dataset? Solution: by_monthly_origin In by_monthly_origin the month column is now first and the rows are sorted by month instead of origin. If you compare the values of count in by_origin_monthly and by_monthly_origin using the View() function, you’ll see that the values are actually the same, just presented in a different order. (LC4.8) How could we identify how many flights left each of the three airports for each carrier? Solution: We could summarize the count from each airport using the n() function, which counts rows. All remarkably similar! Note: the n() function counts rows, whereas the sum(VARIABLE_NAME) funciton sums all values of a certain numerical variable VARIABLE_NAME. (LC4.9) How does the filter operation differ from a group_by followed by a summarize? Solution: filter picks out rows from the original dataset without modifying them, whereas group_by %&gt;% summarize computes summaries of numerical variables, and hence reports new values. (LC4.10) What do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value? Solution: Say a flight departed 20 minutes late, i.e. dep_delay = 20 Then arrived 10 minutes late, i.e. arr_delay = 10. Then gain = dep_delay - arr_delay = 20 - 10 = 10 is positive, so it “made up/gained time in the air.” 0 means the departure and arrival time were the same, so no time was made up in the air. We see in most cases that the gain is near 0 minutes. I never understood this. If the pilot says “we’re going make up time in the air” because of delay by flying faster, why don’t you always just fly faster to begin with? (LC4.11) Could we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights. Solution: No because you can’t do direct arithmetic on times. The difference in time between 12:03 and 11:59 is 4 minutes, but 1203-1159 = 44 (LC4.12) What can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values. Solution: Most of the time the gain is a little under zero, most of the time the gain is between -50 and 50 minutes. There are some extreme cases however! (LC4.13) Looking at Figure 4.7, when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour? Solution: Because hour is simply a value between 0 and 23; to identify a specific hour, we need to know which year, month, day and at which airport. (LC4.14) What surprises you about the top 10 destinations from NYC in 2013? Solution: This question is subjective! What surprises me is the high number of flights to Boston. Wouldn’t it be easier and quicker to take the train? (LC4.15) What are some advantages of data in normal forms? What are some disadvantages? Solution: When datasets are in normal form, we can easily _join them with other datasets! For example, we can join the flights data with the planes data. (LC4.16) What are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways. Solution: (LC4.17) How could one use starts_with, ends_with, and contains to select columns from the flights data frame? Provide three different examples in total: one for starts_with, one for ends_with, and one for contains. Solution: (LC4.18) Why might we want to use the select() function on a data frame? Solution: To narrow down the data frame, to make it easier to look at. Using View() for example. (LC4.19) Create a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013. Solution: (LC4.20) Using the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints: Crucial: Unless you are very confident in what you are doing, it is worthwhile to not starting coding right away, but rather first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code). Take a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles. Figure 4.7 above showing how the various datasets can be joined will also be useful. Consider the data wrangling verbs in Table 4.1 as your toolbox! Solution: Here are some examples of student-written pseudocode. Based on our own pseudocode, let’s first display the entire solution. Let’s now break this down step-by-step. To compute the available seat miles for a given flight, we need the distance variable from the flights data frame and the seats variable from the planes data frame, necessitating a join by the key variable tailnum as illustrated in Figure 4.7. To keep the resulting data frame easy to view, we’ll select() only these two variables and carrier: Now for each flight we can compute the available seat miles ASM by multiplying the number of seats by the distance via a mutate(): Next we want to sum the ASM for each carrier. We achieve this by first grouping by carrier and then summarizing using the sum() function: However, because for certain carriers certain flights have missing NA values, the resulting table also returns NA’s. We can eliminate these by adding a na.rm = TRUE argument to sum(), telling R that we want to remove the NA’s in the sum. We saw this in Section 4.3: Finally, we arrange() the data in desc()ending order of ASM. While the above data frame is correct, the IATA carrier code is not always useful. For example, what carrier is WN? We can address this by joining with the airlines dataset using carrier is the key variable. While this step is not absolutely required, it goes a long way to making the table easier to make sense of. It is important to be empathetic with the ultimate consumers of your presented data! D.4 Chapter 5 Solutions library(dplyr) library(ggplot2) library(nycflights13) library(tidyr) library(readr) (LC5.1) What are common characteristics of “tidy” datasets? Solution: Rows correspond to observations, while columns correspond to variables. (LC5.2) What makes “tidy” datasets useful for organizing data? Solution: Tidy datasets are an organized way of viewing data. This format is required for the ggplot2 and dplyr packages for data visualization and wrangling. (LC5.3) Take a look the airline_safety data frame included in the fivethirtyeight data. Run the following: airline_safety After reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airlines companies’ safety records. This data was originally reported on the data journalism website FiveThirtyEight.com in Nate Silver’s article “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s ignore the incl_reg_subsidiaries and avail_seat_km_per_week variables for simplicity: airline_safety_smaller &lt;- airline_safety %&gt;% select(-c(incl_reg_subsidiaries, avail_seat_km_per_week)) airline_safety_smaller # A tibble: 56 x 7 airline incidents_85_99 fatal_accidents… fatalities_85_99 incidents_00_14 &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 Aer Li… 2 0 0 0 2 Aerofl… 76 14 128 6 3 Aeroli… 6 0 0 1 4 Aerome… 3 1 64 5 5 Air Ca… 2 0 0 2 6 Air Fr… 14 4 79 6 7 Air In… 2 1 329 4 8 Air Ne… 3 0 0 5 9 Alaska… 5 0 0 5 10 Alital… 7 2 50 4 # … with 46 more rows, and 2 more variables: fatal_accidents_00_14 &lt;int&gt;, # fatalities_00_14 &lt;int&gt; This data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable incident_type_years indicating the indicent type/year and a variable count of the counts? Solution: Using the gather() function from the tidyr package: airline_safety_smaller_tidy &lt;- airline_safety_smaller %&gt;% gather(key = incident_type_years, value = count, -airline) airline_safety_smaller_tidy # A tibble: 336 x 3 airline incident_type_years count &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 Aer Lingus incidents_85_99 2 2 Aeroflot incidents_85_99 76 3 Aerolineas Argentinas incidents_85_99 6 4 Aeromexico incidents_85_99 3 5 Air Canada incidents_85_99 2 6 Air France incidents_85_99 14 7 Air India incidents_85_99 2 8 Air New Zealand incidents_85_99 3 9 Alaska Airlines incidents_85_99 5 10 Alitalia incidents_85_99 7 # … with 326 more rows If you look at the resulting airline_safety_smaller_tidy data frame in the spreadsheet viewer, you’ll see that the variable incident_type_years has 6 possible values: &quot;incidents_85_99&quot;, &quot;fatal_accidents_85_99&quot;, &quot;fatalities_85_99&quot;, &quot;incidents_00_14&quot;, &quot;fatal_accidents_00_14&quot;, &quot;fatalities_00_14&quot; corresponding to the 6 columns of airline_safety_smaller we tidied. (LC5.4) Convert the dem_score data frame into a tidy data frame and assign the name of dem_score_tidy to the resulting long-formatted data frame. Solution: Running the following in the console: Let’s now compare the dem_score and dem_score_tidy. dem_score has democracy score information for each year in columns, whereas in dem_score_tidy there are explicit variables year and democracy_score. While both representations of the data contain the same information, we can only use ggplot() to create plots using the dem_score_tidy data frame. (LC5.5) Read in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a tidy data frame. Solution: The code is similar We observe the same construct structure with respect to year in life_expectancy vs life_expectancy_tidy as we did in dem_score vs dem_score_tidy: D.5 Chapter 6 Solutions To come! library(ggplot2) library(dplyr) library(moderndive) library(gapminder) library(skimr) "],
["references.html", "References", " References "]
]
